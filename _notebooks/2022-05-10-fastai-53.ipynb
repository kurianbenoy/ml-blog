{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Deep Learning for Coders Course - Lesson 3\n",
    "\n",
    "> \"This blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland with fast.ai](https://itee.uq.edu.au/event/2022/practical-deep-learning-coders-uq-fastai). So off to week3 where we learn more how neural networks and say the C* word while learning neural networks ...\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- hide_deepnote_badge: true\n",
    "- comments: true\n",
    "- author: Kurian Benoy\n",
    "- categories: [fastai, fastbook]                                                         \n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lesson Setup\n",
    "\n",
    "There was a minor delay in streaming the lessons today, as today the sessions where being conducted \n",
    "in-person by Jeremy at University of Queensland. There were 130 people watching live in\n",
    "youtube, while there was some notable absentees like [Sanyam Bhutani](twitter.com/bhutanisanyam1/).\n",
    "\n",
    "\n",
    "Jeremy started the lesson by saying that usually lesson 1 and 2 are easy for everyone, while it's\n",
    "usually from lesson 3 things start getting hard. There is also a lesson 0 on how to do fast.ai?\n",
    "\n",
    "> youtube: https://youtu.be/gGxe2mN3kAg\n",
    "\n",
    "I had the previously written about [fastai lesson 0](https://kurianbenoy.com/2021-06-16-fastgroup-1/),\n",
    "where Jeremy mentioned about **How to do fast.ai lesson**:\n",
    "\n",
    "1. Watching lecture/book (watching the video first without trying anything)\n",
    "2. Running notebook and Experimentation (going through lesson notebooks and experimenting stuff)\n",
    "3. Reproduce Results (try with fastai clean notebook version, see if you are able to understand and do things on your own)\n",
    "4. Working on a different dataset (play with a different dataset, paraticipate in kaggle ...)\n",
    "\n",
    "\n",
    "Always studying done with other people is the best activity. So it's great to participate in study\n",
    "groups like [Delft-fastai sessions](https://www.meetup.com/delft-fast-ai-study-group/).\n",
    "\n",
    "This week, Jeremy showcased various work based on which all got highest number of votes in share-my group section. My work also got featured ðŸ™‚.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/24592806/168640845-07859116-bf6b-48c7-af61-5ff8a65dc2fb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dogs vs Cat notebooks- which image models are the best?\n",
    "\n",
    "Today Jeremy featured, paperspace gradient platform. He has been using it for \n",
    "paperspace, and it's totally amazing. He got something done by them to update fastbook\n",
    "regularly.\n",
    "\n",
    "**Jeremy says**\n",
    "\n",
    "> In lesson2, it's not about taking a particular tool and using in particular platform. There are two important pieces:\n",
    "\n",
    "> 1) Training piece (train.ipynb)\n",
    "> 2) Deployment piece (app.ipynb)\n",
    "\n",
    "\n",
    "Finding good image models, by [baselines results along with inference time](https://www.kaggle.com/code/jhoward/which-image-models-are-best/) will help us choose good architecture. He tried levit_models, didn't work really great\n",
    "\n",
    "[25.00] - experimenting with convnext tiny models from timm library. It got really good accuracy with almost 0.05 loss. At the moment there are lot of good architectures, which beats resnets really well. In this case of predicting 37 breeds of dogs we can find categories in datablock using vocab of dataloaders.\n",
    "\n",
    "\n",
    "[30:00] get_modules in pytorch and looking into how neural networks work.\n",
    "\n",
    "On looking at the notebook, What are these numbers? What are these parameters. How can these number figure out it's a particular dog breed or not? To understand this we need to understand how neural networks work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at how neural network really work?\n",
    "\n",
    "[Notebook](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)\n",
    "\n",
    "Partial application, it's just something in lot of languages. How applying quardartic\n",
    "\n",
    "data matching function with data, adding noise with data\n",
    "`torch.linspace` which goes from -2 to +4\n",
    "\n",
    "Create a plot quadratic, which helps in interacting. Using @interact\n",
    "most simple and common loss functions MSE\n",
    "\n",
    "Rerun with MSE. Now add MSE, and see if it get's better or worse. This is a manual process,\n",
    "and does he tweak. When we move up, does the loss get's better. Or if it decreases, does loss goes down.\n",
    "\n",
    "\n",
    "The derivate is a function, which tells if we increase does it increase or decrease. In pytorch,\n",
    "youu can automtically done by pytorch.\n",
    "\n",
    "ipythonwidgets using intereact\n",
    "\n",
    "return interact. Does mse made by pytorch interact\n",
    "\n",
    "Rank 1 tensor, 1 D tensor\n",
    "\n",
    "[49:00] onwards follow the calculation again, and try to do it again.\n",
    "Now it got and calculated the loss. \n",
    "\n",
    "```\n",
    "With torch.no_grad():\n",
    "\n",
    "```\n",
    "\n",
    "inner part of machine learning code. This is basic optimizer. This is gradient descent.\n",
    "We calculate gradient, and then build models. Not following stuff in lesson [57:00] is not clear for me.\n",
    "\n",
    "We learned deep learned. This is like how to draw an owl. This is how deep learning is, just\n",
    "go through the course.\n",
    "\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "I want to be around 0.001 second. Doing grid search takes a lot of time. Training your good\n",
    "models, at the first day is not a big requirement. It's very easy to get inputs & outputs,\n",
    "yet getting segmented output is way harder. The important number, learning_rate to calculate parameters.\n",
    "\n",
    "After break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive understanding with neural networks\n",
    "\n",
    "Mathematical trip, we want to do a whole bunch of RELUs. We want lot's of variables.\n",
    "Add all the relus together, then use with different bunch of behaviours. 1000s of RELUs\n",
    "\n",
    "SIngle operation except last layer, with matrix multiplication.\n",
    "Linear algebra, almost all time you need is matrix multiplcator. It's multiplying\n",
    "and adding neural networks together. GPUs are so good at this.\n",
    "\n",
    "http://matrixmultiplication.xyz/\n",
    "\n",
    "fast.ai using excel for matrix multiplication. From [1:28:32]\n",
    "\n",
    "Titanic data, about who survived and who didnt'\n",
    "\n",
    "Next week, we are going to look into how validation sets and more into metrics. We will be looking into Kaggle notebook on [how to get started with NLP](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0rc2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c158fc4cc62aaf8efdf4e544f2c60f81becbac2749f1463ef4629eb66a90f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
