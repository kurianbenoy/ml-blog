{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a99725-5404-45de-988b-7fc7f1f163cf",
   "metadata": {},
   "source": [
    "# A strange bug when using fastai library with Weights & Biases\n",
    "\n",
    "> When I was using fastai library along with weights & biases callback to track my model training, I noticed a strange error when inferencing the same models created with fast.ai library. Let's figure out the issue ...\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- hide_deepnote_badge: true\n",
    "- comments: true\n",
    "- author: Kurian Benoy\n",
    "- categories: [fastai]        \n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856e280-a9ed-4785-a752-cf107f2abd94",
   "metadata": {},
   "source": [
    "After attending an introduction to using weights & biases along with fastai session conducted by [Thomas Capaballe](https://twitter.com/capetorch). I was excited to use weights and biases library along with a few of my hobby projects. I was working on training an Image classification models on the Kaggle competition dataset [Petal to Metal](https://www.kaggle.com/competitions/tpu-getting-started).\n",
    "\n",
    "In general, whenever I am passing a code to any fastai learner objects with callback. I usually directly pass it along with `vision_learner` as shown in below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38d565-d9ae-4059-9ebe-5f8eaa7301a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"convnext_tiny_in22k\"\n",
    "learn = vision_learner(\n",
    "    data,\n",
    "    arch,\n",
    "    metrics=error_rate,\n",
    "    cbs=[\n",
    "        WandbCallback(log_preds=False, log_model=True),\n",
    "        SaveModelCallback(monitor=\"accuracy\"),\n",
    "    ],\n",
    ")\n",
    "learn.fine_tune(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b76e90-3736-4fcd-9af1-7e4ce5347caf",
   "metadata": {},
   "source": [
    "I exported this model, as I was trying to create a [hugging face spaces to identify various flowers species](https://huggingface.co/spaces/kurianbenoy/Identify_which_flower). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2109a59-685d-43a1-a60c-18706289085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02d947-1a29-480e-ab89-f81095249f70",
   "metadata": {},
   "source": [
    "Now I went ahead creating the inference code with requirements for this model. This is when I noticed that the model exported requires `wandb` library to run the inference code. I was totally surprised, why  it was happening at first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb1314-d937-4b72-89ca-bf4fc35548d8",
   "metadata": {},
   "source": [
    "**Why this annoying behaviour?**\n",
    "\n",
    "It's because when passing the callbacks to `Learner` class or it's variants like in case of computer vision fastai uses `vision_learner` class makes it stick around. In my case, I don't want the callback to hang around the `Learner` class forever, as it's just for training job monitoring only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86efae-809b-4214-8605-54fbf6326214",
   "metadata": {},
   "source": [
    "After a bit of googling, I found this solution from one of the forum posts written by [Wayde Gilliam](twitter.com/waydegilliam).\n",
    "\n",
    "> Important: Instead of adding your callback to `Learner` … if it is simply used for training, just include it in your call(s) to `fit or fit_one_cycle`. As the callback is no longer associated to your Learner, they won’t interfere with your call to `get_preds()`.\n",
    "\n",
    "[Original answer](https://forums.fast.ai/t/is-there-anyway-to-call-learn-get-preds-without-triggering-any-of-the-callbacks/64753/10).\n",
    "\n",
    "So inorder to fix it, I just passed the callbacks I am using directly with `fine_tune` method directly. Let's check the code to pass callbacks this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d965f-c438-4fb9-9173-8e8b0bd2b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"convnext_tiny_in22k\"\n",
    "learn = vision_learner(data, arch, metrics=[accuracy, error_rate])\n",
    "learn.fine_tune(\n",
    "    5,\n",
    "    cbs=[\n",
    "        WandbCallback(log_preds=False, log_model=True),\n",
    "        SaveModelCallback(monitor=\"accuracy\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a4fb6a-1236-4826-a64f-968680a10103",
   "metadata": {},
   "source": [
    "Hence I learned this valuable lesson, which fixed my bug in inferencing code for [huggingface spaces which I was creating](https://twitter.com/kurianbenoy2/status/1543985447441145856).\n",
    "\n",
    "> twitter: https://twitter.com/kurianbenoy2/status/1543985447441145856\n",
    "\n",
    "[Zach Mueller](https://twitter.com/TheZachMueller) also confirmed this is the case.\n",
    "\n",
    "> twitter: https://twitter.com/TheZachMueller/status/1544297503771746309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780e6ff-261f-4761-8763-cb92ce953ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
