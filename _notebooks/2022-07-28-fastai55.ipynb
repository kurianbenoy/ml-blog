{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788846f2-148f-4883-85f8-93bcd1abbd88",
   "metadata": {},
   "source": [
    "# Practical Deep Learning for Coders Course - Tabular Models (Linear Regression & Random Forests)\n",
    "\n",
    "> \"This blog-post series captures my weekly notes while I attend the [fastaiv5 course conducted by University of Queensland with fast.ai](https://itee.uq.edu.au/event/2022/practical-deep-learning-coders-uq-fastai). So off to week 5, where we will get started playing with tabular models.\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- hide_deepnote_badge: true\n",
    "- comments: true\n",
    "- author: Kurian Benoy\n",
    "- categories: [fastai, fastaicourse]                                                         \n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d462ac",
   "metadata": {},
   "source": [
    "BTW, it's never a good idea to use functions without understanding them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1094f80",
   "metadata": {},
   "source": [
    "## What is important for an ML practitioner\n",
    "\n",
    "Most of the time as a practitioner, your job is to connect a set of inputs to the sets of outputs you want with machine learning together in a framework.\n",
    "\n",
    "(Copy statement of Jeremy from start of lesson7 or 8)\n",
    "\n",
    "## Prequisites for the chapter\n",
    "\n",
    "a) What is matrix - n*n array\n",
    "b) What is vector? - \n",
    "\n",
    "\n",
    "In using matrix product section of notebook 1, a bunch of jargons more like:\n",
    "\n",
    "Matrx-vector multiplication\n",
    "matrix matrix products\n",
    "\n",
    "### Inputs: Pandas (how the data is cleaned, processed)\n",
    "\n",
    "Data cleaning process\n",
    "\n",
    "Based on data send the data in form of dependent/independent variables\n",
    "\n",
    "- Pandas, never delete columns\n",
    "- replace missing values using model. multiple modes first element\n",
    "- fillna with modes with simplest most way\n",
    "- In first baseline model, don't do complicated things at the start. \n",
    "- Don't discard data\n",
    "- Long tailed distribution(how it's done, we use logs in maths a lot)\n",
    "- dummy variables for Pclass\n",
    "\n",
    "### Outputs: sigmoid, RELU\n",
    "\n",
    "## Implementing Linear model & neural network from scratch notebook\n",
    "\n",
    "Very important notebook even for chapter 8, as it's cross linked.\n",
    "\n",
    "- Talk and dive into details. I feel the lesson should be treated from processiing a set of:\n",
    "\n",
    "- fastai library tabular implementation\n",
    "- using pytorch to turn linear\n",
    "- tensor is what mathematicians call tensor. APL came with the idea called tensor, tensor analysis\n",
    "- Understanding how much the data/model behaves is very important\n",
    "- matrix, vector(be familiar with jargons)\n",
    "- matrix * vector product(comes from APL)\n",
    "- looping 891 times, uses more like math\n",
    "- coding with explaination, then writing functions. Such a huge productivity win.\n",
    "\n",
    "\n",
    "inputs (Passing inputs differently in case of NN, Deep learning etc.)\n",
    "outputs (sigmoid)\n",
    "\n",
    "## Go into details of Linear models\n",
    "\n",
    "In titanic, it's very easy to create a random split. Jeremy was involved in creating this dataset, when he founded fastai\n",
    "\n",
    "- accuracy doesn't have a sensible gradient\n",
    "- but we should measure accuracy to get a good understanding\n",
    "- measuring accuracy, in his code. There is a lot of prose. The function doesn't require comments.\n",
    "\n",
    "Using Sigmoid for survival\n",
    "\n",
    "- sympy simbolic python has a really good way\n",
    "- calc_preds(), the sigmoid python and how it's cool\n",
    "- accuracy and always improved\n",
    "- for binary depended variable, why it's good. Remember that chucking sigmoid gives 0.7% improvements in increase\n",
    "- Input & output philosophy\n",
    "- feature engineering in test data with get_dummies? In fastai it has an other category by default\n",
    "\n",
    "## Neural networks, deep learning\n",
    "\n",
    "Neural network one hidden layer\n",
    "deep learning - a bunch more hidden layers (for loop)\n",
    "\n",
    "training set change and fill values\n",
    "\n",
    "- column vector, matrix of rank2\n",
    "train_dep[:, None] - trailing unit axis\n",
    "- build neural networks with one hidden, then build deep learning with multiple layers\n",
    "- WHen you run yourself, you realize our models are really good\n",
    "- Deep learning competition, with very few columns, it's hard to get good results\n",
    "\n",
    "## Framework example. Always use frameworks\n",
    "\n",
    "My perspective: I have seen this cliche argument that for learning ML, you need to go into details and not using frameworks is a step down. Jeremy emphasises always use good frameworks on top of it. Rather than re-inventing from scratch. Lot of the success of fast.ai comes from it not asking practitioners to go into details, I feel. One of the reasons I like frameworks like blurr, Icevision is also because of that and it's helping users.\n",
    "\n",
    "During a conversation with Icevision core-developer, Dickson Neoh:\n",
    "\n",
    "In icevision, within 10 minutes I can train any object detection model. It may not be most accurate, yet I can iterate so quickly.\n",
    "\n",
    "`Notes`:\n",
    "\n",
    "pandas\n",
    "feature engineering didn't understand\n",
    "\n",
    "test_dl(create entire preprocessing used in same framework)\n",
    "\n",
    "Ensembling \n",
    "\n",
    "Is created, get the prediction and stack the predictions\n",
    "\n",
    "get average predictions of 5 seconds, with this we get top 25%\n",
    "\n",
    "##  Random forests notebook\n",
    "\n",
    "really popularly algos and very popular in 2000s\n",
    "\n",
    "split rows into two group\n",
    "binary-split model\n",
    "\n",
    "Kernel density plots\n",
    "\n",
    "single method, ridiculously simple method in 1990 was 1 R. It was the predecessor to random forests.\n",
    "\n",
    "**Homework dataset**\n",
    "\n",
    "Step 4 of Jeremy notebook is to practice with a different dataset. Try applying this techqnique in monthly Kaggle Tabular Playground dataset competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c8c69-1283-4562-b76d-afd5c1c79bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0rc2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c158fc4cc62aaf8efdf4e544f2c60f81becbac2749f1463ef4629eb66a90f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
