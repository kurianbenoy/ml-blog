{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb12867-c97a-4493-8a85-f20b518fa5d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Starting an open source project - Malayalam Text Classifier\n",
    "\n",
    "> Kurian has commited to start an open source project for Text classification task in Malayalam which is going to be build as open source under [SMC community](https://smc.org.in/). \n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- hide_deepnote_badge: true\n",
    "- comments: true\n",
    "- author: Kurian Benoy\n",
    "- categories: [mtcmodels, malayalam, NLP]                                                         \n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c038a7-64b4-42ac-9f3b-28fd7db36730",
   "metadata": {},
   "source": [
    "I have been doing fastai course from 2018. Yet I have been taking it seriously probably, only after I bought the book [Deep Learning for Coders with FastAI & Pytorch](https://kurianbenoy.com/2021-06-10-Fast-group/) almost one year back. This year I had took the fastai v5 course, and I feel it's time to follow an advice which I have heard multiple times.\n",
    "\n",
    "> Important: Jeremy Howard, who is teaching this course and wrote the book prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those whoâ€™ve found any success with the course [emphasise repeatedly](https://sanyambhutani.com/how-not-to-do-fast-ai--or-any-ml-mooc-/).)\n",
    "\n",
    "## Problem Domain\n",
    "\n",
    "According to [huggingface tasks page](https://huggingface.co/tasks/text-classification):\n",
    "\n",
    "> Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.\n",
    "\n",
    "Malayalam is a highly inflectional and agglutinative language compared to other languages. The quantitative complexity of Malayalam classification was [explained in this paper](https://kavyamanohar.com/documents/tsd_morph_complexity_ml.pdf). Computer still doesn't seem to have understood basic text classification and Malayalam being a language which morphologically complex makes it even more difficult.\n",
    "\n",
    "\n",
    "## Why Text classification is interesting?\n",
    "\n",
    "I beleive working on task like `Text classification` is way more difficult when we are working in low-resource languages like Malayalam. I believe here, it's very important to believe in one's tenacity and try out next things in a field where there is very less research happening, and there is no proper ML datasets for researchers to work.\n",
    "\n",
    "> Note to myself: Will is more important than Skill\n",
    "\n",
    "Very few people seems to have applied techniques in deep learning in Malayalam, and it seems to be a good place to see if really deep learning techniques can be applied in my mother tongue, Malayalam. Yet when working in problems like this, you realize what are things you take granted in English language.\n",
    "\n",
    "In English language, there are plenty of labelled datasets on any problem set you want. Lot of articles and blogs have written on how to apply various NLP techniques in Malayalam. When it comes to Malayalam, there are just handful of people who have tried learning this language, and trying applying various NLP techniques to problems like NLP.\n",
    "\n",
    "This is why I feel this project can be challenging, and my approach is to see if the latest transformer approaches can do something or not.\n",
    "\n",
    "Vaaku2vec code can be found here, and team behind Vaakue2vec presented their work in Pycon India 2019. We trained a Malayalam language model on the Wikipedia article dump from Oct, 2018. The Wikipedia dump had 55k+ articles. The difficuly in training a Malayalam language model is text tokenization, since Malayalam is a highly inflectional and agglutinative language. In the current model, we are using nltk tokenizer (will try better alternative in the future) and the vocab size is 30k. The language model was used to train a classifier which classifies a news into 5 categories (India, Kerala, Sports, Business, Entertainment). Our classifier came out with a whooping 92% accuracy in the classification task.\n",
    "\n",
    "> youtube: https://youtu.be/rgCXWaKzMKU\n",
    "\n",
    "It was revolutionary at that time, to see deep learning techniques applied to get SOTA in NLP. IndicNLP did a lot of work, from working on projects like Word2vec, Vaakk2vec etc. They worked on creating a Named entity recognition dataset, yet after 3 years no one works at IndicNLP. \n",
    "\n",
    "When I was looking for how and where to create a project, I choose SMC as because:\n",
    "\n",
    "\n",
    "SMC I choose, simply because I feel that organization can help in Malayalam and has strong community presence.\n",
    "Even after next 3 years, I believe this project will survive.\n",
    "\n",
    "## What's the plan for project?\n",
    "\n",
    "> Important: Cervantes once wrote that \"the journey is better than the inn\", which means meaning that the process is more valuable than the destination.\n",
    "\n",
    "At moment, the project doesn't have any concrete goals and it's just me who is working in my free time.\n",
    "\n",
    "I have created a few issue, and my next blogpost will be on creating a baseline model on a basic dataset created.\n",
    "I expect the few dataset createion to be a iterative task, and looking forward to blog my working in each stage of project.\n",
    "\n",
    "\n",
    "## Thanks\n",
    "\n",
    "\n",
    "Alex Strickvl\n",
    "\n",
    "Santhosh Thottingal, Kavya Manohar\n",
    "\n",
    "fanbyprinciple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca037299-1a2a-483b-8534-17d8789af008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c8f14-256b-4005-97e6-b3048531353b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
