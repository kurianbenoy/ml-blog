---
toc: true
layout: post
description: "How well does CLIP models perform in a real world kaggle competition and is zero-shot image classification great?"
categories: [fastai, CLIP, openai]
comments: true
author: Kurian Benoy
title: "How well does CLIP models classify corn seeds"
---

The OpenAI CLIP model are really impressive, and I impressed by the wide range of applications it be used for like
[Semantic Video Search](https://huggingface.co/spaces/YiYiXu/it-happened-one-frame-2), [Zero shot image classification](https://github.com/openai/CLIP#zero-shot-prediction), [searching images in your gallery](https://wandb.ai/pcuenq/photo-finder/reports/Creating-a-Semantic-Search-Engine-for-My-Photos--VmlldzoyMDE2NzQ3) etc.

I recently [started reading CLIP paper](https://arxiv.org/abs/2103.00020) and paper claims to have very high accuracy in
image clssification accuracy. I thought of trying it out that in a kaggle competition I had recently participated.

Even after using one of the best trained CLIP models available it's getting close to 0.26 score in private LB in a kaggle competition where Resnet or Convnext models can give easily 0.75+ score.

| Model | Public LB | Private LB | Notebook link|
| --- | --- | --- | --- |
| ViT-B-32-quickgelu | 0.16666 | 0.18397 | [link](https://www.kaggle.com/code/kurianbenoy/playing-with-clip-model?scriptVersionId=108925854) |
| ViT-H-14 | 0.28591 | 0.27955 | [link](https://www.kaggle.com/code/kurianbenoy/playing-with-clip-model?scriptVersionId=109012620) |

