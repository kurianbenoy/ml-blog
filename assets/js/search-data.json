{
  
    
        "post0": {
            "title": "Building a fine-tuned translation system for English-Malayalam",
            "content": "Hey, everyone. We all are familiar with translation systems like using google translate. So today, let&#39;s build a fine tuned translation system for converting text from english to malayalam. It&#39;s built using Blurr library - built on top of hugging face and fastai made by Wayde Gilliam. Also our translation system is going to be fine tuned on top of KDE specific dataset. . . . Loading Data . A translation system is an example of sequence to sequence models, which is usually used for tasks which involves generating new data. Translation usually needs datasets in both the source language and target language (the language to which it needs to be translated). . We are using KDE4 datasets, and choose both source language and translation language as english and malayalam respectively. Usually these datasets are curated by community volunteers to their native language, and this was probably done by KDE community volunteers in Kerala. When someone is localizing these texts into there in local languague, usually computer science specific terms are still written in english. . import pandas from datasets import load_dataset . raw_datasets = load_dataset(&quot;kde4&quot;, lang1=&quot;en&quot;, lang2=&quot;ml&quot;, split=&quot;train[:1000]&quot;) . Using custom data configuration en-ml-lang1=en,lang2=ml . Downloading and preparing dataset kde4/en-ml to /root/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac... Dataset kde4 downloaded and prepared to /root/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac. Subsequent calls will reuse this data. . Most of translation dataset is in form of id and translation json output - with both en and ml as objects. . raw_datasets[0] . {&#39;id&#39;: &#39;0&#39;, &#39;translation&#39;: {&#39;en&#39;: &#39;Add Feed to Akregator&#39;, &#39;ml&#39;: &#39;അക്രിഗേറ്ററില് u200d ഫീഡ് കൂട്ടിച്ചേര് u200dക്കുക&#39;}} . . Transforming data into DataLoaders . Importing libraries and get hugging-face objects . from blurr.data.all import * from blurr.modeling.all import * from fastai.data.all import * from fastai.callback.all import * from fastai.learner import * from fastai.optimizer import * from transformers import * . pretrained_model_name = &quot;Helsinki-NLP/opus-mt-en-ml&quot; model_cls = AutoModelForSeq2SeqLM hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls) hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model) . (&#39;marian&#39;, transformers.models.marian.configuration_marian.MarianConfig, transformers.models.marian.tokenization_marian.MarianTokenizer, transformers.models.marian.modeling_marian.MarianMTModel) . translation_df = pd.DataFrame(raw_datasets[&quot;translation&quot;], columns=[&quot;en&quot;, &quot;ml&quot;]) translation_df.head() . en ml . 0 Add Feed to Akregator | അക്രിഗേറ്ററില്‍ ഫീഡ് കൂട്ടിച്ചേര്‍ക്കുക | . 1 Add Feeds to Akregator | അക്രിഗേറ്ററില്‍ ഫീഡുകള്‍ കൂട്ടിച്ചേര്‍ക്കുക | . 2 Add All Found Feeds to Akregator | എല്ലാ ഫീഡുകളും അക്രിഗേറ്ററില്‍ കൂട്ടിച്ചേര്‍ക്കുക | . 3 Subscribe to site updates (using news feed) | സൈറ്റുകളിലെ പുതുമകളറിയാന്‍ വരിക്കാരനാകുക (വാര്‍ത്താ ഫീഡുകള്‍ ഉപയോഗിച്ചു്) | . 4 Imported Feeds | എടുത്ത ഫീഡുകള്‍ | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; blocks = (Seq2SeqTextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), noop) dblock = DataBlock(blocks=blocks, get_x=ColReader(&quot;en&quot;), get_y=ColReader(&quot;ml&quot;), splitter=RandomSplitter()) . dls = dblock.dataloaders(translation_df, bs=1) dls.show_batch(dataloaders=dls, max_n=2, input_trunc_at=100, target_trunc_at=250) . text target . 0 A▁version▁control▁history▁entry▁consists of▁several▁lines.▁Specify the▁regular▁expression▁to▁detect | ഒരു ഭാഷാന്തര നിയന്ത്രണത്തിന്റെ നാള്വഴി ചേര്ക്കുന്നതില് പല വരികളുണ്ടാകും. ആദ്യത്തെ വരി കണ്ടുപിടിക്കാനുള്ള നിത്യഭാവം നിര്ദ്ദേശിക്കുക (മുന്നിലെ വിശദീകരണം കൂടാതെ). ഇനം തിരിക്കാനുപയോഗിക്കുന്ന കീകളെ ഒന്നിച്ചാക്കാന് ബ്രാക്കറ്റുകള് ഉപയോഗിക്കുക. ഒഴിച്ചു വിട്ട | . . Training fine-tuned translation system . Using blurr High-level API . learn = BlearnerForTranslation.from_data( translation_df, pretrained_model_name, src_lang_name=&quot;English&quot;, src_lang_attr=&quot;en&quot;, trg_lang_name=&quot;Malayalam&quot;, trg_lang_attr=&quot;ml&quot;, dl_kwargs={&quot;bs&quot;: 4}, ) . metrics_cb = BlearnerForTranslation.get_metrics_cb() learn.fit_one_cycle(1, lr_max=4e-5, cbs=[metrics_cb]) . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. [nltk_data] Downloading package omw-1.4 to /root/nltk_data... [nltk_data] Unzipping corpora/omw-1.4.zip. . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 4.847442 | 3.989661 | 0.028411 | 0.188749 | 5.151875 | 00:51 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=250) . text target prediction . 0 A▁version▁control▁history▁entry▁consists of▁several▁lines.▁Specify the▁regular▁expression▁to▁detect the▁first▁line (without the▁leading▁comment).▁Use▁parentheses▁to▁group the▁keys▁you▁want▁to▁use▁for▁sorting.▁If▁left▁empty,▁then▁KDiff3▁assumes▁that▁empty▁lines▁separate▁history▁entries.▁See the▁documentation▁for▁details. | ഒരു ഭാഷാന്തര നിയന്ത്രണത്തിന്റെ നാള്വഴി ചേര്ക്കുന്നതില് പല വരികളുണ്ടാകും. ആദ്യത്തെ വരി കണ്ടുപിടിക്കാനുള്ള നിത്യഭാവം നിര്ദ്ദേശിക്കുക (മുന്നിലെ വിശദീകരണം കൂടാതെ). ഇനം തിരിക്കാനുപയോഗിക്കുന്ന കീകളെ ഒന്നിച്ചാക്കാന് ബ്രാക്കറ്റുകള് ഉപയോഗിക്കുക. ഒഴിച്ചു വിട്ട | [ഒരു പതിപ്പ് നിയന്ത്രകന്റെ നിയന്ത്രണം പല വരികളിലുമുണ്ടു്. ആദ്യത്തെ വരി (വലുപ്പത്തില്നിയ്ക്കു്) കണ്ടെത്താനുള്ള ക്രമമായ പ്രയോഗം വ്യക്തമാക്കുക. നിങ്ങൾ ടൈപ്പ് ചെയ്യുവാനുള്ള കീകൾ ടൈപ്പ് ചെയ്യുക. വിട്ടുപോയാൽ, ഒഴിഞ്ഞ വര്ണ്ണങ്ങള്ക്കുള്ള വര്ണ്ണങ്ങള്ക്കുള്ള വര്ണങ്ങള്ണങ്ങള്ക്ക്ണങ്ങള്ണങ്ങള്ക്കങ്ങള്ണ്ക്കങ്ങള്ക്കണ്ടു്. വിവരങ്ങള്ക്ക് KDef3, ഡാറ്റ നഷ്ടപ്പെടാനോ മറ്റു് കേടുപാടുകള്ക്കോ തടയാനോ നിങ്ങൾക്ക് സിസ്റ്റം സസ്പെൻഡ് ചെയ്യാനോ, ഹ്യൂസ്ബര്ഡ് ചെയ്യാനോ സാധിക്കുന്നു, അതിനാൽ ബാറ്ററി അധികാരത്തിൽ നിന്ന് നിങ്ങൾ ഓടരുത്. മെഷീൻ ക്രമീകരിക്കുന്ന പ്രവർത്തനങ്ങളുടെ എണ്ണം താഴെ ക്രമീകരിക്കുക., ഒരു ഇമെയിൽ വിലാസം പോലെ കാണുന്ന ഒരു കണ്ണിയിൽ ക്ലിക്ക് ചെയ്തിട്ടുണ്ടു്. പക്ഷേ അതിന്റെ പിൻഭാഗത്ത് ഇതു് ചേരുന്നില്ല:% 1 lock to be track about to cause for play for play played by completion% 2, നിങ്ങൾ ശരിക്കും എങ്ങോട്ടു് പോകുമെന്നു് കൃത്യമായി സൂചിപ്പിക്കുവാൻ സാധ്യമല്ലാത്ത ഒരു കണ്ണിയിൽ ക്ലിക്ക് ചെയ്തിട്ടുണ്ടു്. ഈ സർവറിലുള്ള ഒരു താള് ശരിക്കും കാണണമെങ്കിൽ ദയവായി പരിശോധിക്കുക:% 1 അവിടെ പോകണമോ? @ title: group column] | . Using mid-level of blurr APIs . b = dls.one_batch() . len(b), b[0][&quot;input_ids&quot;].shape, b[1].shape . (2, torch.Size([1, 72]), torch.Size([1, 114])) . dls.show_batch(dataloaders=dls, input_trunc_at=250, target_trunc_at=250) . text target . 0 A▁version▁control▁history▁entry▁consists of▁several▁lines.▁Specify the▁regular▁expression▁to▁detect the▁first▁line (without the▁leading▁comment).▁Use▁parentheses▁to▁group the▁keys▁you▁want▁to▁use▁for▁sorting.▁If▁left▁empty,▁then▁KDiff3▁assumes▁that▁e | ഒരു ഭാഷാന്തര നിയന്ത്രണത്തിന്റെ നാള്വഴി ചേര്ക്കുന്നതില് പല വരികളുണ്ടാകും. ആദ്യത്തെ വരി കണ്ടുപിടിക്കാനുള്ള നിത്യഭാവം നിര്ദ്ദേശിക്കുക (മുന്നിലെ വിശദീകരണം കൂടാതെ). ഇനം തിരിക്കാനുപയോഗിക്കുന്ന കീകളെ ഒന്നിച്ചാക്കാന് ബ്രാക്കറ്റുകള് ഉപയോഗിക്കുക. ഒഴിച്ചു വിട്ട | . seq2seq_metrics = {&quot;bleu&quot;: {&quot;returns&quot;: &quot;bleu&quot;}, &quot;meteor&quot;: {&quot;returns&quot;: &quot;meteor&quot;}, &quot;sacrebleu&quot;: {&quot;returns&quot;: &quot;score&quot;}} model = BaseModelWrapper(hf_model) learn_cbs = [BaseModelCallback] fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)] learn = Learner( dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), # CrossEntropyLossFlat() cbs=learn_cbs, splitter=partial(blurr_seq2seq_splitter, arch=hf_arch), ) learn.freeze() . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package omw-1.4 to /root/nltk_data... [nltk_data] Package omw-1.4 is already up-to-date! . learn.lr_find(suggest_funcs=[minimum, steep, valley, slide]) . SuggestedLRs(minimum=0.0002511886414140463, steep=1.3182567499825382e-06, valley=0.0003981071640737355, slide=3.0199516913853586e-05) . learn.fit_one_cycle(3, lr_max=0.0003981071640737355, cbs=fit_cbs) . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 3.522702 | 3.374413 | 0.033723 | 0.155236 | 3.005034 | 02:58 | . 1 | 2.269610 | 3.002045 | 0.022526 | 0.168159 | 3.360931 | 02:09 | . 2 | 1.717019 | 2.910453 | 0.051017 | 0.194193 | 5.273562 | 02:18 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=500) . text target prediction . 0 ▁Regular▁expression▁for▁lines▁where▁KDiff3▁should▁automatically▁choose▁one▁source.▁When▁a▁line▁with▁a▁conflict▁matches the▁regular▁expression▁then -▁if▁available - C,▁otherwise B▁will▁be▁chosen. | കെഡിഫ്3 സ്വതന്ത്രമായി ഒരു സ്രോതസ്സ് തെരഞ്ഞെടുക്കുന്നിടത്ത് വരികള്ക്കുള്ള നിത്യഭാവം. സംഘട്ടനമുള്ള വരി ചേര്ന്നുവരുമ്പോള് അതിന്റെ നിത്യഭാവം - സി ഉണ്ടെങ്കില് അത്, അല്ലെങ്കില് ബി തെരഞ്ഞെടുക്കപ്പെടും. | കെഡിഫ്3 സ്വയം തിരഞ്ഞെടുക്കേണ്ട വരികളുടെ സാധാരണ പ്രയോഗം ഒരു സ്രോതസ്സ് തെരഞ്ഞെടുക്കുക. നിത്യഭാവവുമായി ചേരുമ്പോള്ച്ചയെ - സി - സി | . . Now let&#39;s translate with our trained models . blurr top 3 translation predictions . test_text = &quot;How are you doing&quot; outputs = learn.blurr_generate(test_text, key=&quot;translation_texts&quot;, num_return_sequences=3) outputs . [{&#39;translation_texts&#39;: [&#39;എന്തൊക്കെയുണ്ട്?&#39;, &#39;എങ്ങനെയുണ്ട്?&#39;, &#39;എന്തൊക്കെയുണ്ട് വിശേഷം&#39;]}] . Saving trained ML model . export_fname = &quot;saved_model&quot; learn.metrics = None learn.export(fname=f&quot;{export_fname}.pkl&quot;) . Prediction with our trained model . Correct Prediction . test_text = &quot;How are you doing&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text) . [{&#39;translation_texts&#39;: &#39;എന്തൊക്കെയുണ്ട്?&#39;}] . test_text1 = &quot;Add All Found Feeds to Akregator.&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text1) . [{&#39;translation_texts&#39;: &#39;എല്ലാ ഫീഡുകളും അക്രിഗേറ്ററിയിലേക്ക് ചേരുക.&#39;}] . Wrong Prediction . test_text2 = &quot;Subscribe to site updates (using news feed).&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text2) . [{&#39;translation_texts&#39;: &#39;സൈറ്റ് പുതുക്കുവാനുള്ള ഉപാധികള്color (ഒരു ന്യൂസ് ആഹാരവുമായി.).&#39;}] . Expected: &#39;സൈറ്റുകളിലെ പുതുമകളറിയാന് u200d വരിക്കാരനാകുക (വാര് u200dത്താ ഫീഡുകള് u200d ഉപയോഗിച്ചു് . Thanks to . Wayde Gilliam - for creating blurr, and helping with doubts in translation bits | Kevin Bird - for helping in editing the article. | . fin. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/12/_03_13_huggingace_translation_models.html",
            "relUrl": "/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/12/_03_13_huggingace_translation_models.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
            "content": ". from datasets import load_dataset raw_datasets = load_dataset(&quot;squad&quot;) . Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453... Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data. . raw_datasets . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 87599 }) validation: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 10570 }) }) . Recently a tweet went viral . We can see that NLP is hard, and even the best publicly avaiable models performs poorly in this tasks | . . from transformers import pipeline classifier = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased&quot;) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.weight&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;classifier.weight&#39;, &#39;pre_classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . [{&#39;label&#39;: &#39;LABEL_0&#39;, &#39;score&#39;: 0.5706434845924377}] . checkpoint = &quot;cardiffnlp/twitter-roberta-base-sentiment&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . [{&#39;label&#39;: &#39;LABEL_2&#39;, &#39;score&#39;: 0.9608174562454224}] . checkpoint = &quot;finiteautomata/bertweet-base-sentiment-analysis&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji . [{&#39;label&#39;: &#39;NEG&#39;, &#39;score&#39;: 0.9447618126869202}] . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;Where do I work?&quot;, context=&quot;My name is Sylvain and I work at Hugging Face in Brooklyn&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Hugging Face&#39;, &#39;end&#39;: 45, &#39;score&#39;: 0.6949771046638489, &#39;start&#39;: 33} . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;What was feedback?&quot;, context=&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Brilliant service&#39;, &#39;end&#39;: 101, &#39;score&#39;: 0.13661321997642517, &#39;start&#39;: 84} . For specific question answering - use question answering models . Encoder-only models like BERT tend to be great at extracting answers to factoid questions like “Who invented the Transformer architecture?” but fare poorly when given open-ended questions like “Why is the sky blue?” In these more challenging cases . If you’re interested in this type of generative question answering, we recommend checking out our demo based on the ELI5 dataset. Using Lonformer model. . [1] https://yjernite.github.io/lfqa.html . print(&quot;Context: &quot;, raw_datasets[&quot;train&quot;][0][&quot;context&quot;]) print(&quot;Question: &quot;, raw_datasets[&quot;train&quot;][0][&quot;question&quot;]) print(&quot;Answer: &quot;, raw_datasets[&quot;train&quot;][0][&quot;answers&quot;]) . Context: Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34;Venite Ad Me Omnes&#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? Answer: {&#39;text&#39;: [&#39;Saint Bernadette Soubirous&#39;], &#39;answer_start&#39;: [515]} . raw_datasets[&quot;train&quot;].filter(lambda x: len(x[&quot;answers&quot;][&quot;text&quot;]) != 1) . Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 0 }) . print(raw_datasets[&quot;validation&quot;][0][&quot;answers&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;answers&quot;]) . {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]} {&#39;text&#39;: [&#39;Santa Clara, California&#39;, &#34;Levi&#39;s Stadium&#34;, &#34;Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California.&#34;], &#39;answer_start&#39;: [403, 355, 355]} . print(raw_datasets[&quot;validation&quot;][2][&quot;context&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;question&quot;]) . Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the &#34;golden anniversary&#34; with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as &#34;Super Bowl L&#34;), so that the logo could prominently feature the Arabic numerals 50. Where did Super Bowl 50 take place? . from transformers import AutoTokenizer model_checkpoint = &quot;bert-base-cased&quot; tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . context = raw_datasets[&quot;train&quot;][0][&quot;context&quot;] question = raw_datasets[&quot;train&quot;][0][&quot;question&quot;] inputs = tokenizer(question, context) tokenizer.decode(inputs[&quot;input_ids&quot;]) . &#39;[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]&#39; . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, ) for ids in inputs[&quot;input_ids&quot;]: print(tokenizer.decode(ids)) . [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;offset_mapping&#39;, &#39;overflow_to_sample_mapping&#39;]) . inputs[&quot;overflow_to_sample_mapping&quot;] . [0, 0, 0, 0] . inputs = tokenizer( raw_datasets[&quot;train&quot;][2:6][&quot;question&quot;], raw_datasets[&quot;train&quot;][2:6][&quot;context&quot;], max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) print(f&quot;The 4 examples gave {len(inputs[&#39;input_ids&#39;])} features.&quot;) print(f&quot;Here is where each comes from: {inputs[&#39;overflow_to_sample_mapping&#39;]}.&quot;) . The 4 examples gave 19 features. Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]. . Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If that’s not the case, we loop to find the first and last token of the answer: . answers = raw_datasets[&quot;train&quot;][2:6][&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(inputs[&quot;offset_mapping&quot;]): sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) start_positions, end_positions . ([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0], [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0]) . idx = 0 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] start = start_positions[idx] end = end_positions[idx] labeled_answer = tokenizer.decode(inputs[&quot;input_ids&quot;][idx][start : end + 1]) print(f&quot;Theoretical answer: {answer}, labels give: {labeled_answer}&quot;) . Theoretical answer: the Main Building, labels give: the Main Building . idx = 4 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] decoded_example = tokenizer.decode(inputs[&quot;input_ids&quot;][idx]) print(f&quot;Theoretical answer: {answer}, decoded example: {decoded_example}&quot;) . Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP] . max_length = 384 stride = 128 def preprocess_training_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) offset_mapping = inputs.pop(&quot;offset_mapping&quot;) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) answers = examples[&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(offset_mapping): sample_idx = sample_map[i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) inputs[&quot;start_positions&quot;] = start_positions inputs[&quot;end_positions&quot;] = end_positions return inputs . train_dataset = raw_datasets[&quot;train&quot;].map( preprocess_training_examples, batched=True, remove_columns=raw_datasets[&quot;train&quot;].column_names, ) len(raw_datasets[&quot;train&quot;]), len(train_dataset) . (87599, 88729) . &#39;Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot; Venite Ad Me Omnes &quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]&#39; . Indeed, we don’t see the answer inside the context. . ✏️ Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied. . Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We’ll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here: . def preprocess_validation_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) example_ids = [] for i in range(len(inputs[&quot;input_ids&quot;])): sample_idx = sample_map[i] example_ids.append(examples[&quot;id&quot;][sample_idx]) sequence_ids = inputs.sequence_ids(i) offset = inputs[&quot;offset_mapping&quot;][i] inputs[&quot;offset_mapping&quot;][i] = [ o if sequence_ids[k] == 1 else None for k, o in enumerate(offset) ] inputs[&quot;example_id&quot;] = example_ids return inputs . validation_dataset = raw_datasets[&quot;validation&quot;].map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) len(raw_datasets[&quot;validation&quot;]), len(validation_dataset) . (10570, 10822) . small_eval_set = raw_datasets[&quot;validation&quot;].select(range(100)) trained_checkpoint = &quot;distilbert-base-cased-distilled-squad&quot; tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint) eval_set = small_eval_set.map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) . tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . import torch from transformers import AutoModelForQuestionAnswering eval_set_for_model = eval_set.remove_columns([&quot;example_id&quot;, &quot;offset_mapping&quot;]) eval_set_for_model.set_format(&quot;torch&quot;) device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names} trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to( device ) with torch.no_grad(): outputs = trained_model(**batch) . start_logits = outputs.start_logits.cpu().numpy() end_logits = outputs.end_logits.cpu().numpy() . import collections example_to_features = collections.defaultdict(list) for idx, feature in enumerate(eval_set): example_to_features[feature[&quot;example_id&quot;]].append(idx) . import numpy as np n_best = 20 max_answer_length = 30 predicted_answers = [] for example in small_eval_set: example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = eval_set[&quot;offset_mapping&quot;][feature_index] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length. if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answers.append( { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } ) best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]}) . from datasets import load_metric metric = load_metric(&quot;squad&quot;) . metric . Metric(name: &#34;squad&#34;, features: {&#39;predictions&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;prediction_text&#39;: Value(dtype=&#39;string&#39;, id=None)}, &#39;references&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answers&#39;: Sequence(feature={&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answer_start&#39;: Value(dtype=&#39;int32&#39;, id=None)}, length=-1, id=None)}}, usage: &#34;&#34;&#34; Computes SQuAD scores (F1 and EM). Args: predictions: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair as given in the references (see below) - &#39;prediction_text&#39;: the text of the answer references: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair (see above), - &#39;answers&#39;: a Dict in the SQuAD dataset format { &#39;text&#39;: list of possible texts for the answer, as a list of strings &#39;answer_start&#39;: list of start positions for the answer, as a list of ints } Note that answer_start values are not taken into account to compute the metric. Returns: &#39;exact_match&#39;: Exact match (the normalized answer exactly match the gold answer) &#39;f1&#39;: The F-score of predicted tokens versus the gold answer Examples: &gt;&gt;&gt; predictions = [{&#39;prediction_text&#39;: &#39;1976&#39;, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; references = [{&#39;answers&#39;: {&#39;answer_start&#39;: [97], &#39;text&#39;: [&#39;1976&#39;]}, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; squad_metric = datasets.load_metric(&#34;squad&#34;) &gt;&gt;&gt; results = squad_metric.compute(predictions=predictions, references=references) &gt;&gt;&gt; print(results) {&#39;exact_match&#39;: 100.0, &#39;f1&#39;: 100.0} &#34;&#34;&#34;, stored examples: 0) . theoretical_answers = [ {&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in small_eval_set ] print(predicted_answers[0]) print(theoretical_answers[0]) . {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;prediction_text&#39;: &#39;Denver Broncos&#39;} {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;answers&#39;: {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]}} . metric.compute(predictions=predicted_answers, references=theoretical_answers) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . from tqdm.auto import tqdm def compute_metrics(start_logits, end_logits, features, examples): example_to_features = collections.defaultdict(list) for idx, feature in enumerate(features): example_to_features[feature[&quot;example_id&quot;]].append(idx) predicted_answers = [] for example in tqdm(examples): example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] # Loop through all features associated with that example for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = features[feature_index][&quot;offset_mapping&quot;] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answer = { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } answers.append(answer) # Select the answer with the best score if len(answers) &gt; 0: best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append( {&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]} ) else: predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: &quot;&quot;}) theoretical_answers = [{&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in examples] return metric.compute(predictions=predicted_answers, references=theoretical_answers) . compute_metrics(start_logits, end_logits, eval_set, small_eval_set) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) . Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#39;qa_outputs.bias&#39;, &#39;qa_outputs.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . from huggingface_hub import notebook_login notebook_login() . Login successful Your token has been saved to /root/.huggingface/token Authenticated through git-credential store but this isn&#39;t the helper defined on your machine. You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default git config --global credential.helper store . ! sudo apt install git-lfs ! git-lfs install . Reading package lists... Done Building dependency tree Reading state information... Done git-lfs is already the newest version (2.3.4-1). The following package was automatically installed and is no longer required: libnvidia-common-470 Use &#39;sudo apt autoremove&#39; to remove it. 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Error: Failed to call git rev-parse --git-dir --show-toplevel: &#34;fatal: not a git repository (or any of the parent directories): .git n&#34; Git LFS initialized. . from transformers import TrainingArguments args = TrainingArguments( &quot;bert-finetuned-squad&quot;, evaluation_strategy=&quot;no&quot;, save_strategy=&quot;epoch&quot;, learning_rate=2e-5, num_train_epochs=1, weight_decay=0.01, fp16=True, push_to_hub=True, ) . PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). . from transformers import Trainer trainer = Trainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=validation_dataset, tokenizer=tokenizer, ) trainer.train() . /content/bert-finetuned-squad is already a clone of https://huggingface.co/kurianbenoy/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`. Using amp half precision backend /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 88729 Num Epochs = 1 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 11092 . . [ 4154/11092 2:29:47 &lt; 4:10:18, 0.46 it/s, Epoch 0.37/1] Step Training Loss . 500 | 1.691300 | . 1000 | 1.575600 | . 1500 | 1.441200 | . 2000 | 1.356100 | . 2500 | 1.297700 | . 3000 | 1.245200 | . 3500 | 1.242400 | . 4000 | 1.181400 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; predictions, _ = trainer.predict(validation_dataset) start_logits, end_logits = predictions compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[&quot;validation&quot;]) . trainer.push_to_hub(commit_message=&quot;Training complete&quot;) . &lt;/div&gt; .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/03/12/_02_27_QuestionAnsweringWithHF.html",
            "relUrl": "/2022/03/12/_02_27_QuestionAnsweringWithHF.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Starting_my_ml_blog",
            "content": "Starting my ML blog . I am into the field of AI/DataScience for some time. To dive deep into this field, I am creating this ML blogpost space. This space is exclusively for my ML/AI articles. Also by 27th February, 2022 expect a blog post on question answering w/ blurr. .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/02/21/Starting_my_ML_blog.html",
            "relUrl": "/2022/02/21/Starting_my_ML_blog.html",
            "date": " • Feb 21, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kurianbenoy.github.io/ml-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kurianbenoy.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kurianbenoy.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}