{
  
    
        "post0": {
            "title": "Practical Deep Learning for Coders Course - Lesson 3",
            "content": "Lesson Setup . There was a minor delay in streaming the lessons today, as today the sessions where being conducted in-person by Jeremy at University of Queensland. There were 130 people watching live in youtube, while there was some notable absentees like Sanyam Bhutani. . Jeremy started the lesson by saying that usually lesson 1 and 2 are easy for everyone, while it&#39;s usually from lesson 3 things start getting hard. There is also a lesson 0 on how to do fast.ai? I had the previously written about fastai lesson 0, where Jeremy mentioned about How to do fast.ai lesson: . Watching lecture/book (watching the video first without trying anything) | Running notebook and Experimentation (going through lesson notebooks and experimenting stuff) | Reproduce Results (try with fastai clean notebook version, see if you are able to understand and do things on your own) | Working on a different dataset (play with a different dataset, paraticipate in kaggle ...) | Always studying done with other people is the best activity. So it&#39;s great to participate in study groups like Delft-fastai sessions. . This week, Jeremy showcased various work based on which all got highest number of votes in share-my group section. My work also got featured üôÇ. . . Dogs vs Cat notebooks- which image models are the best? . Today Jeremy featured, paperspace gradient platform. He has been using it for paperspace, and it&#39;s totally amazing. He got something done by them to update fastbook regularly. . Jeremy says . In lesson2, it&#39;s not about taking a particular tool and using in particular platform. There are two important pieces:&gt; 1) Training piece (train.ipynb)&gt; 2) Deployment piece (app.ipynb) . Finding good image models, by baselines results along with inference time will help us choose good architecture. He tried levit_models, didn&#39;t work really great [25.00] - experimenting with convnext tiny models from timm library. It got really good accuracy with almost 0.05 loss. At the moment there are lot of good architectures, which beats resnets really well. In this case of predicting 37 breeds of dogs we can find categories in datablock using vocab of dataloaders. . [30:00] get_modules in pytorch and looking into how neural networks work. . On looking at the notebook, What are these numbers? What are these parameters. How can these number figure out it&#39;s a particular dog breed or not? To understand this we need to understand how neural networks work. . Looking at how neural network really work? . Notebook . Partial application, it&#39;s just something in lot of languages. How applying quardartic . data matching function with data, adding noise with data torch.linspace which goes from -2 to +4 . Create a plot quadratic, which helps in interacting. Using @interact most simple and common loss functions MSE . Rerun with MSE. Now add MSE, and see if it get&#39;s better or worse. This is a manual process, and does he tweak. When we move up, does the loss get&#39;s better. Or if it decreases, does loss goes down. . The derivate is a function, which tells if we increase does it increase or decrease. In pytorch, youu can automtically done by pytorch. . ipythonwidgets using intereact . return interact. Does mse made by pytorch interact . Rank 1 tensor, 1 D tensor . [49:00] onwards follow the calculation again, and try to do it again. Now it got and calculated the loss. . With torch.no_grad(): . inner part of machine learning code. This is basic optimizer. This is gradient descent. We calculate gradient, and then build models. Not following stuff in lesson [57:00] is not clear for me. . We learned deep learned. This is like how to draw an owl. This is how deep learning is, just go through the course. . https://pytorch.org/tutorials/ . I want to be around 0.001 second. Doing grid search takes a lot of time. Training your good models, at the first day is not a big requirement. It&#39;s very easy to get inputs &amp; outputs, yet getting segmented output is way harder. The important number, learning_rate to calculate parameters. . After break . Intuitive understanding with neural networks . Mathematical trip, we want to do a whole bunch of RELUs. We want lot&#39;s of variables. Add all the relus together, then use with different bunch of behaviours. 1000s of RELUs . SIngle operation except last layer, with matrix multiplication. Linear algebra, almost all time you need is matrix multiplcator. It&#39;s multiplying and adding neural networks together. GPUs are so good at this. . http://matrixmultiplication.xyz/ . fast.ai using excel for matrix multiplication. From [1:28:32] . Titanic data, about who survived and who didnt&#39; . Next week, we are going to look into how validation sets and more into metrics. We will be looking into Kaggle notebook on how to get started with NLP. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/05/10/fastai-53.html",
            "relUrl": "/fastai/fastbook/2022/05/10/fastai-53.html",
            "date": " ‚Ä¢ May 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Practical Deep Learning for Coders Course - Lesson 2",
            "content": "Lesson Setup . Jeremy was taking this session from his home, as the venue in University of queensland was already booked by someone else. Jeremy was really really pumped for this lesson and it&#39;s like going to the early days of fast.ai with lot of super exciting work happening. The magic of Top Down Learning üçµ:Just 1 week into the @fastdotai cohort, the ‚Äúshare your projects‚Äù discussion is at 100+ messages üôèThe MOOC releases later this year, but the awesome community never ceases to amaze me. . &mdash; Sanyam Bhutani (@bhutanisanyam1) May 3, 2022 . Jeremy mentioned some technique on using Jupyter notebooks, and asked to take a look at jupyter extensions. The navigation section and how to collapse headings was explained during class. [24:00] . Fastbook Chapter 2 . This week we started by taking a look at putting model in production using fastai. This was the same thing which is covered in chapter 2 of Deep Learning book To build grizzly bears and teddy bears classifier. . Few things have changed in book in this version: . using search_images_ddg instead of bing search apis | using huggingfaces spaces as deployment instead of voila even though it&#39;s still worksRandomResizedCrop could be a good idea to understand different varieties of same image. . | . Does RandomResizedCrop crop duplicate the image -- i.e. you get multiple copies and you ensure that all the parts of the image are used in training? or does it just make one crop? . Jeremy answered it in video at [32:30]. His answer was it doesn&#39;t copying image. In each epoch everyimage get&#39;s written and what happens is in-memory image is being wrapped by recropping and colouring in realtime during model training. It&#39;s like infinitely multi-copies of images. . Check the book to learn more in detail about various augmentations. . Sanyam mentioned that RandomResized crop as a augmentation is very helpful: . Important: Actually this technique is SUPER helpful-in a recent interview, Chris Deotte (4x Grandmaster) shared how these resizing techniques helped them win a solo gold. This was in the petfinder Kaggle competition (2nd run of the comp) . . Note: Jeremy is running on a laptop with 4GB GPU. Jeremy says in GPU, just run one thing at a time else you will get CUDA error. . How to do fast.ai course . Tips for people in Yellow bucket: . Note: If you are in yellow, always stop try. First go ahead and watch video fully without touching your keyboard and write code. Then watch again and follow the course. This is an unusual way as it can&#8217;t be done in real college lectures, but it&#8217;s very effective way indeed. . I asked Wayde Gilliam who is a long term fastai community member after the lesson about his process of watching lectures. He was gracious enough to share it with mith . Important: 1. Watch the livestream and jot down timestamp to go back to for anything I found interesting in journal A (or just a piece of paper) . Important: 2. Go back through the video after 2-3 days, hit those spots I noted during the livestream. Will write detailed notes in another Journal (we&#8217;ll call that journal B) . Important: There&#8217;s too much info to digest in real-time so this approach works well and its what I&#8217;ve been doing for 4-5 yrs. . Huggingface spaces . Jeremy pointed to tanishq tutorial on Gradio + HuggingFace Spaces. . Also Jeremy mentioned some good tools which are useful: . Github Desktop: Hamel who was a employee in github previously, is even using github desktop. Some complicated stuff in git can be solved using this tool. Even knowing terminal is cool. | WSL: As a datascientist, you spend a lot of time in terminals. Just use ubuntu with windows terminal. Any time Jeremy shows in terminal, he just uses windows terminal. | In terminal, he uses Tmux as a terminal emulator as pointed out in fast.ai forums for my question. | . Jeremy like Windows due to easiness in streaming, good apps and recording capabilities. Yet Jeremy also has a linux environment with a good Deep learning jig. . Note: Jupyter notebooks debugging with magic methods %time, %debug . In fastai for inference, it returns back a tensor. One of issue in gradio tensors is not supported at moment. So we need to convert tensors to float and do prediction. . Jeremy created a cats vs dogs classifier using spaces. His daughter when realised he is building such a classifier googled something which is a mix of cat and dog. For that his initial prediction was like 50-50% for both cats and dogs. . This kind of shows how important the support system around you and how much they acknowledge the work you do. This personally touched me. As my sister was encouraging me to go an all-nighter to complete the Music genre classification spaces. . TODO: Look through Jeremy setup and how he worked with gradio in local [58:00 onwards 1:14:00] . fastsetup . Installing python and jupyter-notebooks with proper git and conda setup. . Fastai setup . Important: A big issue in laptops with linux or mac there is a python default version, don&#8217;t use that python. As that python version is for your operating system to do it&#8217;s stuff. Don&#8217;t mess on top of it. Use mamba based installation for fastai now: . mamba install fastai mamba install -c fastchan jupyter nbdev . Trying gradio API with github Pages . An example API in gradio Example Jeremy showcased . With live demo, we could have easily used it with any websites. Without any software just with the browser, you can run this file. That&#39;s the cool thing about javascript and can host in website called github pages . fetch(&#39;https://hf.space/embed/kurianbenoy/audioclassification/+/api/predict/&#39;, { method: &quot;POST&quot;, body: JSON.stringify({&quot;data&quot;:[ {&quot;data&quot;: null, &quot;is_example&quot;: true, &quot;name&quot;: &quot;000003.ogg&quot;} ]}), headers: { &quot;Content-Type&quot;: &quot;application/json&quot; }}) .then(function(response){ return response.json(); }) .then(function(json_response){ console.log(json_response) }) . . He used alembic theme. With a particular configuration. At top of any github pages, you should add three dashes. The world of javascript apps, he build this cool apps. . Important: The magic of using gradio APIs can be summarized as the following. It exposes a reliable way of sharing microservices. With this if you are just creating any hugging face spaces, with that APIs. You can use it any websites, apps etc. It looks to me there is no limitation with using Gradio API at the moment. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/05/03/fastai-52.html",
            "relUrl": "/fastai/fastbook/2022/05/03/fastai-52.html",
            "date": " ‚Ä¢ May 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Music genre classifier using fast.ai",
            "content": "During first lesson of Practical Deep Learning for Coders course, Jeremy had mentioned how using simple computer vision model we can build even a model to classify audio with image classification model itself. . Recently Kaggle grandmaster Rob Mulla conducted a challenge to classify music according to what genre it was. At stakes there was a RTX 3080 Ti GPU. Let&#39;s look how we can classify music genres using a simple computer vision model which was taught in the first lesson of fast.ai. . Downloading packages and importing libraries . . Note: I had already installed fastai, pytorch for training this model before hand. . ! pip install -Uqq kaggle git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[&quot;fastai&quot;] . . from fastai.data.all import * from fastai.imports import * from fastai.vision.all import * from huggingface_hub import push_to_hub_fastai . Collecting Data . In this piece of code, I will show you how you can download datasets from Kaggle in general and the datasets I had used for training model. Inorder to train models in audio, first convert the audio to a spectogram and throw an image model. Check this tweet from Dien Hoa Truong who won a NVIDIA RTX 3080 Ti GPU in this competition. This tweet makes me stick to the spectrogram approach for the Kaggle PogChamp music genre classification competition, and finish #1. Thanks @marktenenholtz :) https://t.co/xwtXQRfk51 . &mdash; Dien Hoa Truong (@DienhoaT) April 28, 2022 . For this competition you need two datasets: . The competition data | Image data generated from converting audio to melspectograms in form of images | The data provided here are over 20,000 royalty free song samples (30 second clips) and their musical genres. Your task is to create a machine learning algorithm capable of predicting the genres of unlabeled music files. Create features, design architectures, do whatever it takes to predict them the best. . The code for downloading data from kaggle has been adopted from Jeremy&#39;s notebook . creds = &quot;&quot; from pathlib import Path cred_path = Path(&quot;~/.kaggle/kaggle.json&quot;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(creds) cred_path.chmod(0o600) . path = Path(&quot;../input/kaggle-pog-series-s01e02&quot;) path.ls() . (#6) [Path(&#39;input/kaggle-pog-series-s01e02/genres.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/sample_submission.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/test.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/test&#39;),Path(&#39;input/kaggle-pog-series-s01e02/train.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/train&#39;)] . from zipfile import ZipFile from kaggle import api if not path.exists(): api.competition_download_cli(str(path)) ZipFile(f&quot;{path}.zip&quot;).extractall(path) . Downloading kaggle-pog-series-s01e02.zip to /home . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.05G/9.05G [07:54&lt;00:00, 20.5MB/s] . . . ! kaggle datasets download -d dienhoa/music-genre-spectrogram-pogchamps . Downloading music-genre-spectrogram-pogchamps.zip to /home 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 6.80G/6.80G [07:00&lt;00:00, 14.7MB/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.80G/6.80G [07:00&lt;00:00, 17.4MB/s] . Quick EDA and Data Cleaning . df_train = pd.read_csv(&quot;../input/kaggle-pog-series-s01e02/train.csv&quot;) df_train.head() . song_id filename filepath genre_id genre . 0 10150 | 010150.ogg | train/010150.ogg | 7 | Instrumental | . 1 7358 | 007358.ogg | train/007358.ogg | 2 | Punk | . 2 20573 | 020573.ogg | train/020573.ogg | 5 | Folk | . 3 11170 | 011170.ogg | train/011170.ogg | 12 | Old-Time / Historic | . 4 16662 | 016662.ogg | train/016662.ogg | 1 | Rock | . df_train[&quot;filepath&quot;] = df_train[&quot;filepath&quot;].str.replace(&quot;ogg&quot;, &quot;png&quot;) . Shows a highly imbalanced dataset . df_train[&quot;genre&quot;].value_counts() . Rock 3097 Electronic 3073 Punk 2584 Experimental 1801 Hip-Hop 1761 Folk 1215 Chiptune / Glitch 1181 Instrumental 1045 Pop 945 International 814 Ambient Electronic 796 Classical 495 Old-Time / Historic 408 Jazz 306 Country 142 Soul-RnB 94 Spoken 94 Blues 58 Easy Listening 13 Name: genre, dtype: int64 . df_train.head() . song_id filename filepath genre_id genre . 0 10150 | 010150.ogg | train/010150.png | 7 | Instrumental | . 1 7358 | 007358.ogg | train/007358.png | 2 | Punk | . 2 20573 | 020573.ogg | train/020573.png | 5 | Folk | . 3 11170 | 011170.ogg | train/011170.png | 12 | Old-Time / Historic | . 4 16662 | 016662.ogg | train/016662.png | 1 | Rock | . df_train = df_train.set_index(&quot;song_id&quot;) . df_train = df_train.drop( [ 23078, 3137, 4040, 15980, 11088, 9963, 24899, 16312, 22698, 17940, 22295, 3071, 13954, ] ) . df_train.shape . (19909, 4) . Loading Data using fastai DataLoaders . For creating this notebook, I spend a major portion of my time in cleaning and sorting out appropriate datablocks/dataloaders for training image models using fast.ai. This is something which you as a practitioner experience, compared to learning all the theory and backpropogation algorithm. . So let&#39;s see how we load this data using fast.ai. There are two approaches which we will discuss below. Both the approaches of loading data works, but the first approach as a disadvantage, which I will tell in a moment. . Approach 1. Using DataBlock and loading images . Create a data frame temp_train and create new column is_valid | is_valid is default column named created for using ColSplitter | Now set get_x which specifies the path of files for inputting data which is set as base_path+filename path: lambda o:f&#39;{path}/&#39;+o.path | Now set get_y which specifies the variable to predict, ie the genre of music | . temp_train = df_train temp_train.loc[:15000, &quot;is_valid&quot;] = True temp_train.loc[15000:, &quot;is_valid&quot;] = False . path = Path(&quot;../input/music-genre-spectrogram-pogchamps/spectograms/&quot;) . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), splitter=ColSplitter(), get_x=lambda o: f&quot;{path}/&quot; + o.path, get_y=lambda o: o.genre, item_tfms=Resize(224), batch_tfms=aug_transforms(), ) . dls = dblock.dataloaders(temp_train) . dls.show_batch() . # dblock.summary(df_train) . This worked really well, and with this approach I was even able to train a ML model which got 50% accuracy. Saturday evening side-project: Trained a baseline ML model to classify audio files to identify their music genre using @fastdotai based on a kaggle dataset.Acheived only 50% accuracy, probably because problem is hard. Next job is to check what @DienhoaT has done to win a GPU. pic.twitter.com/EahvgtYBDL . &mdash; Kurian Benoy (@kurianbenoy2) April 30, 2022 . Yet when it came to export models, due to usage of lamda method in DataBlock. I got Pickling error as the model was not able to be exported with learn.export() method. . 2. Using DataLoaders methods with loading from dataframe method . This issue got me into using approach that using ImageDataLoaders.from_df in fastai. Let&#39;s first take a look at our df_train dataframe: . df_train.head() . filename filepath genre_id genre . song_id . 10150 010150.ogg | train/010150.png | 7 | Instrumental | . 7358 007358.ogg | train/007358.png | 2 | Punk | . 20573 020573.ogg | train/020573.png | 5 | Folk | . 11170 011170.ogg | train/011170.png | 12 | Old-Time / Historic | . 16662 016662.ogg | train/016662.png | 1 | Rock | . If you look at the dataframe, we know that on appending to the path, the filepath column. . This is the exact value for get_x method in fastai fn_col = 1 which specifies the column name filepath at position 1. | label or get_y is specified by the column name genre at position 3. | valid_pct (ensure what percentage of data to be used for validation) | y_block=CategoryBlock to ensure it&#39;s used for normal classification only and not multi-label | . dls = ImageDataLoaders.from_df( df_train, path, valid_pct=0.2, seed=34, y_block=CategoryBlock, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224), fn_col=1, label_col=3, ) dls.show_batch() . Training fastai model . I trained using a resnet18 model at first, later we stepped up to use resnet50 model. . learn = vision_learner(dls, resnet50, metrics=error_rate) . learn.lr_find() . SuggestedLRs(valley=0.0008317637839354575) . learn.fine_tune(10, 0.0008317637839354575) . epoch train_loss valid_loss error_rate time . 0 | 2.869285 | 2.171426 | 0.616428 | 01:43 | . epoch train_loss valid_loss error_rate time . 0 | 2.312176 | 1.843815 | 0.558654 | 02:07 | . 1 | 2.102361 | 1.719162 | 0.539061 | 02:08 | . 2 | 1.867139 | 1.623988 | 0.527003 | 02:08 | . 3 | 1.710557 | 1.527913 | 0.507661 | 02:07 | . 4 | 1.629478 | 1.456836 | 0.479779 | 02:05 | . 5 | 1.519305 | 1.433036 | 0.474253 | 02:05 | . 6 | 1.457465 | 1.379757 | 0.464456 | 02:05 | . 7 | 1.396283 | 1.369344 | 0.457925 | 02:05 | . 8 | 1.359388 | 1.367973 | 0.453655 | 02:05 | . 9 | 1.364363 | 1.368887 | 0.456167 | 02:04 | . learn.export(&quot;model.pkl&quot;) . Pushing models to hugging face . huggingface_hub has released two new functions to easily push fastai models to Huggingface Hub. . Using push_to_hub_fastai you can easily push the fastai Learner to huggingface. | Additionally, you can load any fastai Learner from the Hub using from_pretrained_fastai | . Omar Espejel had shared a fantastic notebook on these new functionalities in huggingface here. . Note: You should install git-lfs and login to huggingface account with token before pushing . from huggingface_hub import push_to_hub_fastai push_to_hub_fastai( learn, &quot;kurianbenoy/music_genre_classification_baseline&quot;, commit_message=&quot;Resnet50 with 10 epochs of training&quot;, ) . /home/kurianbenoy/music_genre_classification_baseline is already a clone of https://huggingface.co/kurianbenoy/music_genre_classification_baseline. Make sure you pull the latest changes with `repo.git_pull()`. To https://huggingface.co/kurianbenoy/music_genre_classification_baseline 390320d..3605083 main -&gt; main . &#39;https://huggingface.co/kurianbenoy/music_genre_classification_baseline/commit/360508311005aefeb3ca29933f2173202afe4f30&#39; . If you want to load this model in fastai and use it directly for inference just from_pretrain_fastai as shown in the below screenshot: . . Taking a look at results . learn.show_results() . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(15, 10)) . Inference function . from fastai.vision.all import * from huggingface_hub import from_pretrained_fastai learn = from_pretrained_fastai(&quot;kurianbenoy/music_genre_classification_baseline&quot;) def predict(img): img = PILImage.create(img) _pred, _pred_w_idx, probs = learn.predict(img) labels_probs = {labels[i]: float(probs[i]) for i, _ in enumerate(labels)} return labels_probs . Conclusion . We trained a ML model which can identify with 54.4% accuracy to classify in a music file which genre it is. It&#39;s not so bad for a baseline model. Dien Hoa Truong has shared some techniques which he learned during Kaggle competition with music genres. [Machine Learning] Speed up your experiment so that you can try out a lot of ideas and find what works best for your problem. Below ‚¨áÔ∏è is what I have learned during the Kaggle competition with music genreüé∂: . &mdash; Dien Hoa Truong (@DienhoaT) April 30, 2022 . Thanks for reading :pray: .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/05/01/AudioCNNDemo.html",
            "relUrl": "/fastai/fastbook/2022/05/01/AudioCNNDemo.html",
            "date": " ‚Ä¢ May 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Practical Deep Learning for Coders Course - Lesson 1",
            "content": "First there was a set of introductions by university officials at UQ like VC. One curious thing was everyone of UQ staff were honouring something traditionaly of that land to live in reconciliation. . Then lecture of Jeremy starts, seeing his face the chatbox is in delight. . Jeremy mentions there are two categories of students who attend the course: . Students who have enrolled via University of Queensland(with almost 350 people attending in-person and about 100 people remotely as well). | fastai fellows who have acknowledged for their contribution to community. | Jeremy recommends having study buddies when we are learning the course is important. So he asks to create Study groups wherever possible. This course is now happening after a gap of 2 years, so there is a lot of new things which has to be covered as Deep learning moves so fast. . Using Dalle-2 technique we can generative creative images from generate twitter bios. For a creative person, this can be very helpful to create good artwork. Then one of another popular techniques was using Pathways language model which is able to answers question with explanations and even explains why some jokes are funny. . Jeremy talks about his interest in education.He is a homeschooler and learned from books by Paul Lockhart &amp; David Perkins which were inspiration for fast.ai. fastai teaches stuff in top-down manner. You will go into as much technical stuff as you, yet you will learn and implement cool stuff steadily. . About fast.ai course . He wrote an awesome book and this course. His book is one of the best sellers in Deep Learning and used to teach folks in companies like Tesla, OpenAI etc. Almost 6 million people watched his videos so far.Jeremy has won multiple competitions in Kaggle, was the CEO of kaggle. He build Enlitic, a medical company which was build for medical purpose with two other succesful startups. . Jeremy mentioned for this course, we are not using any material directly from Deep Learning For Coders with Fastai &amp; Pytorch book. Yet he recommends to read portions of book after each chapter. . Usually multiple people learn better if the same idea is exposed in different way from multiple sources. That&#39;s the why behind this approach. . Jeremy started coding hands-own a bird or park classifier, which was considered as a very difficult problem in 2015. Even a comic depicted this. Yet things have changed so drastically in past few years, that it&#39;s very easy to do that now. . Yet let&#39;s look, why we couldn&#39;t build a bird classifer in 2015:- For classifying histopothical images. They used computer vision techniques.- They got big team of datascientist, mathematicans with lot of features who build relevant feature for machine learning hand by hand. . These project took years | Also deep learning was not in radar for researchers then. | . What has now changed? . Using neural network they build these features on their own. | Mathew D Zeiler &amp; Rob Fergus(and actual weights) showed with visualization how neural networks work | Combine all features to learn and go slowly in past, neural networks learned on it&#39;s own these techniques. | . If it&#39;s a bird or not? notebook can be found here. I am slightly tweaking this model to leverage pytorch image-models released by timm. . urls = search_images(&#39;bird photos&#39;, max_images=1) urls[0] . from fastdownload import download_url dest = &#39;bird.jpg&#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) . Note: . Image based algorithms, are not for images. Image for music classification by Deolho, Ethan sutin sounds from image recognizer. You can do music classification, with some creativity using cnns. . | Also needing lots of data is a myth created by companies who sell data processng units. There are lot of free resources like Kaggle, Colab etc. . | . Observation by Jeremy:Tensorflow is slowly dying. Check this article which he cited. Yet pytorch has lot of hairy code, which can be solved using good abstractions in fastai. . fastai library tries to provide good and the best fine-tuned models, which work well compared to other libraries. He showed code required for implementing AdamW in pytorch and in fastai. | . Tanishq Abraham pointed me to implemtation of AdamW to chapter 16 in fastbook. . download_url(search_images(&#39;forest photos&#39;, max_images=1)[0], &#39;forest.jpg&#39;, show_progress=False) Image.open(&#39;forest.jpg&#39;).to_thumb(256,256) searches = &#39;forest&#39;,&#39;bird&#39; path = Path(&#39;bird_or_not&#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) resize_images(path/o, max_size=400, dest=path/o) . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . As the code showed, data cleaning is a big part of machine learninng. When we are learning this course as practitioners, we will spend lot of time of building and loading models. Like in compiler course lot of time is not spend on techniques, but on getting the environment up and ready. . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(224, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch() . After examining, 100s of project and datascience requirments. fastai came up with this approach of DataBlock, which consists of five things: . blocks | get_items | splitter | Batch_tfms(optional) | get_y | item_tfms | Without validation data, it won&#39;t allow to train. parent_label, return parent folder. we saved as forests or birds. We need same size. Idea to do quickly, why not publish vision_learners with pets dataset. . Now it&#39;s time to train our model . learn = vision_learner(dls, &#39;vit_tiny_patch16_224&#39;, metrics=error_rate) learn.fine_tune(10) . One thing which is cool is that the whole presentation is also made with Jupyter Notebooks using RiseJS. Also jupyter notebooks can be used for writing books like Deep Learning for Coders, for blogging using fastpages, for CI/CD pipeline to run in parallel execution in fastai repo. . Tanishq Mathew Abraham has summarized on what can be done in this twitter threads. Awesome and surprising things you can do with Jupyter Notebooks ‚¨á . &mdash; Tanishq Mathew Abraham (@iScienceLuvr) April 27, 2022 After this Jeremy, showed all the examples in Chapter 1 in Deep Learning for coders. My notes then: . We are still scratching the surface. Lot of marketing out there, some of first open source models available. The deep learning when it broke X, y, z in domain. In NLP it breaks lot of stuff . What&#39;s really go in on : in arthur samuel with graph. The graphs are build with gv2 in jupyter notebook. Deploying models in ML is a bit tricky. But it&#39;s just predict and shows results. . Conclusion by Jeremy . So after first lesson: . a) If you know python, then it&#39;s kind of easy for you. b) If don&#39;t know python, it&#39;s very difficult . Regardless of what level you are. Experiment yourself and do something more complex. Go ahead and push yourself a little bit, but not much. Then present your work. Do stuff on things where you are interested. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/04/26/fastai-51.html",
            "relUrl": "/fastai/fastbook/2022/04/26/fastai-51.html",
            "date": " ‚Ä¢ Apr 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Practical Deep Learning for Coders Course - Lesson 0",
            "content": "Last week I enrolled for the live cohort of Deep Learning For Coders with fastai coruse which is going to be taken by Jeremy Howard. It&#39;s&#39; a previlege at the same time, a dream come true moment for me, as a fastai student who took some part of fast.ai from the year 2018. . The fastai course has seen a lot of success stories over the years. Amazing folks like Aman Arora, Even Oldridge, Sanyam Bhutani,Radek Osmulski, Jason Antic, Wayde Gilliam, Zach Mueller ... While there are lot of hidden gems whom may not be so well know in community as well like Bhuvana who shared her journey in Pycon India 2019. The way this course has democratized AI &amp; ML is simply amazing. . Today Radek Osmulski joined NVIDIA AI. Today is my first day at @NVIDIAAI! ü•≥-From learning to code at 29-through learning ML @fastdotai-winning a @kaggle competition-jobs at üî• startups-moving continents thx to AI-to joining the illustrious Merlin team ‚ù§Ô∏èI am beyond grateful üôèWill make this one count! . &mdash; Radek Osmulski üá∫üá¶ (@radekosmulski) April 26, 2022 . It was Radek who had posted in fastai forums few years back on how he will approach taking the fastai course. Inspired by that I am creating this action list items on things which I am planning to achieve during duration of this course. . I will do following: . Try out every notebook given by Jeremy during class and writes in Kaggle | Write blogpost every week with lesson summary and occasionally on new things I learning during course. | Build some hugging face spaces using fastai. | Challenge myself to complete the existing project I am doing on IPL Commentary Summarizer and learn NLP extensively. | Attend the delft-fast-ai-study-group sessions atleast for five weeks. | Go to sleep before midnight | Dream and breath pytorch | Extra Credits: . Participate in NLP competition provided by jeremy link. | Frequent in fast.ai forums and discord group | Take a look at notebooks in Transformer Book to learn NLP extensively during class. | Try to read Pytorch book | Complete all the college assignments in Pytorch | I won&#39;t do: . Be active in twitter during course | Write or read EDA notebooks | Won&#39;t write any notebooks in tensorflow :) | Won&#39;t participate in any other kaggle comps during course period. | I will be not attending Real Python office hours, unless I have a very compelling doubt. | With that I am winding for today. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/04/26/fastai-50-ipynb.html",
            "relUrl": "/fastai/fastbook/2022/04/26/fastai-50-ipynb.html",
            "date": " ‚Ä¢ Apr 26, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Building a fine-tuned translation system for English-Malayalam",
            "content": "Hey, everyone. We all are familiar with translation systems like using google translate. So today, let&#39;s build a fine tuned translation system for converting text from english to malayalam. It&#39;s built using Blurr library - built on top of Hugging face and fast.ai made by Wayde Gilliam. Also our translation system is going to be fine tuned on top of KDE specific dataset. You can find the trained model here. . . Installation . ! python3 -m pip install -Uqq datasets== fastai==2.6.3 ! python3 -m pip install -Uqq transformers[sentencepiece] ! python3 -m pip install -Uqq ohmeow-blurr==1.0.5 ! python3 -m pip install -Uqq nltk ! python3 -m pip install -Uqq sacrebleu ! python3 -m pip install -Uqq git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[&quot;fastai&quot;] . . ERROR: Could not find a version that satisfies the requirement datasets== (from versions: 0.0.9, 1.0.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.2, 1.13.3, 1.14.0, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.17.0, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 2.0.0, 2.1.0, 2.2.0) ERROR: No matching distribution found for datasets== . . Loading Data . A translation system is an example of sequence to sequence models, which is usually used for tasks which involves generating new data. Translation usually needs datasets in both the source language and target language (the language to which it needs to be translated). . We are using KDE4 datasets, and choose both source language and translation language as english and malayalam respectively. Usually these datasets are curated by community volunteers to their native language, and this was probably done by KDE community volunteers in Kerala. When someone is localizing these texts into there in local languague, usually computer science specific terms are still written in english. . import pandas from datasets import load_dataset . raw_datasets = load_dataset(&quot;kde4&quot;, lang1=&quot;en&quot;, lang2=&quot;ml&quot;, split=&quot;train[:1000]&quot;) . Using custom data configuration en-ml-lang1=en,lang2=ml Reusing dataset kde4 (/home/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac) . Most of translation dataset is in form of id and translation json output - with both en and ml as objects. . raw_datasets[0] . {&#39;id&#39;: &#39;0&#39;, &#39;translation&#39;: {&#39;en&#39;: &#39;Add Feed to Akregator&#39;, &#39;ml&#39;: &#39;‡¥Ö‡¥ï‡µç‡¥∞‡¥ø‡¥ó‡µá‡¥±‡µç‡¥±‡¥±‡¥ø‡¥≤‡µç u200d ‡¥´‡µÄ‡¥°‡µç ‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µç u200d‡¥ï‡µç‡¥ï‡µÅ‡¥ï&#39;}} . . Transforming data into DataLoaders . Importing libraries and get hugging-face objects . from blurr.text.data.all import * from blurr.text.modeling.all import * from blurr.text.utils import * from fastai.data.all import * from fastai.callback.all import * from fastai.learner import load_learner, Learner from fastai.optimizer import * from transformers import * . pretrained_model_name = &quot;Helsinki-NLP/opus-mt-en-ml&quot; model_cls = AutoModelForSeq2SeqLM hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=model_cls) hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model) . translation_df = pd.DataFrame(raw_datasets[&quot;translation&quot;], columns=[&quot;en&quot;, &quot;ml&quot;]) translation_df.head() . en ml . 0 Add Feed to Akregator | ‡¥Ö‡¥ï‡µç‡¥∞‡¥ø‡¥ó‡µá‡¥±‡µç‡¥±‡¥±‡¥ø‡¥≤‡µç‚Äç ‡¥´‡µÄ‡¥°‡µç ‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µç‚Äç‡¥ï‡µç‡¥ï‡µÅ‡¥ï | . 1 Add Feeds to Akregator | ‡¥Ö‡¥ï‡µç‡¥∞‡¥ø‡¥ó‡µá‡¥±‡µç‡¥±‡¥±‡¥ø‡¥≤‡µç‚Äç ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µç‚Äç ‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µç‚Äç‡¥ï‡µç‡¥ï‡µÅ‡¥ï | . 2 Add All Found Feeds to Akregator | ‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥Ö‡¥ï‡µç‡¥∞‡¥ø‡¥ó‡µá‡¥±‡µç‡¥±‡¥±‡¥ø‡¥≤‡µç‚Äç ‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µç‚Äç‡¥ï‡µç‡¥ï‡µÅ‡¥ï | . 3 Subscribe to site updates (using news feed) | ‡¥∏‡µà‡¥±‡µç‡¥±‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µÜ ‡¥™‡µÅ‡¥§‡µÅ‡¥Æ‡¥ï‡¥≥‡¥±‡¥ø‡¥Ø‡¥æ‡¥®‡µç‚Äç ‡¥µ‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥∞‡¥®‡¥æ‡¥ï‡µÅ‡¥ï (‡¥µ‡¥æ‡¥∞‡µç‚Äç‡¥§‡µç‡¥§‡¥æ ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µç‚Äç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡µç) | . 4 Imported Feeds | ‡¥é‡¥ü‡µÅ‡¥§‡µç‡¥§ ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µç‚Äç | . blocks = (Seq2SeqTextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), noop) dblock = DataBlock(blocks=blocks, get_x=ColReader(&quot;en&quot;), get_y=ColReader(&quot;ml&quot;), splitter=RandomSplitter()) . dls = dblock.dataloaders(translation_df, bs=16) dls.show_batch(dataloaders=dls, max_n=2, input_trunc_at=100, target_trunc_at=250) . text target . 0 A‚ñÅversion‚ñÅcontrol‚ñÅhistory‚ñÅentry‚ñÅconsists of‚ñÅseveral‚ñÅlines.‚ñÅSpecify the‚ñÅregular‚ñÅexpression‚ñÅto‚ñÅdetect | ‡¥í‡¥∞‡µÅ ‡¥≠‡¥æ‡¥∑‡¥æ‡¥®‡µç‡¥§‡¥∞ ‡¥®‡¥ø‡¥Ø‡¥®‡µç‡¥§‡µç‡¥∞‡¥£‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥®‡¥æ‡¥≥‡µç‡¥µ‡¥¥‡¥ø ‡¥ö‡µá‡¥∞‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡¥≤‡µç ‡¥™‡¥≤ ‡¥µ‡¥∞‡¥ø‡¥ï‡¥≥‡µÅ‡¥£‡µç‡¥ü‡¥æ‡¥ï‡µÅ‡¥Ç. ‡¥Ü‡¥¶‡µç‡¥Ø‡¥§‡µç‡¥§‡µÜ ‡¥µ‡¥∞‡¥ø ‡¥ï‡¥£‡µç‡¥ü‡µÅ‡¥™‡¥ø‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥®‡¥ø‡¥§‡µç‡¥Ø‡¥≠‡¥æ‡¥µ‡¥Ç ‡¥®‡¥ø‡¥∞‡µç‡¥¶‡µç‡¥¶‡µá‡¥∂‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï (‡¥Æ‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µÜ ‡¥µ‡¥ø‡¥∂‡¥¶‡µÄ‡¥ï‡¥∞‡¥£‡¥Ç ‡¥ï‡µÇ‡¥ü‡¥æ‡¥§‡µÜ). ‡¥á‡¥®‡¥Ç ‡¥§‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥ï‡µÄ‡¥ï‡¥≥‡µÜ ‡¥í‡¥®‡µç‡¥®‡¥ø‡¥ö‡µç‡¥ö‡¥æ‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µç ‡¥¨‡µç‡¥∞‡¥æ‡¥ï‡µç‡¥ï‡¥±‡µç‡¥±‡µÅ‡¥ï‡¥≥‡µç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï. ‡¥í‡¥¥‡¥ø‡¥ö‡µç‡¥ö‡µÅ ‡¥µ‡¥ø‡¥ü‡µç‡¥ü | . 1 ‚ñÅMailody‚ñÅcan‚ñÅstore‚ñÅall‚ñÅattchements of‚ñÅall‚ñÅmessages in‚ñÅa‚ñÅcertain‚ñÅfolder.‚ñÅThen‚ñÅyou‚ñÅnever‚ñÅhave‚ñÅto‚ñÅsave‚ñÅ | ‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µá‡¥Ø‡µÅ‡¥Ç ‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥Ö‡¥®‡µÅ‡¥¨‡¥®‡µç‡¥ß‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥Ç ‡¥í‡¥∞‡µÅ ‡¥™‡µç‡¥∞‡¥§‡µç‡¥Ø‡µá‡¥ï ‡¥Ö‡¥±‡¥Ø‡¥ø‡¥≤‡µç ‡¥∏‡µÇ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µç ‡¥Æ‡µÜ‡¥Ø‡¥ø‡¥≤‡¥°‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç. ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡¥µ‡¥Ø‡µÜ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥ô‡µç‡¥ô‡¥≥‡¥ø‡¥≤‡µç‡¥®‡¥ø‡¥®‡µç‡¥®‡µç ‡¥™‡µç‡¥∞‡¥§‡µç‡¥Ø‡µá‡¥ï‡¥Ç ‡¥∏‡µÇ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥£‡µç‡¥ü‡¥§‡¥ø‡¥≤‡µç‡¥≤. ‡¥Ö‡¥µ ‡¥Ö‡¥±‡¥Ø‡¥ø‡¥≤‡µç ‡¥â‡¥£‡µç‡¥ü‡¥æ‡¥Ø‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Ç. ‡¥™‡µç‡¥∞‡¥§‡µç‡¥Ø‡µá‡¥ï‡¥Ç ‡¥∂‡µç‡¥∞‡¥¶‡µç‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï, ‡¥à ‡¥Ö‡¥± ‡¥á‡¥ü‡¥ï‡µç‡¥ï‡¥ø‡¥ü‡¥ï‡µç‡¥ï‡µç ‡¥ï‡¥æ‡¥≤‡¥ø‡¥Ø‡¥æ‡¥ï‡µç‡¥ï‡¥ø‡¥ï‡µç‡¥ï‡µä‡¥£‡µç‡¥ü‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï | . . Training fine-tuned translation system . Using blurr High-level API . Bugs in ohmeow v1.0.4 has been fixed by the open-source maintainer. . from blurr.text.utils import BlurrText NLP = BlurrText() . learn = BlearnerForTranslation.from_data( translation_df, pretrained_model_name, src_lang_name=&quot;English&quot;, src_lang_attr=&quot;en&quot;, trg_lang_name=&quot;Malayalam&quot;, trg_lang_attr=&quot;ml&quot;, dl_kwargs={&quot;bs&quot;: 16}, ) . metrics_cb = BlearnerForTranslation.get_metrics_cb() learn.fit_one_cycle(1, lr_max=4e-5, cbs=[metrics_cb]) . [nltk_data] Downloading package wordnet to /home/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /home/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package omw-1.4 to /home/nltk_data... [nltk_data] Package omw-1.4 is already up-to-date! . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 5.512897 | 4.821253 | 0.023251 | 0.158193 | 4.086147 | 00:19 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=250) . text target prediction . 0 ‚ñÅMailody‚ñÅis‚ñÅable‚ñÅto‚ñÅconvert‚ñÅyour‚ñÅplain‚ñÅmessage‚ñÅto‚ñÅa‚ñÅhtml‚ñÅmessage‚ñÅand‚ñÅinclude‚ñÅthat in the‚ñÅoutgoing‚ñÅmessage.‚ñÅThis‚ñÅmeans the‚ñÅreceiver‚ñÅwill‚ñÅalso‚ñÅhave‚ñÅclickable‚ñÅlinks‚ñÅand‚ñÅcolored‚ñÅquote‚ñÅlevels. | ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∏‡¥æ‡¥¶‡¥æ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥é‡¥ö‡µç‡¥ö‡µç‡¥ü‡¥ø‡¥é‡¥Ç‡¥é‡¥≤‡µç ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥ø ‡¥Æ‡¥æ‡¥±‡µç‡¥±‡¥ø ‡¥Ö‡¥§‡µç ‡¥™‡µÅ‡¥±‡¥§‡µç‡¥§‡µá‡¥ï‡µç‡¥ï‡µç ‡¥Ö‡¥Ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥§‡µç‡¥§‡¥ø‡¥≤‡µç ‡¥â‡¥≥‡µç‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡¥æ‡¥®‡µç ‡¥Æ‡µÜ‡¥Ø‡¥ø‡¥≤‡¥°‡¥ø‡¥ï‡µç‡¥ï‡µÅ ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç. ‡¥Ö‡¥§‡¥æ‡¥Ø‡¥§‡µç ‡¥û‡µä‡¥ü‡µç‡¥ü‡¥æ‡¥µ‡µÅ‡¥®‡µç‡¥® ‡¥ï‡¥£‡µç‡¥£‡¥ø‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥µ‡¥∞‡µç‡¥£‡µç‡¥£ ‡¥â‡¥¶‡µç‡¥ß‡¥∞‡¥£‡¥ø ‡¥§‡¥≤‡¥µ‡µÅ‡¥Ç ‡¥∏‡µç‡¥µ‡µÄ‡¥ï‡¥∞‡µç‡¥§‡µç‡¥§‡¥æ‡¥µ‡¥ø‡¥®‡µÅ‡¥ï‡µÇ‡¥ü‡¥ø ‡¥≤‡¥≠‡µç‡¥Ø‡¥Æ‡¥æ‡¥µ‡µÅ‡¥Ç | [‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∏‡¥Æ‡µç‡¥™‡¥æ‡¥¶‡¥® ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥í‡¥∞‡µÅ html ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥ø ‡¥Æ‡¥æ‡¥±‡µç‡¥±‡µÅ‡¥µ‡¥æ‡¥®‡µÅ‡¥Ç ‡¥™‡µÅ‡¥±‡¥§‡µç‡¥§‡µÅ‡¥≥‡µç‡¥≥ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥â‡µæ ‡¥ï‡µç‡¥ï‡µä‡¥≥‡µç‡¥≥‡µÅ‡¥®‡µç‡¥® ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥â‡µæ‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡µÅ‡¥µ‡¥æ‡¥®‡µÅ‡¥Ç Middi‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡µç ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç. ‡¥á‡¥§‡¥ø‡¥®‡µº‡¥§‡µç‡¥•‡¥Ç ‡¥±‡¥ø‡¥ï‡µç‡¥ï‡µã‡µº‡¥°‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥æ‡¥µ‡µÅ‡¥®‡µç‡¥® ‡¥ï‡¥£‡µç‡¥£‡¥ø‡¥ï‡¥≥‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥®‡¥ø‡¥±‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥µ‡¥≤‡¥ï‡µç‡¥ï‡µÜ‡¥ü‡µç‡¥ü‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥®‡¥ø‡¥±‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡µÅ‡µç., ‡¥™‡¥§‡¥ø‡¥™‡µç‡¥™‡µç ‡¥®‡¥ø‡¥Ø‡¥®‡µç‡¥§‡µç‡¥∞‡¥ø‡¥§ ‡¥ö‡¥∞‡¥ø‡¥§‡µç‡¥∞‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥™‡µç‡¥∞‡¥æ‡¥∞‡¥Ç‡¥≠‡¥Æ‡¥æ‡¥Ø‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥∏‡¥æ‡¥ß‡¥æ‡¥∞‡¥£ ‡¥™‡µç‡¥∞‡¥Ø‡µã‡¥ó‡¥Ç. ‡¥∏‡¥æ‡¥ß‡¥æ‡¥∞‡¥£‡¥Ø‡¥æ‡¥Ø‡¥ø ‡¥à ‡¥µ‡¥∞‡¥ø‡¥Ø‡¥ø‡µΩ &quot;$2Loux&quot; ‡¥ï‡µÄ‡¥µ‡¥æ‡¥§‡¥ï‡¥Ç ‡¥â‡¥£‡µç‡¥ü‡µç. ‡¥∏‡µç‡¥µ‡¥§‡¥µ‡µá‡¥Ø‡µÅ‡¥≥‡µç‡¥≥ ‡¥Æ‡µÇ‡¥≤‡µç‡¥≤‡µç‡¥Ø‡¥Ç: description from play played for play play for play for play play play for filme cout fillulume for for courtyourtime fume time ck., ‡¥§‡µÅ‡¥ü‡¥ô‡µç‡¥ô‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡¥®‡¥æ‡¥Ø‡¥ø, &quot;‡¥™‡µÅ‡¥§‡¥ø‡¥Ø&quot; ‡¥§‡µÜ‡¥∞‡¥û‡µç‡¥û‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡µç ‡¥Ü‡¥¶‡µç‡¥Ø‡¥Ç ‡¥í‡¥∞‡µÅ ‡¥™‡µÅ‡¥§‡¥ø‡¥Ø ‡¥í‡¥™‡µç‡¥™‡µç ‡¥â‡¥£‡µç‡¥ü‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï. ‡¥Ö‡¥™‡µç‡¥™‡µã‡µæ ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µÅ‡µç ‡¥§‡¥ø‡¥∞‡µÅ‡¥§‡µç‡¥§‡¥æ‡¥®‡µÅ‡¥Ç ‡¥í‡¥™‡µç‡¥™‡µÅ‡¥ï‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∂‡µá‡¥ñ‡¥∞‡¥Ç ‡¥∏‡¥Ç‡¥∞‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥Ç ‡¥∏‡¥æ‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Ç., ‡¥§‡¥ø‡¥∞‡µÅ‡¥§‡µç‡¥§‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥§ ‡¥´‡¥Ø‡µΩ ‡¥∏‡µÇ‡¥ï‡µç‡¥∑‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡µæ ‡¥µ‡¥∞‡¥ø‡¥Ø‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥ô‡µç‡¥ô‡µæ ‡¥∏‡¥ú‡µç‡¥ú‡µÄ‡¥ï‡¥∞‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥°‡µã‡¥é‡¥∏‡µç/ ‡¥ú‡¥æ‡¥≤‡¥ï‡¥ô‡µç‡¥ô‡µæ: CRS+LLF; ‡¥Ø‡µÅ‡¥é‡¥´‡µç: LRIFX; ‡¥í‡¥™‡µç‡¥™‡¥Ç CRL++D=0, LRD=0A, LRFAA +0A, ‡¥Æ‡µÅ‡µª‡¥ï‡¥æ‡¥¥‡µç‡¥ö‡¥ï‡µæ ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µΩ ‡¥™‡¥∞‡¥æ‡¥ú‡¥Ø‡¥Ç. ‡¥à ‡¥ï‡¥Æ‡¥æ‡µª‡¥°‡µç ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥ï:% 1 ‡¥é‡¥®‡µç‡¥® ‡¥Ü‡¥ú‡µç‡¥û ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥∞‡¥π‡¥ø‡¥§‡¥Æ‡¥æ‡¥Ø‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Ç., &quot;Subject&quot; ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ &#39;suck&#39; ‡¥í‡¥∞‡µÅ ‡¥Ü‡¥¥‡¥§‡µç‡¥§‡¥ø‡¥≤‡µÅ‡¥≥‡µç‡¥≥ ‡¥Ö‡¥µ‡¥∏‡µç‡¥•‡¥Ø‡¥æ‡¥£‡µç, ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥Ç ‡¥™‡µÇ‡µº‡¥£‡µç‡¥£‡¥Æ‡¥æ‡¥Ø‡µÅ‡¥Ç ‡¥Ö‡¥ß‡¥ø‡¥ï‡¥æ‡¥∞‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥ï‡µä‡¥£‡µç‡¥ü‡µÅ‡¥µ‡¥∞‡µÅ‡¥®‡µç‡¥®‡µÅ, ‡¥∏‡¥∏‡µç‡¥™‡µÜ‡µª‡¥°‡µç ‡¥í‡¥∞‡µÅ ‡¥®‡¥ø‡¥¶‡µç‡¥∞‡¥æ ‡¥∏‡¥Ç‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥Æ‡¥æ‡¥£‡µç, ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥Ç ‡¥ä‡µº‡¥ú‡µç‡¥ú‡¥Ç ‡¥ï‡µÅ‡¥±‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥≥‡µç ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Ç ‡¥ä‡µº‡¥ú‡µç‡¥ú‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Æ‡µá ‡¥â‡¥≥‡µç‡¥≥‡µÇ., ‡¥à ‡¥Æ‡µÅ‡µª‡¥ï‡¥∞‡µÅ‡¥§‡µΩ ‡¥≤‡µà‡¥®‡¥ø‡¥Ç‡¥ó‡µç ‡¥≤‡µà‡¥®‡¥ø‡¥Ç‡¥ó‡µç ‡¥≤‡µà‡¥®‡¥ø‡¥Ç‡¥ó‡µç ‡¥µ‡µá‡¥≥‡¥Ø‡¥ø‡µΩ ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Æ‡µá ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥Æ‡µÅ‡¥≥‡µç‡¥≥‡µÅ. ( ‡¥µ‡¥ø‡¥∂‡¥¶‡¥æ‡¥Ç‡¥∂‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥°‡µã‡¥ï‡µç‡¥∏‡µç ‡¥ï‡¥æ‡¥£‡µÅ‡¥ï.), ‡¥´‡¥Ø‡¥≤‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥™‡¥ï‡µº‡¥™‡µç‡¥™‡µç ‡¥™‡µç‡¥∞‡¥ï‡µç‡¥∞‡¥ø‡¥Ø‡¥Ø‡¥ø‡¥≤‡µç ‡¥™‡¥ø‡¥∂‡¥ï‡µç: ‡¥µ‡¥æ‡¥Ø‡¥®‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥≥‡µç‡¥≥ ‡¥´‡¥Ø‡µΩ ‡¥§‡µÅ‡¥±‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡µΩ ‡¥™‡¥∞‡¥æ‡¥ú‡¥Ø‡¥Ç:% 1, ‚ñ™ ‡¥Æ‡¥æ‡¥Æ‡µã‡¥¶‡µÄ‡¥∏ ‡¥§‡µÅ‡¥±‡¥®‡µç‡¥®‡µÅ ‡¥®‡µã‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡µÅ‡¥Ç ‡¥Ö‡¥ü‡¥Ø‡µç ‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡µÅ‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥®‡µç‡¥®‡¥§‡µç ‡¥ï‡µç‡¥∞‡¥Æ‡¥Æ‡¥æ‡¥Ø ‡¥™‡µç‡¥∞‡¥Ø‡µã‡¥ó‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥ö‡µá‡¥∞‡µÅ‡¥ï‡¥Ø‡¥ø‡¥≤‡µç‡¥≤., ‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡¥æ‡¥ï‡µç‡¥ï‡µæ‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥Ü‡¥™‡µç‡¥™‡¥ø‡µæ‡¥ü‡µç‡¥ü‡µÅ‡¥ï‡µæ ‡¥á‡µª‡¥∏‡µç‡¥±‡µç‡¥±‡µã‡µæ ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï‡¥Ø‡µã ‡¥®‡µÄ‡¥ï‡µç‡¥ï‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï‡¥Ø‡µã ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï., ‡¥´‡¥Ø‡¥≤‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥™‡¥ï‡µº‡¥™‡µç‡¥™‡µç ‡¥™‡µç‡¥∞‡¥ï‡µç‡¥∞‡¥ø‡¥Ø‡¥Ø‡¥ø‡¥≤‡µç ‡¥™‡¥ø‡¥∂‡¥ï‡µç: ‡¥µ‡¥æ‡¥Ø‡¥® ‡¥™‡¥∞‡¥æ‡¥ú‡¥Ø‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü‡µÅ:% 1, ‡¥µ‡µà‡¥∞‡µÅ‡¥¶‡µç‡¥ß‡µç‡¥Ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥é‡¥£‡µç‡¥£‡¥Ç ‡¥∏‡¥Ç‡¥¨‡¥®‡µç‡¥ß‡¥ø‡¥ö‡µç‡¥ö‡µç ‡¥í‡¥∞‡µÅ ‡¥∏‡¥Ç‡¥µ‡¥æ‡¥¶‡¥Ç ‡¥ï‡¥æ‡¥£‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï., ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥¨‡¥æ‡¥±‡µç‡¥±‡¥±‡¥ø ‡¥Ö‡¥ß‡¥ø‡¥ï‡¥æ‡¥∞‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥®‡¥ø‡¥®‡µç‡¥®‡µÅ‡¥Ç ‡¥ì‡¥ü‡¥ø ‡¥∞‡¥ï‡µç‡¥∑‡¥™‡µÜ‡¥ü‡¥æ‡µª ‡¥™‡µã‡¥µ‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç, ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥í‡¥®‡µç‡¥®‡µÅ‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥æ‡¥®‡¥ø‡¥≤‡µç‡¥≤., ‡¥µ‡¥ø‡¥≤‡¥æ‡¥∏‡¥ô‡µç‡¥ô‡¥≥‡µç ‡¥ö‡µá‡µº‡¥§‡µç‡¥§‡¥ø‡¥ü‡µç‡¥ü‡¥ø‡¥≤‡µç‡¥≤. ‡¥Ö‡¥Ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡¥®‡µç ‡¥Æ‡µÅ‡¥Æ‡µç‡¥™‡µç ‡¥ï‡µÅ‡¥±‡¥û‡µç‡¥û‡¥§‡µç ‡¥í‡¥®‡µç‡¥®‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡¥≤‡µÅ‡¥Ç ‡¥ö‡µá‡µº‡¥ï‡µç‡¥ï‡µÇ., ‡¥ï‡µç‡¥∑‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥´‡µã‡¥£‡µ∫ ‡¥™‡¥§‡¥ø‡¥™‡µç‡¥™‡µç ‡¥™‡¥ø‡¥®‡µç‡¥§‡µÅ‡¥£‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µç‡¥≤.] | . Using mid-level of blurr APIs . b = dls.one_batch() . len(b), b[0][&quot;input_ids&quot;].shape, b[1].shape . (2, torch.Size([16, 72]), torch.Size([16, 114])) . dls.show_batch(dataloaders=dls, input_trunc_at=250, target_trunc_at=250) . text target . 0 A‚ñÅversion‚ñÅcontrol‚ñÅhistory‚ñÅentry‚ñÅconsists of‚ñÅseveral‚ñÅlines.‚ñÅSpecify the‚ñÅregular‚ñÅexpression‚ñÅto‚ñÅdetect the‚ñÅfirst‚ñÅline (without the‚ñÅleading‚ñÅcomment).‚ñÅUse‚ñÅparentheses‚ñÅto‚ñÅgroup the‚ñÅkeys‚ñÅyou‚ñÅwant‚ñÅto‚ñÅuse‚ñÅfor‚ñÅsorting.‚ñÅIf‚ñÅleft‚ñÅempty,‚ñÅthen‚ñÅKDiff3‚ñÅassumes‚ñÅthat‚ñÅe | ‡¥í‡¥∞‡µÅ ‡¥≠‡¥æ‡¥∑‡¥æ‡¥®‡µç‡¥§‡¥∞ ‡¥®‡¥ø‡¥Ø‡¥®‡µç‡¥§‡µç‡¥∞‡¥£‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥®‡¥æ‡¥≥‡µç‡¥µ‡¥¥‡¥ø ‡¥ö‡µá‡¥∞‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡¥≤‡µç ‡¥™‡¥≤ ‡¥µ‡¥∞‡¥ø‡¥ï‡¥≥‡µÅ‡¥£‡µç‡¥ü‡¥æ‡¥ï‡µÅ‡¥Ç. ‡¥Ü‡¥¶‡µç‡¥Ø‡¥§‡µç‡¥§‡µÜ ‡¥µ‡¥∞‡¥ø ‡¥ï‡¥£‡µç‡¥ü‡µÅ‡¥™‡¥ø‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥®‡¥ø‡¥§‡µç‡¥Ø‡¥≠‡¥æ‡¥µ‡¥Ç ‡¥®‡¥ø‡¥∞‡µç‡¥¶‡µç‡¥¶‡µá‡¥∂‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï (‡¥Æ‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µÜ ‡¥µ‡¥ø‡¥∂‡¥¶‡µÄ‡¥ï‡¥∞‡¥£‡¥Ç ‡¥ï‡µÇ‡¥ü‡¥æ‡¥§‡µÜ). ‡¥á‡¥®‡¥Ç ‡¥§‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥ï‡µÄ‡¥ï‡¥≥‡µÜ ‡¥í‡¥®‡µç‡¥®‡¥ø‡¥ö‡µç‡¥ö‡¥æ‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µç ‡¥¨‡µç‡¥∞‡¥æ‡¥ï‡µç‡¥ï‡¥±‡µç‡¥±‡µÅ‡¥ï‡¥≥‡µç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï. ‡¥í‡¥¥‡¥ø‡¥ö‡µç‡¥ö‡µÅ ‡¥µ‡¥ø‡¥ü‡µç‡¥ü | . 1 ‚ñÅThere‚ñÅis‚ñÅno‚ñÅInbox‚ñÅfound in‚ñÅany‚ñÅresource.‚ñÅStarting‚ñÅa‚ñÅnew‚ñÅmessage‚ñÅwill‚ñÅcause the‚ñÅmessage‚ñÅto‚ñÅbe‚ñÅlost‚ñÅafter‚ñÅyou‚ñÅhave‚ñÅsent‚ñÅit.‚ñÅYou‚ñÅwill‚ñÅnot‚ñÅhave‚ñÅa‚ñÅlocal‚ñÅcopy‚ñÅanymore.‚ñÅIf‚ñÅyou‚ñÅwant‚ñÅa‚ñÅcopy,‚ñÅone‚ñÅway‚ñÅto‚ñÅdo‚ñÅthis‚ñÅis‚ñÅto‚ñÅadd‚ñÅyourself‚ñÅas‚ñÅa CC‚ñÅto the‚ñÅmessage.&lt;pad&gt;&lt; | ‡¥µ‡¥ø‡¥≠‡¥µ‡¥ô‡µç‡¥ô‡¥≥‡¥ø‡¥≤‡µä‡¥®‡µç‡¥®‡µÅ‡¥Ç ‡¥í‡¥∞‡µÅ ‡¥á‡¥®‡µç‡¥¨‡µã‡¥ï‡µç‡¥∏‡µç ‡¥ï‡¥æ‡¥£‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µç‡¥≤. ‡¥í‡¥∞‡µÅ ‡¥™‡µÅ‡¥§‡¥ø‡¥Ø ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥§‡µÅ‡¥ü‡¥ô‡µç‡¥ô‡µÅ‡¥®‡µç‡¥®‡¥§‡µç ‡¥Ö‡¥§‡µç ‡¥Ö‡¥Ø‡¥ö‡µç‡¥ö ‡¥∂‡µá‡¥∑‡¥Ç ‡¥®‡¥∑‡µç‡¥ü‡¥™‡µç‡¥™‡µÜ‡¥ü‡¥æ‡¥®‡µç ‡¥ï‡¥æ‡¥∞‡¥£‡¥Æ‡¥æ‡¥ï‡µÅ‡¥Ç. ‡¥™‡µç‡¥∞‡¥æ‡¥¶‡µá‡¥∂‡¥ø‡¥ï ‡¥™‡¥ï‡¥∞‡µç‡¥™‡µç‡¥™‡µÅ‡¥ï‡¥≥‡µä‡¥®‡µç‡¥®‡µÅ‡¥Ç ‡¥í‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥≤‡µÅ‡¥Ç ‡¥≤‡¥≠‡µç‡¥Ø‡¥Æ‡¥≤‡µç‡¥≤‡¥æ‡¥§‡¥æ‡¥µ‡µÅ‡¥Ç. ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥í‡¥∞‡µÅ ‡¥™‡¥ï‡¥∞‡µç‡¥™‡µç‡¥™‡µç ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥Æ‡µÅ‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡¥≤‡µç ‡¥í‡¥∞‡µÅ ‡¥ï‡¥æ‡¥∞‡µç‡¥¨‡¥£‡µç ‡¥™‡¥§‡¥ø‡¥™‡µç‡¥™‡µÅ‡¥ï‡µÇ‡¥ü‡¥ø ‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µç‡¥§‡µç‡¥§‡¥§‡¥æ | . 2 ‚ñÅTo‚ñÅprevent‚ñÅdata‚ñÅloss‚ñÅor‚ñÅother‚ñÅdamage,‚ñÅyou‚ñÅcan‚ñÅhave the‚ñÅsystem suspend‚ñÅor‚ñÅhibernate,‚ñÅso‚ñÅyou‚ñÅdo‚ñÅnot‚ñÅaccidentally‚ñÅrun‚ñÅout of‚ñÅbattery‚ñÅpower.‚ñÅConfigure the‚ñÅnumber of‚ñÅminutes‚ñÅbelow‚ñÅwhich the‚ñÅmachine‚ñÅwill‚ñÅrun the‚ñÅconfigured‚ñÅaction.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt; | ‡¥µ‡¥ø‡¥µ‡¥∞‡¥®‡¥∑‡µç‡¥ü‡¥Æ‡µã ‡¥π‡¥æ‡¥®‡¥ø‡¥Ø‡µã ‡¥§‡¥ü‡¥Ø‡¥æ‡¥®‡µç, ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥Ç ‡¥Æ‡¥Ø‡¥ô‡µç‡¥ô‡µÅ‡¥ï‡¥Ø‡µã ‡¥∂‡¥ø‡¥∂‡¥ø‡¥∞‡¥®‡¥ø‡¥¶‡µç‡¥∞‡¥Ø‡¥ø‡¥≤‡¥æ‡¥ï‡µÅ‡¥ï‡¥Ø‡µã‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥æ‡¥µ‡µÅ‡¥®‡µç‡¥®‡¥§‡¥æ‡¥£‡µç, ‡¥Ö‡¥ô‡µç‡¥ô‡¥®‡µÜ ‡¥Ü‡¥ï‡¥∏‡µç‡¥Æ‡¥ø‡¥ï‡¥Æ‡¥æ‡¥Ø‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥™‡µã‡¥≤‡µÅ‡¥Ç ‡¥¨‡¥æ‡¥±‡µç‡¥±‡¥±‡¥ø ‡¥ä‡¥∞‡µç‡¥ú‡µç‡¥ú‡¥Ç ‡¥§‡µÄ‡¥∞‡¥æ‡¥§‡¥ø‡¥∞‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥Ç. ‡¥ï‡µç‡¥∞‡¥Æ‡µÄ‡¥ï‡¥∞‡¥ø‡¥ö‡µç‡¥ö ‡¥®‡¥ü‡¥™‡¥ü‡¥ø‡¥Ø‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥Æ‡µÅ‡¥®‡µç‡¥®‡µã‡¥ü‡µç‡¥ü‡µÅ‡µç ‡¥™‡µã‡¥ï‡µá‡¥£‡µç‡¥ü‡¥§‡µÅ‡µç ‡¥é‡¥§‡µç‡¥∞ ‡¥Æ‡¥ø‡¥®‡¥ø‡¥±‡µç‡¥±‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µç ‡¥§‡¥æ‡¥¥‡µÜ‡¥Ø‡¥æ‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥¥‡¥æ‡¥£‡µÜ‡¥®‡µç‡¥®‡µÅ‡µç ‡¥§‡¥æ‡¥¥‡µÜ ‡¥ï‡µç‡¥∞‡¥Æ‡µÄ‡¥ï‡¥∞‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥ï | . 3 ‚ñÅAutoSync‚ñÅis‚ñÅa‚ñÅfeature‚ñÅfrom MP3tunes‚ñÅwhich‚ñÅallows‚ñÅyou‚ñÅto‚ñÅautomatically‚ñÅmove‚ñÅyour‚ñÅmusic‚ñÅbetween‚ñÅcomputers‚ñÅand‚ñÅdevices.‚ñÅYou‚ñÅcan‚ñÅupload‚ñÅmusic‚ñÅfrom‚ñÅone‚ñÅlocation‚ñÅand‚ñÅhave‚ñÅit‚ñÅdownload‚ñÅinstantly‚ñÅto‚ñÅother‚ñÅlocations.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa | Enable harmony | . 4 ‚ñÅRegular‚ñÅexpression‚ñÅfor‚ñÅlines‚ñÅwhere‚ñÅKDiff3‚ñÅshould‚ñÅautomatically‚ñÅchoose‚ñÅone‚ñÅsource.‚ñÅWhen‚ñÅa‚ñÅline‚ñÅwith‚ñÅa‚ñÅconflict‚ñÅmatches the‚ñÅregular‚ñÅexpression‚ñÅthen -‚ñÅif‚ñÅavailable - C,‚ñÅotherwise B‚ñÅwill‚ñÅbe‚ñÅchosen.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | ‡¥ï‡µÜ‡¥°‡¥ø‡¥´‡µç3 ‡¥∏‡µç‡¥µ‡¥§‡¥®‡µç‡¥§‡µç‡¥∞‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥í‡¥∞‡µÅ ‡¥∏‡µç‡¥∞‡µã‡¥§‡¥∏‡µç‡¥∏‡µç ‡¥§‡µÜ‡¥∞‡¥û‡µç‡¥û‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥ü‡¥§‡µç‡¥§‡µç ‡¥µ‡¥∞‡¥ø‡¥ï‡¥≥‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥≥‡µç‡¥≥ ‡¥®‡¥ø‡¥§‡µç‡¥Ø‡¥≠‡¥æ‡¥µ‡¥Ç. ‡¥∏‡¥Ç‡¥ò‡¥ü‡µç‡¥ü‡¥®‡¥Æ‡µÅ‡¥≥‡µç‡¥≥ ‡¥µ‡¥∞‡¥ø ‡¥ö‡µá‡¥∞‡µç‡¥®‡µç‡¥®‡µÅ‡¥µ‡¥∞‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥≥‡µç ‡¥Ö‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥®‡¥ø‡¥§‡µç‡¥Ø‡¥≠‡¥æ‡¥µ‡¥Ç - ‡¥∏‡¥ø ‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡¥≤‡µç ‡¥Ö‡¥§‡µç, ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡¥≤‡µç ‡¥¨‡¥ø ‡¥§‡µÜ‡¥∞‡¥û‡µç‡¥û‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥Ç. | . 5 ‚ñÅLoading‚ñÅexternal‚ñÅimages‚ñÅgives‚ñÅspammers the‚ñÅacknowledgement‚ñÅthat‚ñÅyou‚ñÅreceived‚ñÅthis‚ñÅmessage‚ñÅso‚ñÅthey‚ñÅwill‚ñÅuse‚ñÅyour‚ñÅemail‚ñÅaddress‚ñÅto‚ñÅspam‚ñÅyou.‚ñÅSo‚ñÅyou‚ñÅshould‚ñÅonly‚ñÅcontinue‚ñÅfor‚ñÅvery‚ñÅtrusted‚ñÅmessages.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | ‡¥™‡µÅ‡¥±‡¥Æ‡µÜ‡¥®‡¥ø‡¥®‡µç‡¥®‡µä‡¥∞‡µÅ ‡¥ö‡¥ø‡¥§‡µç‡¥∞‡¥Ç ‡¥ï‡¥Ø‡¥±‡µç‡¥±‡µÅ‡¥®‡µç‡¥®‡¥§‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡µç ‡¥à ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥≤‡¥≠‡¥ø‡¥ö‡µç‡¥ö‡µÜ‡¥®‡µç‡¥® ‡¥Æ‡¥ü‡¥ï‡µç‡¥ï‡¥∞‡¥∂‡µÄ‡¥§‡¥ø ‡¥ö‡¥µ‡¥±‡¥Ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥µ‡¥∞‡µç‡¥ï‡µç‡¥ï‡µç ‡¥≤‡¥≠‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥ï‡µç‡¥ï‡¥æ‡¥Ç. ‡¥Ö‡¥µ‡¥∞‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü ‡¥á‡¥§‡¥™‡¥æ‡¥≤‡µç ‡¥µ‡¥ø‡¥≤‡¥æ‡¥∏‡¥Ç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡µÅ‡µç ‡¥®‡µá‡¥∞‡µá‡¥Ø‡µÅ‡¥Ç ‡¥ö‡¥µ‡¥±‡¥Ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥ï‡µç‡¥ï‡¥æ‡¥Ç. ‡¥Ö‡¥§‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µç ‡¥µ‡¥≥‡¥∞‡µÜ ‡¥µ‡¥ø‡¥∂‡µç‡¥µ‡¥∏‡µç‡¥§ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥ô‡µç‡¥ô‡¥≥‡µç ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Ç ‡¥§‡µÅ‡¥ü‡¥∞‡µç‡¥®‡µç‡¥®‡¥æ‡¥≤‡µç ‡¥Æ‡¥§‡¥ø. | . 6 ‚ñÅMailody‚ñÅis‚ñÅable‚ñÅto‚ñÅconvert‚ñÅyour‚ñÅplain‚ñÅmessage‚ñÅto‚ñÅa‚ñÅhtml‚ñÅmessage‚ñÅand‚ñÅinclude‚ñÅthat in the‚ñÅoutgoing‚ñÅmessage.‚ñÅThis‚ñÅmeans the‚ñÅreceiver‚ñÅwill‚ñÅalso‚ñÅhave‚ñÅclickable‚ñÅlinks‚ñÅand‚ñÅcolored‚ñÅquote‚ñÅlevels.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa | ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∏‡¥æ‡¥¶‡¥æ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥é‡¥ö‡µç‡¥ö‡µç‡¥ü‡¥ø‡¥é‡¥Ç‡¥é‡¥≤‡µç ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥ø ‡¥Æ‡¥æ‡¥±‡µç‡¥±‡¥ø ‡¥Ö‡¥§‡µç ‡¥™‡µÅ‡¥±‡¥§‡µç‡¥§‡µá‡¥ï‡µç‡¥ï‡µç ‡¥Ö‡¥Ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥§‡µç‡¥§‡¥ø‡¥≤‡µç ‡¥â‡¥≥‡µç‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡¥æ‡¥®‡µç ‡¥Æ‡µÜ‡¥Ø‡¥ø‡¥≤‡¥°‡¥ø‡¥ï‡µç‡¥ï‡µÅ ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç. ‡¥Ö‡¥§‡¥æ‡¥Ø‡¥§‡µç ‡¥û‡µä‡¥ü‡µç‡¥ü‡¥æ‡¥µ‡µÅ‡¥®‡µç‡¥® ‡¥ï‡¥£‡µç‡¥£‡¥ø‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥µ‡¥∞‡µç‡¥£‡µç‡¥£ ‡¥â‡¥¶‡µç‡¥ß‡¥∞‡¥£‡¥ø ‡¥§‡¥≤‡¥µ‡µÅ‡¥Ç ‡¥∏‡µç‡¥µ‡µÄ‡¥ï‡¥∞‡µç‡¥§‡µç‡¥§‡¥æ‡¥µ‡¥ø‡¥®‡µÅ‡¥ï‡µÇ‡¥ü‡¥ø ‡¥≤‡¥≠‡µç‡¥Ø‡¥Æ‡¥æ‡¥µ‡µÅ‡¥Ç | . 7 ‚ñÅYou‚ñÅhave‚ñÅclicked‚ñÅon‚ñÅa‚ñÅlink‚ñÅwhich‚ñÅmight‚ñÅnot‚ñÅindicate‚ñÅcorrectly‚ñÅwhere‚ñÅyou‚ñÅare‚ñÅreally‚ñÅgoing‚ñÅto.‚ñÅPlease‚ñÅcheck‚ñÅif‚ñÅyou‚ñÅreally‚ñÅwant‚ñÅto‚ñÅview‚ñÅa‚ñÅpage‚ñÅon‚ñÅthis‚ñÅserver:‚ñÅ%1‚ñÅDo‚ñÅyou‚ñÅwant‚ñÅto‚ñÅgo‚ñÅthere?&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µç ‡¥í‡¥∞‡µÅ ‡¥ï‡¥£‡µç‡¥£‡¥ø‡¥Ø‡¥ø‡¥≤‡µç ‡¥û‡µä‡¥ü‡µç‡¥ü‡¥ø‡¥Ø‡¥§‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ‡¥µ‡¥ø‡¥ü‡µá‡¥ï‡µç‡¥ï‡¥æ‡¥£‡µç ‡¥µ‡¥æ‡¥∏‡µç‡¥§‡¥µ‡¥§‡µç‡¥§‡¥ø‡¥≤‡µç ‡¥™‡µã‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µç ‡¥é‡¥®‡µç‡¥®‡µç ‡¥ï‡µÉ‡¥§‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥∏‡µÇ‡¥ö‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µç‡¥≤. ‡¥á ‡¥∏‡µá‡¥µ‡¥ï‡¥®‡µç‡¥±‡µÜ ‡¥§‡¥æ‡¥≥‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µÅ ‡¥§‡¥®‡µç‡¥®‡µÜ ‡¥Ø‡¥æ‡¥£‡µã ‡¥™‡µã‡¥ï‡µá‡¥£‡µç‡¥ü‡¥§‡µÜ‡¥®‡µç‡¥®‡µç ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï:% 1 ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡¥ø‡¥µ‡¥ø‡¥ü‡µÜ ‡¥™‡µã‡¥ï‡¥£‡µã? | . 8 ‚ñÅTry‚ñÅto‚ñÅalign B‚ñÅand C‚ñÅwhen‚ñÅcomparing‚ñÅor‚ñÅmerging‚ñÅthree input‚ñÅfiles.‚ñÅNot‚ñÅrecommended‚ñÅfor‚ñÅmerging‚ñÅbecause‚ñÅmerge‚ñÅmight‚ñÅget‚ñÅmore‚ñÅcomplicated. (Default‚ñÅis‚ñÅoff.)&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | ‡¥Ö‡¥ï‡¥§‡µç‡¥§‡µÅ‡¥µ‡¥ø‡¥ü‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ 3‡¥´‡¥Ø‡¥≤‡µÅ‡¥ï‡¥≥‡µç ‡¥§‡¥æ‡¥∞‡¥§‡¥Æ‡µç‡¥Ø‡¥Æ‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥¥‡µã ‡¥≤‡¥Ø‡¥®‡¥Ç ‡¥®‡¥ü‡¥§‡µç‡¥§‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥¥‡µã‡¥¨‡¥ø‡¥Ø‡µÅ‡¥Ç ‡¥∏‡¥ø‡¥Ø‡µÅ‡¥Ç ‡¥®‡¥ø‡¥∞‡¥í‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µç ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç. ‡¥ï‡µÇ‡¥ü‡µÅ‡¥§‡¥≤‡µç ‡¥ï‡µÅ‡¥¥‡¥™‡µç‡¥™‡¥Ç ‡¥™‡¥ø‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡¥§‡¥æ‡¥Ø‡¥§‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µç ‡¥≤‡¥Ø‡¥®‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥§‡µç‡¥§‡¥ø‡¥≤‡µç ‡¥Ö‡¥§‡µç ‡¥∂‡µÅ‡¥™‡¥æ‡¥∞‡µç‡¥∂ ‡¥ö‡µÜ‡¥Ø‡µç‡¥§‡¥ø‡¥ü‡µç‡¥ü‡¥ø‡¥≤‡µç‡¥≤. (‡¥ì‡¥´‡µç ‡¥Ü‡¥£‡µç ‡¥∏‡¥π‡¥ú‡¥Ç.) | . seq2seq_metrics = {&quot;bleu&quot;: {&quot;returns&quot;: &quot;bleu&quot;}, &quot;meteor&quot;: {&quot;returns&quot;: &quot;meteor&quot;}, &quot;sacrebleu&quot;: {&quot;returns&quot;: &quot;score&quot;}} model = BaseModelWrapper(hf_model) learn_cbs = [BaseModelCallback] fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)] learn = Learner( dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), # CrossEntropyLossFlat() cbs=learn_cbs, splitter=partial(blurr_seq2seq_splitter, arch=hf_arch), ) learn.freeze() . [nltk_data] Downloading package wordnet to /home/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /home/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package omw-1.4 to /home/nltk_data... [nltk_data] Package omw-1.4 is already up-to-date! . learn.lr_find(suggest_funcs=[minimum, steep, valley, slide]) . SuggestedLRs(minimum=0.00010000000474974513, steep=2.75422871709452e-06, valley=0.00013182566908653826, slide=0.2089296132326126) . learn.fit_one_cycle(15, lr_max=5e-4, cbs=fit_cbs) . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 5.345715 | 4.811335 | 0.038182 | 0.183787 | 5.607312 | 00:15 | . 1 | 4.658613 | 4.272337 | 0.058925 | 0.206963 | 3.932045 | 00:32 | . 2 | 4.196794 | 4.031921 | 0.069354 | 0.167185 | 4.876618 | 00:25 | . 3 | 3.826715 | 4.102871 | 0.071109 | 0.122637 | 4.639123 | 00:18 | . 4 | 3.551352 | 4.046614 | 0.080512 | 0.202187 | 6.996449 | 00:26 | . 5 | 3.329875 | 3.928597 | 0.054780 | 0.179460 | 3.403195 | 00:53 | . 6 | 3.197556 | 3.818610 | 0.109263 | 0.212960 | 9.055532 | 00:21 | . 7 | 3.034615 | 3.821332 | 0.100355 | 0.200819 | 8.208149 | 00:20 | . 8 | 2.900438 | 3.807550 | 0.101014 | 0.222323 | 7.221607 | 00:35 | . 9 | 2.820454 | 3.813579 | 0.111548 | 0.219794 | 9.026264 | 00:29 | . 10 | 2.756891 | 3.791952 | 0.107236 | 0.223636 | 9.965611 | 00:31 | . 11 | 2.740331 | 3.809624 | 0.115999 | 0.239258 | 10.261043 | 00:24 | . 12 | 2.685602 | 3.804891 | 0.126693 | 0.234446 | 11.131677 | 00:22 | . 13 | 2.653799 | 3.800799 | 0.119988 | 0.227742 | 10.985718 | 00:23 | . 14 | 2.636473 | 3.801427 | 0.124357 | 0.229134 | 10.967688 | 00:25 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=500) . text target prediction . 0 ‚ñÅWith‚ñÅa‚ñÅdynamic‚ñÅplaylist, Amarok‚ñÅbecomes‚ñÅyour‚ñÅown‚ñÅpersonal‚ñÅdj,‚ñÅautomatically‚ñÅselecting‚ñÅtracks‚ñÅfor‚ñÅyou,‚ñÅbased‚ñÅon‚ñÅa‚ñÅnumber of‚ñÅparameters‚ñÅthat‚ñÅyou‚ñÅselect. | Turn dynamic mode on | [Username for logins to disabled, format: Artist - Track (Album), from the currently playlist column name and token for playlist layouts, ‡¥µ‡¥∞‡¥ø‡¥ï‡¥≥‡¥ø‡¥≤‡µÜ ‡¥Ö‡¥ï‡µç‡¥∑‡¥∞‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥µ‡¥∏‡µç‡¥• ‡¥µ‡¥ø‡¥ó‡¥£‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï. (‡¥Æ‡µÅ‡¥®‡µç‡¥®‡¥ø‡¥≤‡µÜ ‡¥µ‡¥ø‡¥µ‡¥∞‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥§‡¥æ‡¥∞‡¥§‡¥Æ‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥æ‡¥®‡µÅ‡¥Ç ‡¥∏‡¥ø‡¥Æ‡µÅ‡¥≤‡µá‡¥±‡µç‡¥±‡¥±‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç.), ‡¥í‡¥∞‡µÅ ‡¥ü‡¥æ‡¥¨‡µç ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Æ‡µá ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÇ. ‡¥í‡¥∞‡µÅ ‡¥ö‡µÜ‡¥±‡¥ø‡¥Ø ‡¥ü‡¥æ‡¥¨‡µç ‡¥¨‡¥æ‡¥±‡µç‡¥±‡¥±‡¥ø‡¥Ø‡¥ø‡¥≤‡µç‡¥≤‡¥æ‡¥§‡µÜ ‡¥∏‡µç‡¥µ‡¥Ø‡¥Ç ‡¥ü‡¥æ‡¥¨‡µç ‡¥¨‡¥æ‡¥±‡µç‡¥±‡¥±‡¥ø ‡¥Ø‡¥æ‡¥®‡µç‡¥§‡µç‡¥∞‡¥ø‡¥ï‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥Æ‡¥±‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥í‡¥∞‡µÅ ‡¥ü‡¥æ‡¥¨‡µç ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Æ‡µá ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÇ., ‡¥á‡¥§‡µç‡¥§‡¥∞‡¥Ç ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥®‡¥Ç ‡¥§‡µÅ‡¥ü‡¥ô‡µç‡¥ô‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥≥‡µçÍ†±, ‡¥Æ‡µÜ‡¥Ø‡¥ø‡¥≤‡¥°‡¥ø‡¥ï‡¥≥‡¥ø‡µΩ ‡¥ö‡¥ø‡¥∞‡¥ø‡¥ö‡µç‡¥ö‡¥§‡¥æ‡¥Ø‡¥ø ‡¥∏‡µç‡¥µ‡¥Ø‡¥Æ‡µá‡¥µ ‡¥≤‡¥Ø‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï. ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥í‡¥∞‡µÅ ‡¥Ö‡¥ï‡µç‡¥∑‡¥∞‡¥∞‡µÇ‡¥™‡¥Ç ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ‡¥£‡µç‡¥ü‡µã‡¥Ø‡µÜ‡¥®‡µç‡¥®‡µç ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥§‡µÜ ‡¥∏‡µç‡¥µ‡¥Ø‡¥Ç ‡¥ö‡¥ø‡¥∞‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡¥æ‡¥µ‡µÅ‡¥Ç., ‡¥Æ‡µÅ‡¥®‡µçÍ†±‡¥ú‡µç‡¥ú‡µç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µç ‡¥™‡¥∞‡¥æ‡¥ú‡¥Ø‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü‡µÅ. ‡¥à ‡¥Ü‡¥ú‡µç‡¥û ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÇ:% 1 ‡¥Æ‡µÅ‡¥®‡µçÍ†±‡¥®‡¥ü‡¥™‡¥ü‡¥ø ‡¥Ü‡¥ú‡µç‡¥û ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥™‡µç‡¥∞‡¥µ‡¥∞‡µç ‡¥™‡¥∂‡µç‡¥ö‡¥æ‡¥§‡µç‡¥§‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥§‡µç‡¥§‡¥®‡¥∞‡¥π‡¥ø‡¥§‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥Ç., ‡¥™‡µÅ‡¥±‡¥Æ‡µÜ‡¥®‡¥ø‡¥®‡µç‡¥®‡µç- ‡¥Æ‡¥ø‡¥ö‡µç‡¥ö‡¥Æ‡µÅ‡¥≥‡µç‡¥≥ ‡¥Æ‡¥ü‡µç‡¥ü‡µÅ‡¥Ç ‡¥™‡µç‡¥∞‡¥æ‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥µ‡¥≤‡¥ø‡¥Ø ‡¥´‡¥Ø‡¥≤‡µÅ‡¥ï‡¥≥‡µÜ‡¥ï‡µç‡¥ï‡µÅ‡¥±‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡¥≥‡µç‡¥≥ ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ï‡¥®‡µç‡¥±‡µÜ ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ï‡¥®‡µÅ‡¥≠‡¥µ‡¥Ç ‡¥µ‡¥≥‡¥∞‡µÜ ‡¥™‡¥§‡µÅ‡¥ï‡µç‡¥ï‡µÜ‡¥Ø‡¥æ‡¥µ‡µÅ‡¥Ç., ‡¥à ‡¥∏‡µç‡¥≤‡µà‡¥°‡¥®‡µÅ‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡µç, ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥Ç ‡¥∏‡µã‡¥ï‡µç‡¥ï‡¥±‡µç‡¥±‡µç ‡¥™‡µÅ‡¥±‡¥§‡µç‡¥§‡µá‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡µç ‡¥™‡¥§‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ, ‡¥™‡µÅ‡¥±‡¥§‡µç‡¥§‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥µ‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥Æ‡µÜ‡¥Ø‡¥ø‡¥≤‡¥°‡¥ø‡¥≠‡¥æ‡¥ó‡¥Ç ‡¥≤‡¥≠‡µç‡¥Ø‡¥Æ‡¥≤‡µç‡¥≤, ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥Ö‡¥§‡µç ‡¥Ö‡¥Ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡¥®‡µç ‡¥Æ‡µÅ‡¥Æ‡µç‡¥™‡¥æ‡¥Ø‡¥ø ‡¥ï‡µç‡¥∞‡¥Æ‡µÄ‡¥ï‡¥∞‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥ï, ‡¥à ‡¥Æ‡µÅ‡¥®‡µçÍ†± ‡¥™‡µÇ‡¥∞‡µçÍ†±‡¥µ‡µç‡¥µ ‡¥™‡µç‡¥∞‡¥ï‡µç‡¥∞‡¥ø‡¥Ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥≥‡µç‡¥≥ ‡¥∏‡¥Æ‡¥Ø‡¥§‡µç‡¥§‡µç ‡¥µ‡¥∞‡¥ø‡¥Ø‡¥ø‡¥≤‡µçÍ†± ‡¥Æ‡¥æ‡¥§‡µç‡¥∞‡¥Æ‡µá ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÇ. (‡¥µ‡¥ø‡¥∂‡¥¶‡µÄ‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡¥≥‡µç‡¥ï‡µç‡¥ï‡µç ‡¥™‡µç‡¥∞‡¥Æ‡¥æ‡¥£‡¥™‡¥§‡µç‡¥∞‡¥Ç ‡¥®‡µã‡¥ï‡µç‡¥ï‡µÅ‡¥ï.), ‡¥´‡¥Ø‡¥≤‡µçÍ†± ‡¥™‡¥ï‡¥∞‡µçÍ†±‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥≥‡µçÍ†± ‡¥™‡¥ø‡¥∂‡¥ï‡µç: ‡¥´‡¥Ø‡¥≤‡¥ø‡¥≤‡µç‚ô¨ ‡¥µ‡¥æ‡¥Ø‡¥®‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥≥‡µç‡¥≥ ‡¥´‡¥Ø‡¥≤‡µçÍ†± ‡¥§‡µÅ‡¥±‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µç ‡¥™‡¥∞‡¥æ‡¥ú‡¥Ø‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü‡µÅ. ‡¥´‡¥Ø‡¥≤‡µç ‡¥™‡¥∂‡µç‡¥ö‡¥æ‡¥§‡µç‡¥§‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï ‡¥®‡¥æ‡¥Æ‡¥Ç:% 1, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥¨‡µÄ‡¥ó‡¥ø‡¥≥‡µçÍ†± ‡¥¨‡¥æ‡¥ï‡µç‡¥ï‡µÜ‡¥®‡µç ‡¥™‡¥∂‡µç‡¥ö‡¥æ‡¥§‡µç‡¥§‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥°‡µÅ‡¥ï‡¥≥‡µçÍ†± ‡¥§‡µÜ‡¥∞‡¥û‡µç‡¥û‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡µÅ‡¥ï., ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡¥æ‡¥µ‡µç‡¥®‡¥æ‡¥Æ‡¥Ç ‡¥§‡¥ø‡¥∞‡¥ø‡¥ö‡µç‡¥ö‡¥±‡¥ø‡¥û‡µç‡¥û‡¥ø‡¥ü‡µç‡¥ü‡¥ø‡¥≤‡µç‡¥≤, ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µÜ ‡¥Ö‡¥ü‡¥Ø‡¥æ‡¥≥‡¥Æ‡¥ø‡¥ü‡µÅ‡¥ï., ‡¥Ö‡¥®‡µÅ‡¥¨‡¥®‡µç‡¥ß‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥≥‡¥µ‡µç% 1 is new name for translate is the filter, representing the playlist column name and token for playlist layouts, ‡¥¨‡¥æ‡¥±‡µç‡¥±‡¥±‡¥ø‡¥ï‡¥≥‡¥ø‡¥≤‡µÜ‡¥§‡µç‡¥§‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µÅ‡¥Æ‡µç‡¥™‡µã‡¥≥‡µçÍ†± ‡¥∂‡µã‡¥≠ ‡¥®‡¥ø‡¥Ø‡¥®‡µç‡¥§‡µç‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ, (&quot;‡¥í‡¥¥‡¥ø‡¥û‡µç‡¥û ‡¥á‡¥ü‡¥ô‡µç‡¥ô‡¥≥‡µçÍ†± ‡¥ï‡¥æ‡¥£‡¥ø‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥ï&quot; ‡¥™‡µç‡¥∞‡¥µ‡¥∞‡µçÍ†±‡¥§‡µç‡¥§‡¥Æ‡¥∞‡¥π‡¥ø‡¥§‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡¥ø‡¥Ø‡¥æ‡¥≤‡µç ‡¥™‡¥∂‡µç‡¥ö‡¥æ‡¥§‡µç‡¥§‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï ‡¥í‡¥¥‡¥ø‡¥û‡µç‡¥û ‡¥á‡¥ü‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥µ‡µç‡¥Ø ‡¥Ö‡¥≤‡µç‡¥≤‡¥æ‡¥π‡µÅ‡¥ï‡µç‡¥ï‡¥≥‡µçÍ†± ‡¥í‡¥¥‡¥ø‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥Ç.), ‡¥Æ‡¥æ‡¥§‡¥æ‡¥™‡¥ø‡¥§‡¥æ‡¥ï‡µç‡¥ï‡µæ ‡¥§‡µÅ‡¥±‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µÅ‡¥Ç ‡¥Ö‡¥ü‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µÅ‡¥Ç ‡¥≤‡¥Ø‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µÅ‡¥Ç ‡¥≤‡¥Ø‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï.] | . . Now let&#39;s translate with our trained models . blurr top 3 translation predictions . test_text = &quot;How are you doing&quot; outputs = learn.blurr_generate(test_text, key=&quot;translation_texts&quot;, num_return_sequences=3) outputs . [{&#39;translation_texts&#39;: [&#39;‡¥é‡¥®‡µç‡¥§‡µä‡¥ï‡µç‡¥ï‡µÜ‡¥Ø‡µÅ‡¥£‡µç‡¥ü‡µç?&#39;, &#39;‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ‡¥Ø‡µÅ‡¥£‡µç‡¥ü‡µç?&#39;, &#39;‡¥é‡¥®‡µç‡¥§‡µä‡¥ï‡µç‡¥ï‡µÜ‡¥Ø‡µÅ‡¥£‡µç‡¥ü‡µç.&#39;]}] . Saving trained ML model . export_fname = &quot;saved_model&quot; learn.metrics = None learn.export(fname=f&quot;{export_fname}.pkl&quot;) . from huggingface_hub import push_to_hub_fastai push_to_hub_fastai( learn, &quot;kurianbenoy/kde_en_ml_translation_model&quot;, commit_message=&quot;New version with 15 epoch of training&quot;, ) . /home/kurianbenoy/kde_en_ml_translation_model is already a clone of https://huggingface.co/kurianbenoy/kde_en_ml_translation_model. Make sure you pull the latest changes with `repo.git_pull()`. W0511 16:57:02.992604 140162781013824 repository.py:685] /home/kurianbenoy/kde_en_ml_translation_model is already a clone of https://huggingface.co/kurianbenoy/kde_en_ml_translation_model. Make sure you pull the latest changes with `repo.git_pull()`. remote: Enforcing permissions... remote: Allowed refs: all To https://huggingface.co/kurianbenoy/kde_en_ml_translation_model 62f36f9..766c8a0 main -&gt; main W0511 16:58:04.917035 140162781013824 repository.py:1144] remote: Enforcing permissions... remote: Allowed refs: all To https://huggingface.co/kurianbenoy/kde_en_ml_translation_model 62f36f9..766c8a0 main -&gt; main . &#39;https://huggingface.co/kurianbenoy/kde_en_ml_translation_model/commit/766c8a06c07bb6352d0537ac1972d3c70360fd53&#39; . Prediction with our trained model . Correct Prediction . test_text = &quot;How are you doing&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text) . [{&#39;translation_texts&#39;: &#39;‡¥é‡¥®‡µç‡¥§‡µä‡¥ï‡µç‡¥ï‡µÜ‡¥Ø‡µÅ‡¥£‡µç‡¥ü‡µç?&#39;}] . test_text1 = &quot;Add All Found Feeds to Akregator.&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text1) . [{&#39;translation_texts&#39;: &#39;‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥Ö‡¥ï‡µç‡¥∞‡¥ø‡¥ó‡µá‡¥±‡µç‡¥±‡¥±‡¥ø‡¥≤‡µçÍ†± ‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µá‡¥∞‡µç ‡¥™‡¥∂‡µç‡¥ö‡¥æ‡¥§‡µç‡¥§‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥ï‡µç‡¥ï‡µÅ‡¥ï.&#39;}] . Wrong Prediction . test_text2 = &quot;Subscribe to site updates (using news feed).&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text2) . [{&#39;translation_texts&#39;: &#39;‡¥∏‡µà‡¥±‡µç‡¥±‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µÜ ‡¥™‡µÅ‡¥§‡µÅ‡¥Æ‡¥ï‡¥≥‡¥±‡¥ø‡¥Ø‡¥æ‡¥®‡µçÍ†± ‡¥µ‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥∞‡¥®‡¥æ‡¥ï‡µÅ‡¥ï (‡¥µ‡¥æ‡¥∞‡µçÍ†±‡¥§‡µç‡¥§‡¥æ ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µç‚ô¨ ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡µç).&#39;}] . Expected: &#39;‡¥∏‡µà‡¥±‡µç‡¥±‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µÜ ‡¥™‡µÅ‡¥§‡µÅ‡¥Æ‡¥ï‡¥≥‡¥±‡¥ø‡¥Ø‡¥æ‡¥®‡µç u200d ‡¥µ‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥∞‡¥®‡¥æ‡¥ï‡µÅ‡¥ï (‡¥µ‡¥æ‡¥∞‡µç u200d‡¥§‡µç‡¥§‡¥æ ‡¥´‡µÄ‡¥°‡µÅ‡¥ï‡¥≥‡µç u200d ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡µç . Thanks to . Wayde Gilliam - for creating blurr, and helping with doubts in translation bits | Kevin Bird - for helping in editing the article. | . fin. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/13/hfenml-translate.html",
            "relUrl": "/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/13/hfenml-translate.html",
            "date": " ‚Ä¢ Mar 13, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
            "content": ". from datasets import load_dataset raw_datasets = load_dataset(&quot;squad&quot;) . Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453... Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data. . raw_datasets . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 87599 }) validation: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 10570 }) }) . Recently a tweet went viral . We can see that NLP is hard, and even the best publicly avaiable models performs poorly in this tasks | . . from transformers import pipeline classifier = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased&quot;) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.weight&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;classifier.weight&#39;, &#39;pre_classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . [{&#39;label&#39;: &#39;LABEL_0&#39;, &#39;score&#39;: 0.5706434845924377}] . checkpoint = &quot;cardiffnlp/twitter-roberta-base-sentiment&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . [{&#39;label&#39;: &#39;LABEL_2&#39;, &#39;score&#39;: 0.9608174562454224}] . checkpoint = &quot;finiteautomata/bertweet-base-sentiment-analysis&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji . [{&#39;label&#39;: &#39;NEG&#39;, &#39;score&#39;: 0.9447618126869202}] . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;Where do I work?&quot;, context=&quot;My name is Sylvain and I work at Hugging Face in Brooklyn&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Hugging Face&#39;, &#39;end&#39;: 45, &#39;score&#39;: 0.6949771046638489, &#39;start&#39;: 33} . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;What was feedback?&quot;, context=&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Brilliant service&#39;, &#39;end&#39;: 101, &#39;score&#39;: 0.13661321997642517, &#39;start&#39;: 84} . For specific question answering - use question answering models . Encoder-only models like BERT tend to be great at extracting answers to factoid questions like ‚ÄúWho invented the Transformer architecture?‚Äù but fare poorly when given open-ended questions like ‚ÄúWhy is the sky blue?‚Äù In these more challenging cases . If you‚Äôre interested in this type of generative question answering, we recommend checking out our demo based on the ELI5 dataset. Using Lonformer model. . [1] https://yjernite.github.io/lfqa.html . print(&quot;Context: &quot;, raw_datasets[&quot;train&quot;][0][&quot;context&quot;]) print(&quot;Question: &quot;, raw_datasets[&quot;train&quot;][0][&quot;question&quot;]) print(&quot;Answer: &quot;, raw_datasets[&quot;train&quot;][0][&quot;answers&quot;]) . Context: Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34;Venite Ad Me Omnes&#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? Answer: {&#39;text&#39;: [&#39;Saint Bernadette Soubirous&#39;], &#39;answer_start&#39;: [515]} . raw_datasets[&quot;train&quot;].filter(lambda x: len(x[&quot;answers&quot;][&quot;text&quot;]) != 1) . Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 0 }) . print(raw_datasets[&quot;validation&quot;][0][&quot;answers&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;answers&quot;]) . {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]} {&#39;text&#39;: [&#39;Santa Clara, California&#39;, &#34;Levi&#39;s Stadium&#34;, &#34;Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California.&#34;], &#39;answer_start&#39;: [403, 355, 355]} . print(raw_datasets[&quot;validation&quot;][2][&quot;context&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;question&quot;]) . Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the &#34;golden anniversary&#34; with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as &#34;Super Bowl L&#34;), so that the logo could prominently feature the Arabic numerals 50. Where did Super Bowl 50 take place? . from transformers import AutoTokenizer model_checkpoint = &quot;bert-base-cased&quot; tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . context = raw_datasets[&quot;train&quot;][0][&quot;context&quot;] question = raw_datasets[&quot;train&quot;][0][&quot;question&quot;] inputs = tokenizer(question, context) tokenizer.decode(inputs[&quot;input_ids&quot;]) . &#39;[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]&#39; . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, ) for ids in inputs[&quot;input_ids&quot;]: print(tokenizer.decode(ids)) . [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;offset_mapping&#39;, &#39;overflow_to_sample_mapping&#39;]) . inputs[&quot;overflow_to_sample_mapping&quot;] . [0, 0, 0, 0] . inputs = tokenizer( raw_datasets[&quot;train&quot;][2:6][&quot;question&quot;], raw_datasets[&quot;train&quot;][2:6][&quot;context&quot;], max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) print(f&quot;The 4 examples gave {len(inputs[&#39;input_ids&#39;])} features.&quot;) print(f&quot;Here is where each comes from: {inputs[&#39;overflow_to_sample_mapping&#39;]}.&quot;) . The 4 examples gave 19 features. Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]. . Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If that‚Äôs not the case, we loop to find the first and last token of the answer: . answers = raw_datasets[&quot;train&quot;][2:6][&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(inputs[&quot;offset_mapping&quot;]): sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) start_positions, end_positions . ([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0], [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0]) . idx = 0 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] start = start_positions[idx] end = end_positions[idx] labeled_answer = tokenizer.decode(inputs[&quot;input_ids&quot;][idx][start : end + 1]) print(f&quot;Theoretical answer: {answer}, labels give: {labeled_answer}&quot;) . Theoretical answer: the Main Building, labels give: the Main Building . idx = 4 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] decoded_example = tokenizer.decode(inputs[&quot;input_ids&quot;][idx]) print(f&quot;Theoretical answer: {answer}, decoded example: {decoded_example}&quot;) . Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP] . max_length = 384 stride = 128 def preprocess_training_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) offset_mapping = inputs.pop(&quot;offset_mapping&quot;) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) answers = examples[&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(offset_mapping): sample_idx = sample_map[i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) inputs[&quot;start_positions&quot;] = start_positions inputs[&quot;end_positions&quot;] = end_positions return inputs . train_dataset = raw_datasets[&quot;train&quot;].map( preprocess_training_examples, batched=True, remove_columns=raw_datasets[&quot;train&quot;].column_names, ) len(raw_datasets[&quot;train&quot;]), len(train_dataset) . (87599, 88729) . &#39;Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot; Venite Ad Me Omnes &quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]&#39; . Indeed, we don‚Äôt see the answer inside the context. . ‚úèÔ∏è Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied. . Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We‚Äôll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here: . def preprocess_validation_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) example_ids = [] for i in range(len(inputs[&quot;input_ids&quot;])): sample_idx = sample_map[i] example_ids.append(examples[&quot;id&quot;][sample_idx]) sequence_ids = inputs.sequence_ids(i) offset = inputs[&quot;offset_mapping&quot;][i] inputs[&quot;offset_mapping&quot;][i] = [ o if sequence_ids[k] == 1 else None for k, o in enumerate(offset) ] inputs[&quot;example_id&quot;] = example_ids return inputs . validation_dataset = raw_datasets[&quot;validation&quot;].map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) len(raw_datasets[&quot;validation&quot;]), len(validation_dataset) . (10570, 10822) . small_eval_set = raw_datasets[&quot;validation&quot;].select(range(100)) trained_checkpoint = &quot;distilbert-base-cased-distilled-squad&quot; tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint) eval_set = small_eval_set.map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) . tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . import torch from transformers import AutoModelForQuestionAnswering eval_set_for_model = eval_set.remove_columns([&quot;example_id&quot;, &quot;offset_mapping&quot;]) eval_set_for_model.set_format(&quot;torch&quot;) device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names} trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to( device ) with torch.no_grad(): outputs = trained_model(**batch) . start_logits = outputs.start_logits.cpu().numpy() end_logits = outputs.end_logits.cpu().numpy() . import collections example_to_features = collections.defaultdict(list) for idx, feature in enumerate(eval_set): example_to_features[feature[&quot;example_id&quot;]].append(idx) . import numpy as np n_best = 20 max_answer_length = 30 predicted_answers = [] for example in small_eval_set: example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = eval_set[&quot;offset_mapping&quot;][feature_index] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length. if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answers.append( { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } ) best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]}) . from datasets import load_metric metric = load_metric(&quot;squad&quot;) . metric . Metric(name: &#34;squad&#34;, features: {&#39;predictions&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;prediction_text&#39;: Value(dtype=&#39;string&#39;, id=None)}, &#39;references&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answers&#39;: Sequence(feature={&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answer_start&#39;: Value(dtype=&#39;int32&#39;, id=None)}, length=-1, id=None)}}, usage: &#34;&#34;&#34; Computes SQuAD scores (F1 and EM). Args: predictions: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair as given in the references (see below) - &#39;prediction_text&#39;: the text of the answer references: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair (see above), - &#39;answers&#39;: a Dict in the SQuAD dataset format { &#39;text&#39;: list of possible texts for the answer, as a list of strings &#39;answer_start&#39;: list of start positions for the answer, as a list of ints } Note that answer_start values are not taken into account to compute the metric. Returns: &#39;exact_match&#39;: Exact match (the normalized answer exactly match the gold answer) &#39;f1&#39;: The F-score of predicted tokens versus the gold answer Examples: &gt;&gt;&gt; predictions = [{&#39;prediction_text&#39;: &#39;1976&#39;, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; references = [{&#39;answers&#39;: {&#39;answer_start&#39;: [97], &#39;text&#39;: [&#39;1976&#39;]}, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; squad_metric = datasets.load_metric(&#34;squad&#34;) &gt;&gt;&gt; results = squad_metric.compute(predictions=predictions, references=references) &gt;&gt;&gt; print(results) {&#39;exact_match&#39;: 100.0, &#39;f1&#39;: 100.0} &#34;&#34;&#34;, stored examples: 0) . theoretical_answers = [ {&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in small_eval_set ] print(predicted_answers[0]) print(theoretical_answers[0]) . {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;prediction_text&#39;: &#39;Denver Broncos&#39;} {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;answers&#39;: {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]}} . metric.compute(predictions=predicted_answers, references=theoretical_answers) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . from tqdm.auto import tqdm def compute_metrics(start_logits, end_logits, features, examples): example_to_features = collections.defaultdict(list) for idx, feature in enumerate(features): example_to_features[feature[&quot;example_id&quot;]].append(idx) predicted_answers = [] for example in tqdm(examples): example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] # Loop through all features associated with that example for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = features[feature_index][&quot;offset_mapping&quot;] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answer = { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } answers.append(answer) # Select the answer with the best score if len(answers) &gt; 0: best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append( {&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]} ) else: predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: &quot;&quot;}) theoretical_answers = [{&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in examples] return metric.compute(predictions=predicted_answers, references=theoretical_answers) . compute_metrics(start_logits, end_logits, eval_set, small_eval_set) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) . Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#39;qa_outputs.bias&#39;, &#39;qa_outputs.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . from huggingface_hub import notebook_login notebook_login() . Login successful Your token has been saved to /root/.huggingface/token Authenticated through git-credential store but this isn&#39;t the helper defined on your machine. You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default git config --global credential.helper store . ! sudo apt install git-lfs ! git-lfs install . Reading package lists... Done Building dependency tree Reading state information... Done git-lfs is already the newest version (2.3.4-1). The following package was automatically installed and is no longer required: libnvidia-common-470 Use &#39;sudo apt autoremove&#39; to remove it. 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Error: Failed to call git rev-parse --git-dir --show-toplevel: &#34;fatal: not a git repository (or any of the parent directories): .git n&#34; Git LFS initialized. . from transformers import TrainingArguments args = TrainingArguments( &quot;bert-finetuned-squad&quot;, evaluation_strategy=&quot;no&quot;, save_strategy=&quot;epoch&quot;, learning_rate=2e-5, num_train_epochs=1, weight_decay=0.01, fp16=True, push_to_hub=True, ) . PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). . from transformers import Trainer trainer = Trainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=validation_dataset, tokenizer=tokenizer, ) trainer.train() . /content/bert-finetuned-squad is already a clone of https://huggingface.co/kurianbenoy/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`. Using amp half precision backend /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 88729 Num Epochs = 1 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 11092 . . [ 4154/11092 2:29:47 &lt; 4:10:18, 0.46 it/s, Epoch 0.37/1] Step Training Loss . 500 | 1.691300 | . 1000 | 1.575600 | . 1500 | 1.441200 | . 2000 | 1.356100 | . 2500 | 1.297700 | . 3000 | 1.245200 | . 3500 | 1.242400 | . 4000 | 1.181400 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; predictions, _ = trainer.predict(validation_dataset) start_logits, end_logits = predictions compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[&quot;validation&quot;]) . trainer.push_to_hub(commit_message=&quot;Training complete&quot;) . &lt;/div&gt; .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/02/27/QuestionAnsweringWithHF.html",
            "relUrl": "/2022/02/27/QuestionAnsweringWithHF.html",
            "date": " ‚Ä¢ Feb 27, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Starting_my_ml_blog",
            "content": "Starting my ML blog . I am into the field of AI/DataScience for some time. To dive deep into this field, I am creating this ML blogpost space. This space is exclusively for my ML/AI articles. Also by 27th February, 2022 expect a blog post on question answering with blurr. .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/02/21/Starting_my_ML_blog.html",
            "relUrl": "/2022/02/21/Starting_my_ML_blog.html",
            "date": " ‚Ä¢ Feb 21, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://kurianbenoy.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://kurianbenoy.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}