{
  
    
        "post0": {
            "title": "Introduction to linux Terminal and Installing Python Packages & Libraries for Data Sciene",
            "content": "Introduction . Jeremy has been conducting these official course walk-thrus which start on May 27, 22 for students of fastaiv5 course. I personally felt some of these things were very useful to me personally, and I may use it again and again. So thought of writing it down. . You can install terminal in Windows by downloading Windows terminal. In case of linux, just type terminal and you will notice it will come with a terminal. In MacOS, you can just open terminal. . What is a terminal? What is a shell? Why use them? . A terminal is a program which can display console window to run program. Yet thing inside is not stricly a terminal. Usually in a terminal there can be multiple shells, which can have different colours, shells etc. In a windows terminal it can start no of shells like PowerShell, Command Prompt, Ubuntu etc. We use both interchangebly which is ok. . In Windows, Jeremy recommends to use WSL. It&#39;s a good idea to change the default shell from Power shell to Ubuntu for easy usage. Also learn some shortcuts like: . Ctrl+Shift+1 - Open shell set for default profile Ctrl+Shift+3 - Open shell listed as number 3 in WSL Alt+Enter - Enter screen in full screen Alt+Shift++ - Open a new vertical pane Alt+Shift+- - Open a new horizontal pane Ctrl + l - Clear the screen Ctrl+Shift+w - Closing a Pane . Important: Learning Keyboard shortcuts can be immensely valuable. . Installing Python Packages . . Important: Never use the python which comes by default with Operating system, always use a differnt python to work on stuff you want. Else it will become really messy. Let&#39;s install first the python packages with mamba aka. faster version of conda.Go to the mambaforge installer. Based on your operating system, download the shell script to install with: . wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh . Then install with: . bash Mambaforge-Linux-x86_64.sh . After downloading run the command: . . ~/.bashrc . to refresh the terminal . Note: Jeremy recommends to install popular libraries which are supported in anaconda from mamba. If it&#8217;s not there in conda, or something which requires editable install use pip then only. . Using fastsetup . Goto github.com/fastai/fastsetup, download the repo or wget setup-conda.sh. This is a normal shell script, so just run: . bash setup-conda.sh . On running you need to refresh your terminal again, I used . ~/.bashrc, that conda install -yq mamba. . Uninstalling packages . Check if ipython, jupyter installed | If yes, first uninstall install | Then remove mamabaforge folder | Then delete conda package | . Running Jupyter . mamba install jupyter . Run, jupyter lab . open in localhost:8888 port .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastaicourse/terminal/python/2022/05/28/fastai-walthrus1.html",
            "relUrl": "/fastaicourse/terminal/python/2022/05/28/fastai-walthrus1.html",
            "date": " â€¢ May 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Quickly trying out a NLP model for Kaggle Competition",
            "content": "This is my attempt to see how well we can build a NLP model for Natural Language Processing with Disaster Tweets. . According to competition you are required to : . In this competition, youâ€™re challenged to build a machine learning model that predicts which Tweets are about real disasters and which oneâ€™s arenâ€™t. Youâ€™ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we&#39;ve created a quick tutorial to get you up and running. . Downloading Data . creds = &#39;&#39; . from pathlib import Path cred_path = Path(&quot;~/.kaggle/kaggle.json&quot;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(creds) cred_path.chmod(0o600) . ! kaggle competitions download -c nlp-getting-started . nlp-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download) . ! unzip nlp-getting-started.zip . import pandas as pd . df = pd.read_csv(&quot;train.csv&quot;) df.head() . id keyword location text target . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | . df.describe(include=&quot;object&quot;) . keyword location text . count 7552 | 5080 | 7613 | . unique 221 | 3341 | 7503 | . top fatalities | USA | 11-Year-Old Boy Charged With Manslaughter of T... | . freq 45 | 104 | 10 | . df[&quot;input&quot;] = df[&quot;text&quot;] . Tokenization . from datasets import Dataset, DatasetDict ds = Dataset.from_pandas(df) . ds . Dataset({ features: [&#39;id&#39;, &#39;keyword&#39;, &#39;location&#39;, &#39;text&#39;, &#39;target&#39;, &#39;input&#39;], num_rows: 7613 }) . model_nm = &quot;microsoft/deberta-v3-small&quot; . from transformers import AutoModelForSequenceClassification, AutoTokenizer tokz = AutoTokenizer.from_pretrained(model_nm) . def tok_func(x): return tokz(x[&quot;input&quot;]) tok_ds = ds.map(tok_func, batched=True) . Parameter &#39;function&#39;=&lt;function tok_func at 0x7f28da60b8b0&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed. . row = tok_ds[0] row[&quot;input&quot;], row[&quot;input_ids&quot;] . (&#39;Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all&#39;, [1, 581, 65453, 281, 262, 18037, 265, 291, 953, 117831, 903, 4924, 17018, 43632, 381, 305, 2]) . . tok_ds = tok_ds.rename_columns({&quot;target&quot;: &quot;labels&quot;}) . tok_ds . Dataset({ features: [&#39;id&#39;, &#39;keyword&#39;, &#39;location&#39;, &#39;text&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 7613 }) . tok_ds[0] . {&#39;id&#39;: 1, &#39;keyword&#39;: None, &#39;location&#39;: None, &#39;text&#39;: &#39;Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all&#39;, &#39;labels&#39;: 1, &#39;input&#39;: &#39;Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all&#39;, &#39;input_ids&#39;: [1, 581, 65453, 281, 262, 18037, 265, 291, 953, 117831, 903, 4924, 17018, 43632, 381, 305, 2], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} . . Validation, Traning, Testing . eval_df = pd.read_csv(&quot;test.csv&quot;) eval_df.head() . id keyword location text . 0 0 | NaN | NaN | Just happened a terrible car crash | . 1 2 | NaN | NaN | Heard about #earthquake is different cities, s... | . 2 3 | NaN | NaN | there is a forest fire at spot pond, geese are... | . 3 9 | NaN | NaN | Apocalypse lighting. #Spokane #wildfires | . 4 11 | NaN | NaN | Typhoon Soudelor kills 28 in China and Taiwan | . eval_df.describe(include=&quot;object&quot;) . keyword location text . count 3237 | 2158 | 3263 | . unique 221 | 1602 | 3243 | . top deluged | New York | 11-Year-Old Boy Charged With Manslaughter of T... | . freq 23 | 38 | 3 | . model_dataset = tok_ds.train_test_split(0.25, seed=34) model_dataset . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;keyword&#39;, &#39;location&#39;, &#39;text&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 5709 }) test: Dataset({ features: [&#39;id&#39;, &#39;keyword&#39;, &#39;location&#39;, &#39;text&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 1904 }) }) . eval_df[&quot;input&quot;] = eval_df[&quot;text&quot;] eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True) . Training Models . from transformers import TrainingArguments, Trainer, DataCollatorWithPadding . bs = 128 epochs = 4 . data_collator = DataCollatorWithPadding(tokenizer=tokz) . training_args = TrainingArguments(&quot;test-trainer&quot;) . model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2) . Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: [&#39;lm_predictions.lm_head.bias&#39;, &#39;mask_predictions.dense.bias&#39;, &#39;mask_predictions.LayerNorm.bias&#39;, &#39;mask_predictions.classifier.weight&#39;, &#39;mask_predictions.LayerNorm.weight&#39;, &#39;lm_predictions.lm_head.LayerNorm.weight&#39;, &#39;lm_predictions.lm_head.dense.bias&#39;, &#39;mask_predictions.dense.weight&#39;, &#39;lm_predictions.lm_head.dense.weight&#39;, &#39;lm_predictions.lm_head.LayerNorm.bias&#39;, &#39;mask_predictions.classifier.bias&#39;] - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: [&#39;classifier.weight&#39;, &#39;pooler.dense.weight&#39;, &#39;classifier.bias&#39;, &#39;pooler.dense.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . trainer = Trainer( model, training_args, train_dataset=model_dataset[&#39;train&#39;], eval_dataset=model_dataset[&#39;test&#39;], data_collator=data_collator, tokenizer=tokz, ) . trainer.train() . The following columns in the training set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: location, text, id, input, keyword. If location, text, id, input, keyword are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. /opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 5709 Num Epochs = 3 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 2142 . . [2142/2142 03:04, Epoch 3/3] Step Training Loss . 500 | 0.491000 | . 1000 | 0.406300 | . 1500 | 0.323600 | . 2000 | 0.265800 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to test-trainer/checkpoint-500 Configuration saved in test-trainer/checkpoint-500/config.json Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json Saving model checkpoint to test-trainer/checkpoint-1000 Configuration saved in test-trainer/checkpoint-1000/config.json Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json Saving model checkpoint to test-trainer/checkpoint-1500 Configuration saved in test-trainer/checkpoint-1500/config.json Model weights saved in test-trainer/checkpoint-1500/pytorch_model.bin tokenizer config file saved in test-trainer/checkpoint-1500/tokenizer_config.json Special tokens file saved in test-trainer/checkpoint-1500/special_tokens_map.json Saving model checkpoint to test-trainer/checkpoint-2000 Configuration saved in test-trainer/checkpoint-2000/config.json Model weights saved in test-trainer/checkpoint-2000/pytorch_model.bin tokenizer config file saved in test-trainer/checkpoint-2000/tokenizer_config.json Special tokens file saved in test-trainer/checkpoint-2000/special_tokens_map.json Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=2142, training_loss=0.3674473464210717, metrics={&#39;train_runtime&#39;: 184.9649, &#39;train_samples_per_second&#39;: 92.596, &#39;train_steps_per_second&#39;: 11.581, &#39;total_flos&#39;: 222000241127892.0, &#39;train_loss&#39;: 0.3674473464210717, &#39;epoch&#39;: 3.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; preds = trainer.predict(eval_ds).predictions.astype(float) preds . The following columns in the test set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: location, text, id, input, keyword. If location, text, id, input, keyword are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Prediction ***** Num examples = 3263 Batch size = 8 . . [408/408 00:05] array([[-2.78964901, 3.02934074], [-2.77013326, 3.00309706], [-2.74731326, 2.972296 ], ..., [-2.8556931 , 3.08512282], [-2.7085278 , 2.88177919], [-2.7887187 , 3.00746083]]) . 1. Just happened a terrible car crash 2. Heard about #earthquake is different cities, stay safe everyone. 3. There is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all. . The above are samples from our Test set, looks all disaster tweets which seems to have been predicted correctly. This is my first iteration in which I tried mostly editing from Jeremy&#39;s notebook on getting started with NLP in about 1 hour. . &lt;/div&gt; .",
            "url": "https://kurianbenoy.github.io/ml-blog/kaggle/fastaicourse/nlp/huggingface/2022/05/23/NLPKaggleComp.html",
            "relUrl": "/kaggle/fastaicourse/nlp/huggingface/2022/05/23/NLPKaggleComp.html",
            "date": " â€¢ May 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Practical Deep Learning for Coders Course - Lesson 4",
            "content": "Introduction to lesson . Almost 100+ people watched live virtually and lesson were held live in front of a bunch of audience in University of Queensland. Prof. John Williams opened session by telling about filling a separate form, for people interested in attending the hackathon organized end of the course. . During the start Jeremy mentioned he would love folks to organize a online hackathon by community for folks attending remotely as well. Yet right now Jeremy and John doesn&#39;t have the capacity to organize one. . Todays lesson is something a lot of regulars of fast.ai course are excited about as it covers really new material on transformers. . Why using a different framework - Transformers . Since this course is fastai, it may feel a bit weird when we are today going to use a different library called transformers. . Important:As practitioners, it&#39;s important for us to learn more than one framework. . Note: Differences with fastai and transformers: . Transformers provide lot of state of art models, and the Tokenizers library build with Rust is really good at the moment. | It&#39;s good to get exposure to a library which is not so layered like fast.ai, which is reason that makes it super useful for beginners. | ULMFiT architecture . The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm called Universal Language Model Fine-tuning for Text Classification aka ULMFiT which was first presented actually in a fastai course. . What&#39;s a pretrained model and what is finetuning? . Consider finetuning, as tweaking functions in such a way when if you are already some values of a, b lever are good and optimal for a particular function. Then tweaking value of c is easier right? . Important: A pre-trained model is a bunch of parameters that have already been fitted, where some of them weâ€™re already pretty confident of what they should be, and some of them we really have no idea at all. And so fine-tuning is the process of taking those ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit. . Steps in ULMFiT . ULMFiT archtecture consits of three steps: . Training a language models with general dataset like wikipedia. So it gets so good in predicting next words. Now in transformers one big difference compared to ULMFiT is we use masking instead of predicting next word | IMDB lnagage build a language model, build on top of LM for wikipedia | In three step, is where model classifier comes and based on this label sentences as postive, negative etc. | . Fundamental libraries in ML . Four fundamental libraries you always need in datascience are: . NumPy | Pandas | matplotlib | Pytorch . Important: It looks pretty cool, if you build the state of art stuff. Yet if you don&#8217;t know fundamentals, you will encounter trouble. So i will recommend you to get started by first complete reading the Deep Learning for Coders book, then the Macinskey book on Python for Data Analysis, 3E which is free completely online. | NLP notebook tokenization . Getting started with NLP for absolute beginners . It&#39;s been only a year or two since NLP has been getting good results, for computer vision things are being optimistic for a long time now. . Tokenization, is converting the text blurbs into a set of small tokens. | Numericalization is the process of converting these tokens to numbers for models to train | . We used deberta-v3 as base model as some models are always found to give good results. Yet there are lot of pretrained models available in public which can just found by searching like Patent for patent models in Huggingface models hub. . Note: (Jeremy) For under 2000 words use transformers approach, for more than 2000 words per sequence it would a good idea to try ULMFiT also along with transformers. . Test, Validation, Training Dataset . The most important concept in ML is creating: . Test set | Validation set | Training set . Important: (Jeremy) Kaggle competitions are really a good way to create a good validation set... Beginners generally tend to overfit ... In real world outside of kaggle you often won&#8217;t know it&#8217;s overfit. You just destroy value for organizations silently... You really don&#8217;t get it untill you screw it up a few times. How (and why) to create a good validation set | Test Set is a separate data which is not used by ML model for learning. It&#39;s kept as separate hold out dataset for further testing. . Understanding metrics . With the validation set, we are measuring some metrics like accuracy which tell how good our ML model is. In Kaggle for every competition there is a metric available to optimize based on. . Why metrics is different from loss? . . Important: (Jeremy) If you were taking something like accuracy as loss function for classifying cats vs dogs. Then it may go on finding gradient and optimizing, yet at some point if inacurate cats are still labelled as dogs by models. Evaluating with accuracy as loss functions will be same. We can&#8217;t proceed further with such a loss function, that&#8217;s why we use functions like MSE for loss usually. Metrics in real world is ofcourse a big issue, so with one ML model which claims to have got good results in a particular metrics when implemented has caused issues in real world which affect lot of people. . So check the article written by Rachael Thomas on The problem with metrics is a big problem for AI. . Pearson Coefficient . Understanding metrics is very key, especially in Kaggle competitons. According to this Kaggle competition page: &quot;Submissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores.&quot; . Note: &quot;This coefficient is usually abbreviated using the single letter r. It is the most widely used measure of the degree of relationship between two variables. r can vary between -1, which means perfect inverse correlation, and +1, which means perfect positive correlation. &quot; source: Kaggle Notebook . Jeremy&#39;s way of teaching this concept was explaining with code for us to get intuition . import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_california_housing df = fetch_california_housing(as_frame=True) df = df[&quot;data&quot;].join(df[&quot;target&quot;]).sample(1000, random_state=52) df.head() . MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude MedHouseVal . 7506 3.0550 | 37.0 | 5.152778 | 1.048611 | 729.0 | 5.062500 | 33.92 | -118.28 | 1.054 | . 4720 3.0862 | 35.0 | 4.697897 | 1.055449 | 1159.0 | 2.216061 | 34.05 | -118.37 | 3.453 | . 12888 2.5556 | 24.0 | 4.864905 | 1.129222 | 1631.0 | 2.395007 | 38.66 | -121.35 | 1.057 | . 13344 3.0057 | 32.0 | 4.212687 | 0.936567 | 1378.0 | 5.141791 | 34.05 | -117.64 | 0.969 | . 7173 1.9083 | 42.0 | 3.888554 | 1.039157 | 1535.0 | 4.623494 | 34.05 | -118.19 | 1.192 | . np.set_printoptions(precision=2, suppress=True) # to get correlation coefficent between every row of matrix with every other matrix np.corrcoef(df, rowvar=False) . array([[ 1. , -0.12, 0.43, -0.08, 0.01, -0.07, -0.12, 0.04, 0.68], [-0.12, 1. , -0.17, -0.06, -0.31, 0. , 0.03, -0.13, 0.12], [ 0.43, -0.17, 1. , 0.76, -0.09, -0.07, 0.12, -0.03, 0.21], [-0.08, -0.06, 0.76, 1. , -0.08, -0.07, 0.09, 0. , -0.04], [ 0.01, -0.31, -0.09, -0.08, 1. , 0.16, -0.15, 0.13, 0. ], [-0.07, 0. , -0.07, -0.07, 0.16, 1. , -0.16, 0.17, -0.27], [-0.12, 0.03, 0.12, 0.09, -0.15, -0.16, 1. , -0.93, -0.16], [ 0.04, -0.13, -0.03, 0. , 0.13, 0.17, -0.93, 1. , -0.03], [ 0.68, 0.12, 0.21, -0.04, 0. , -0.27, -0.16, -0.03, 1. ]]) . def relation_matrix(x, y): return np.corrcoef(x, y)[0][1] . np.corrcoef(df.HouseAge, df.MedHouseVal) . array([[1. , 0.12], [0.12, 1. ]]) . relation_matrix(df.HouseAge, df.MedHouseVal) . 0.1165853555067797 . When I ran through this code I was thinking about how [0,1] element in this corelation matrix is 0.12, when value of relation_matrix returns something as 0.11658535. I asked this simple doubt in the forum and after a while, I got answer from one of the course TAs Nick(n-e-w). . . Note: This is one of the best things IMO about taking the course live, rather than attending online. There is lot more activity, and you even get some of your questions answered by lot of experts and even Jeremy too. . def show_corr(df, a, b): x, y = df[a], df[b] plt.scatter(x, y, alpha=0.5, s=4) plt.title(f&quot;{a} vs {b}; r: {relation_matrix(x, y):.2f}&quot;) . show_corr(df, &quot;MedInc&quot;, &quot;AveRooms&quot;) . show_corr(df[df.AveRooms &lt; 15], &quot;MedInc&quot;, &quot;AveRooms&quot;) . If you look at two graphs, once we removed the Average room &lt;15. We notice a huge difference in r value which denotes the pearson coefficent are sensitive to outliers. Thus we got an intutive feeling of what the metrics is doing, and how it&#39;s being affected by outliers. Even if you make small error in some of predictions you will notice a hugh bump in leaderboard which affects your position, as pearson correlation penalizes heavily for wrong predictions. . Next week, will be the fifth lesson and the last one for month of May. The course will resume again after a three weeks breaks during month of June when monsoon season delights us here in Kerala with rain and everyone else with more fastai. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastaicourse/2022/05/19/fastai-54.html",
            "relUrl": "/fastai/fastaicourse/2022/05/19/fastai-54.html",
            "date": " â€¢ May 19, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Practical Deep Learning for Coders Course - Lesson 3",
            "content": "Lesson Setup . There was a minor delay in streaming the lessons today, as today the sessions where being conducted in-person by Jeremy at University of Queensland. There were 130 people watching live in youtube, while there was some notable absentees like Sanyam Bhutani. . Jeremy started the lesson by saying that usually lesson 1 and 2 are easy for everyone, while it&#39;s usually from lesson 3 things start getting hard. There is also a lesson 0 on how to do fast.ai? . I had the previously written about fastai lesson 0, where Jeremy mentioned about How to do fast.ai lesson through the following five steps: . Watching lecture/book (watching the video first without trying anything) | Running notebook and Experimentation (going through lesson notebooks and experimenting stuff) | Reproduce Results (try with fastai clean notebook version, see if you are able to understand and do things on your own) | Working on a different dataset (play with a different dataset, paraticipate in kaggle ...) | Always studying done with other people is the best activity. So it&#39;s great to participate in study groups like Delft-fastai sessions. . This week, Jeremy showcased various work based on which all got highest number of votes in share-my group section. My work also got featured ðŸ™‚. . . Dogs vs Cat notebooks- which image models are the best? . Today Jeremy featured, paperspace gradient platform. He has been using it for paperspace, and it&#39;s totally amazing. He got something done by them to update fastbook regularly. . Important: In lesson2 the main things, is not about taking a particular platform and deploying them through javascript websites or online applications. But the key thing is to undestand the concept. There are two pieces: . Note: 1. The Training piece by end of which you get a model.pkl file. Once you got that (train.ipynb) . Note: 2. Then part which takes inputs, spits out output ... this separate step is deploying (app.ipynb) . Finding good image models, by baselines results along with inference time will help us choose good architecture. He tried levit_models, which didn&#39;t work really great. . From [13:52] he experiments with convnext tiny models from timm library. It got really good accuracy with almost 0.05 loss. At the moment for computer vision there are lot of good architectures, which beats resnets really well. In this case for predicting 37 breeds of dogs we can find categories in dataset using vocab of dataloaders in model. . labels = model.dls.vocab . It&#39;s very important to understand what&#39;s in a model? Using get_submodule in pytorch we can look at the various neural networks, what is their input and ouput and each layers. [21:24] . Let&#39;s explore the architecture of a translation model which translates from their target form to english. . from transformers import AutoModelForSeq2SeqLM model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-mul-en&quot;) . Looking at model architecture . model . MarianMTModel( (model): MarianModel( (shared): Embedding(64172, 512, padding_idx=64171) (encoder): MarianEncoder( (embed_tokens): Embedding(64172, 512, padding_idx=64171) (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512) (layers): ModuleList( (0): MarianEncoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (activation_fn): SiLUActivation() (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (1): MarianEncoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (activation_fn): SiLUActivation() (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (2): MarianEncoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (activation_fn): SiLUActivation() (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (3): MarianEncoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (activation_fn): SiLUActivation() (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (4): MarianEncoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (activation_fn): SiLUActivation() (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (5): MarianEncoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (activation_fn): SiLUActivation() (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) ) ) (decoder): MarianDecoder( (embed_tokens): Embedding(64172, 512, padding_idx=64171) (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512) (layers): ModuleList( (0): MarianDecoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (activation_fn): SiLUActivation() (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (encoder_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (1): MarianDecoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (activation_fn): SiLUActivation() (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (encoder_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (2): MarianDecoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (activation_fn): SiLUActivation() (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (encoder_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (3): MarianDecoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (activation_fn): SiLUActivation() (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (encoder_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (4): MarianDecoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (activation_fn): SiLUActivation() (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (encoder_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) (5): MarianDecoderLayer( (self_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (activation_fn): SiLUActivation() (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (encoder_attn): MarianAttention( (k_proj): Linear(in_features=512, out_features=512, bias=True) (v_proj): Linear(in_features=512, out_features=512, bias=True) (q_proj): Linear(in_features=512, out_features=512, bias=True) (out_proj): Linear(in_features=512, out_features=512, bias=True) ) (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=512, out_features=2048, bias=True) (fc2): Linear(in_features=2048, out_features=512, bias=True) (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) ) ) ) (lm_head): Linear(in_features=512, out_features=64172, bias=False) ) . . Looking at layer 1 self_attn_layer_norm . attention_layer = model.get_submodule(&#39;model.encoder.layers.0.self_attn_layer_norm&#39;) list(attention_layer.parameters()) . [Parameter containing: tensor([0.3865, 0.6348, 0.6938, 0.7140, 1.1017, 1.0888, 0.7801, 0.7572, 0.7402, 0.5655, 0.5940, 0.7477, 0.6920, 0.6781, 0.5128, 0.5862, 0.7173, 0.5140, 0.5940, 0.5998, 0.5002, 0.5931, 0.3720, 0.8686, 0.6557, 0.7436, 0.7564, 0.5402, 0.6773, 0.6831, 0.7060, 0.8484, 0.8874, 0.9380, 0.7360, 0.6073, 0.7911, 0.6247, 0.6225, 0.7281, 0.7470, 0.8066, 0.6336, 0.5607, 0.6914, 0.7630, 1.0365, 0.5133, 0.8260, 0.9167, 0.6362, 0.6375, 0.7296, 1.0838, 0.7916, 0.8332, 1.0474, 0.9655, 0.9446, 0.8361, 0.9928, 0.7550, 0.8335, 0.9597, 0.3449, 0.6119, 0.9266, 0.8208, 0.7301, 0.9969, 0.4639, 0.6579, 1.0493, 0.9808, 0.9181, 0.7736, 0.7346, 0.9642, 1.2211, 1.3974, 1.3712, 1.4836, 1.2050, 1.1015, 1.3986, 1.4113, 1.3771, 1.5623, 1.5389, 1.0727, 1.5310, 1.3641, 1.5365, 1.4774, 1.4893, 1.4168, 1.5904, 1.5720, 1.3812, 1.5914, 1.5096, 1.2807, 0.1877, 1.3947, 1.6565, 1.2572, 1.7532, 1.7136, 1.5001, 1.7059, 1.6033, 1.5448, 1.5357, 1.5565, 1.5366, 1.3784, 1.6677, 1.6570, 1.6885, 1.6925, 1.5795, 1.6837, 1.7601, 1.6240, 1.8309, 1.6668, 1.7021, 1.7827, 1.8194, 1.8531, 1.9633, 1.7518, 1.9518, 1.8846, 2.0106, 1.9608, 1.8964, 1.9245, 0.0996, 1.8191, 1.8534, 1.7096, 1.7831, 0.1533, 2.0808, 1.8960, 2.1153, 1.8570, 2.0739, 2.1022, 2.0319, 1.3613, 1.9232, 2.1441, 2.0704, 2.1557, 2.1526, 2.2401, 2.0910, 1.8356, 2.1069, 1.7451, 0.1487, 2.1800, 2.1589, 2.0273, 0.1957, 2.2119, 2.1048, 1.4881, 1.7567, 2.2064, 2.1753, 2.2111, 2.1907, 2.1288, 1.8702, 2.1218, 2.1744, 2.2581, 2.2565, 2.1913, 2.0952, 2.2975, 1.9853, 1.9851, 2.1758, 2.1094, 2.0666, 2.0578, 1.7592, 2.1246, 2.1616, 2.1781, 2.1823, 2.4415, 2.0122, 1.9394, 2.1719, 2.1455, 2.3547, 1.0006, 2.1169, 1.6765, 2.2037, 2.1994, 2.2939, 2.1233, 2.1261, 2.1542, 2.1301, 2.0364, 2.2253, 2.1832, 2.2080, 2.0617, 2.2758, 2.1373, 2.2573, 2.0367, 2.2055, 2.2531, 1.9362, 2.1346, 2.3110, 1.8304, 2.2435, 2.0757, 2.1346, 2.0784, 2.2972, 1.9981, 2.2595, 2.3887, 2.3544, 2.1077, 2.2306, 2.2086, 1.6925, 2.1120, 2.2147, 2.2832, 2.1880, 2.0909, 2.1869, 2.3249, 2.2425, 2.2322, 2.2695, 2.3331, 0.1346, 0.2001, 1.9555, 2.0758, 2.0961, 2.2567, 0.4750, 0.5842, 0.7058, 0.7570, 0.9744, 1.0287, 0.9519, 0.8539, 0.6670, 0.0686, 0.5976, 0.6930, 0.7278, 0.5867, 0.5813, 0.7097, 0.5000, 0.6474, 0.5425, 0.5578, 0.5803, 0.6271, 0.6408, 0.0821, 0.6325, 0.8464, 0.9188, 0.7320, 0.1289, 0.1581, 0.7063, 0.8729, 0.7022, 0.8077, 0.7002, 0.6772, 0.5950, 0.6649, 0.7646, 0.4813, 0.7579, 0.5831, 0.4914, 0.7263, 0.5337, 0.5253, 0.7073, 0.3907, 0.7041, 0.6702, 0.4874, 0.5163, 0.2580, 0.6476, 0.5674, 0.4555, 0.5476, 0.5859, 0.6279, 0.4089, 0.5099, 0.5995, 0.5399, 0.7964, 0.4036, 0.6919, 0.6908, 0.5914, 0.5730, 0.6122, 0.4277, 0.4590, 0.7666, 0.9008, 0.3882, 0.1257, 0.6154, 0.6206, 0.1595, 0.6308, 0.4924, 0.5181, 0.5823, 0.2778, 0.8624, 0.2661, 0.7717, 0.9022, 1.2887, 1.2015, 0.6473, 0.4860, 0.4110, 0.4339, 0.5128, 1.1724, 1.1852, 1.2922, 1.0709, 1.2392, 1.2499, 1.4100, 1.3137, 0.8466, 1.4344, 1.4693, 0.6968, 0.1751, 0.1710, 0.1834, 1.4736, 1.6201, 1.3277, 1.6475, 1.4915, 1.5697, 1.4164, 1.7855, 0.0784, 1.7240, 1.5680, 1.7145, 1.9040, 1.7964, 1.9526, 1.9328, 2.0737, 1.9253, 1.7730, 2.2707, 2.0602, 0.4566, 0.5279, 2.1403, 2.0589, 2.0557, 2.1391, 2.1761, 1.8147, 2.0583, 1.8788, 2.0470, 2.0793, 2.0560, 2.2968, 0.2280, 2.2384, 0.1449, 2.3148, 2.2568, 2.1043, 2.2506, 0.1906, 2.1942, 2.3548, 2.2405, 2.1008, 2.2179, 2.2754, 0.6110, 0.2974, 1.9307, 2.1931, 2.0484, 2.0105, 2.1261, 2.0659, 2.1462, 2.1739, 1.9466, 2.4105, 2.2565, 2.0342, 2.1688, 0.2608, 2.0383, 2.0664, 1.9995, 2.1393, 2.2680, 2.0550, 2.2346, 1.9870, 2.0796, 1.9112, 1.2930, 2.2390, 2.1678, 0.1801, 2.0002, 1.6783, 2.0918, 2.3177, 1.8342, 1.9244, 1.9471, 2.2717, 2.1227, 2.2932, 2.3473, 1.7774, 2.0945, 2.3712, 2.1550, 2.0802, 0.1087, 2.2277, 1.9290, 2.2212, 2.0705, 1.8797, 2.1542, 0.3608, 2.1922, 2.1362, 2.1825, 1.9593, 2.1429, 0.2623, 1.6499, 2.0807, 2.0261, 2.1480, 1.9283, 0.1497, 2.1901, 2.0398, 2.0140, 2.5195, 2.0685, 1.4206, 2.0745, 2.2225, 0.1621, 2.2012, 0.4932, 2.0481, 2.1097, 2.3599, 0.4743, 1.9034, 2.2135, 2.0947, 2.1751, 1.7660, 2.4012, 2.1536, 1.9608, 2.1268, 1.9698, 2.2014, 2.3058, 2.1618, 1.8719, 1.9626, 2.2343], requires_grad=True), Parameter containing: tensor([ 7.7839e-02, 1.4282e-01, -6.7494e-02, 6.3598e-02, -1.2071e-01, 7.2978e-02, -1.3550e-01, 3.5607e-02, -4.4458e-02, -4.4257e-03, -3.3140e-01, -8.8216e-02, 2.0695e-01, -1.7521e-01, -7.0075e-02, -2.3476e-01, -3.5785e-01, -4.3914e-01, 1.4167e-01, -9.0072e-02, -1.6590e-01, -2.4325e-02, -9.6055e-02, -3.2896e-01, 1.2258e-02, -1.0973e-03, 2.2662e-01, -1.3086e-02, -2.1918e-01, -4.5178e-02, -1.9418e-01, -1.8878e-02, -1.3459e-02, -2.9698e-01, -3.9941e-02, -9.4998e-02, -1.9507e-01, -4.1943e-02, 1.6590e-01, -1.1282e-01, 1.1039e-01, 2.5711e-02, -1.5641e-01, 5.5295e-02, -1.1544e-01, -1.7157e-01, -2.8929e-01, 2.3132e-01, -2.6698e-01, -2.9870e-02, -1.4797e-02, -1.4169e-01, -4.8199e-03, 1.4835e-02, -8.9909e-02, -4.6198e-02, -2.8071e-01, -4.3290e-01, -1.6699e-01, -2.0422e-01, -5.8818e-03, -2.2520e-01, 6.2375e-03, 3.9504e-02, 7.5439e-02, -1.4287e-01, -5.1881e-01, -5.5721e-02, -5.5866e-02, -5.3829e-01, -1.4044e-02, -9.2953e-02, -1.1587e-01, -3.8476e-02, -2.5480e-01, -5.7539e-02, -2.8871e-01, 3.5020e-02, -3.1672e-02, -5.8393e-02, -3.2713e-01, -1.8932e-01, 8.0913e-02, -4.6087e-01, -6.5291e-02, -4.0539e-01, 6.4874e-02, -1.7552e-01, 4.5883e-02, 9.9371e-03, 1.4575e-02, -1.4779e-01, 3.0300e-01, -7.1591e-02, -3.0603e-02, -5.1550e-02, 3.3196e-01, -2.6409e-01, -1.0252e-01, -9.0839e-02, -6.5229e-02, 4.6278e-03, 6.9909e-01, -3.8764e-01, -1.8178e-01, -1.6395e-01, -4.2978e-01, -9.3517e-02, -2.7543e-02, -1.2259e-01, -2.8473e-01, 2.5956e-01, -2.6014e-01, 5.4886e-02, -2.7227e-02, -1.3363e-01, -1.5168e-01, 8.5377e-02, 2.9195e-01, 2.1162e-02, -3.9784e-02, 4.4097e-02, 9.6993e-02, 1.4139e-01, 2.4818e-01, 1.8267e-02, -1.1592e-01, 1.0816e-01, 7.5200e-02, -1.3003e-01, -8.0244e-03, 5.8670e-02, -3.7428e-01, 2.2588e-01, -5.0269e-01, -2.3895e-01, 8.2600e-02, -6.8347e-02, -1.0482e+00, -1.3551e-01, 1.1412e-02, -2.1185e-01, -2.4042e-01, 2.4737e-02, -2.5176e-01, 1.5020e-01, -2.3560e-01, 1.1241e-01, -6.4413e-02, -3.5118e-01, -1.2333e-01, 1.9045e-01, 4.3384e-02, -2.4544e-01, -4.2071e-01, -7.8986e-02, -4.2295e-02, 4.0794e-01, -3.2176e-01, -5.9337e-01, -6.2764e-02, 1.0759e-01, -4.0607e-01, -1.3816e-01, -3.3327e-01, -2.0288e-01, -3.6235e-01, -4.0601e-01, -3.3251e-01, 1.0679e-01, -3.2651e-01, -4.5523e-01, 9.8463e-03, -1.7090e-01, 5.6157e-02, -2.0125e-01, -1.0815e-01, -1.1430e-01, -4.0327e-02, -3.9167e-01, -3.6428e-01, -4.4570e-01, -8.9959e-02, -4.9760e-01, -1.0579e-01, 1.3707e-01, -7.0252e-02, 2.7966e-02, -2.4773e-01, -5.1971e-04, 8.3816e-02, 2.1685e-02, -6.9780e-01, 2.2206e-02, 3.3752e-01, -3.2891e-01, -7.8279e-02, 3.3331e-03, -1.5812e-01, -7.3529e-02, -2.4885e-01, 7.1563e-03, -1.0669e-01, -9.1697e-02, 3.7219e-02, 2.2590e-01, -3.7476e-01, 8.3716e-02, 5.5841e-02, -3.0678e-01, -3.4485e-01, -4.4003e-01, 1.9830e-01, -4.7639e-01, -6.4421e-02, -2.7313e-01, -1.4385e-01, -1.4548e-01, -3.6821e-01, 2.6972e-01, -3.0483e-01, 4.1683e-02, -2.3375e-02, -2.3032e-01, -4.5438e-01, -2.6145e-01, -2.2000e-01, -5.7517e-02, 4.7594e-02, -9.9610e-03, -4.2952e-01, 1.8124e-01, -1.1407e-01, -2.7262e-01, -1.1815e-01, -2.3155e-01, -4.2597e-01, -4.4960e-01, -1.8752e-01, -3.0844e-01, 3.5617e-02, -3.7852e-01, -3.3136e-01, -1.9491e-01, -2.1862e-01, -3.3167e-01, 2.6676e-01, -1.9840e-01, -3.3605e-01, -1.6330e-01, -6.2717e-02, -8.3715e-01, -2.5243e-01, -1.3302e-01, -3.6257e-01, 5.8300e-01, -1.1160e-01, -1.1229e-01, -4.1968e-01, -1.0799e-01, -1.9890e-01, -6.1067e-02, -2.9817e-01, -6.8028e-02, -1.3047e-01, -8.3282e-01, -2.1888e-01, -1.1378e-01, -1.4994e-02, -3.3752e-01, 1.4736e-01, -2.0098e-01, -3.8907e-01, 1.4387e-01, -1.3784e-01, 1.6391e-02, -1.7244e-01, 7.5800e-02, -2.3648e-01, -3.8036e-01, 1.9662e-01, 7.4968e-02, -1.1686e-01, -3.6071e-01, -7.9299e-02, 1.8760e-01, 1.6195e-01, -3.2272e-01, -2.1438e-01, -7.2898e-02, 9.8829e-02, 7.1539e-02, 1.3703e-01, -1.5568e-01, 6.3408e-04, -3.5787e-02, -2.7407e-01, -5.7378e-02, -2.0438e-01, -2.4371e-02, 1.7313e-01, -4.1306e-01, -9.4938e-02, 3.8556e-02, -2.3727e-01, 5.0274e-02, -5.2022e-02, 6.9763e-03, 1.2209e-01, -1.4279e-01, -2.5014e-01, -1.8495e-02, -1.3463e-02, -2.4504e-01, -1.3166e-01, -7.7291e-02, 7.7370e-02, 1.1513e-02, -7.0425e-02, 1.5736e-01, -2.1174e-01, -4.2664e-02, -2.9207e-01, 3.2393e-02, 2.1656e-02, 9.9900e-02, -1.3805e-01, 2.5438e-01, 2.0831e-01, 3.6837e-02, -3.3914e-03, 4.1395e-01, 5.6420e-02, 8.9263e-02, 2.1450e-02, -5.5800e-02, 7.0606e-02, -4.1126e-02, 3.8725e-03, -1.5734e-01, 5.0738e-01, 1.5756e-02, 3.4117e-01, -3.4182e-01, 2.3014e-01, 2.9587e-02, -8.8264e-02, 3.3711e-01, -1.4313e-01, 1.5262e-01, -8.7762e-02, 2.4450e-01, -2.0987e-01, 1.9820e-01, 1.7844e-01, 1.4303e-01, -5.0851e-02, -9.4576e-02, 1.8408e-02, 1.1286e-01, 3.3272e-01, 3.5103e-01, -4.2428e-02, -1.9907e-01, 9.6479e-02, 3.2967e-02, -1.9729e-01, 2.2756e-01, 8.3037e-02, 2.5401e-01, 2.9031e-01, -1.5839e-01, -1.3418e-02, 1.0571e-01, -3.5190e-01, -8.5125e-02, 1.5848e-01, 2.5322e-01, 2.0388e-02, 1.4573e-01, 1.7365e-02, 3.1611e-01, -2.0127e-01, 8.0616e-02, -1.4502e-02, 6.7866e-01, 5.2572e-01, 6.3858e-02, 3.9846e-02, 5.1869e-01, -7.9728e-03, 3.9597e-01, 4.7967e-01, 2.7590e-01, 9.2782e-02, 3.3310e-01, 2.2875e-01, 3.4428e-01, 4.6610e-01, -1.0366e-01, 3.4020e-01, 2.3838e-01, 3.1878e-01, 1.2648e-01, 5.1629e-01, 3.4091e-01, 4.3710e-01, 6.2221e-01, 1.7226e-01, 4.4662e-01, 4.0081e-01, 7.7952e-01, 4.0586e-01, 1.0278e+00, 3.0402e-01, 1.5113e-01, 8.0986e-02, 2.9811e-01, 6.0928e-01, 3.3816e-01, 5.8209e-01, 5.3371e-01, 3.8662e-01, 2.0641e-01, 3.6023e-01, 3.1196e-02, 4.9345e-01, 3.2226e-01, 2.7840e-01, 2.7691e-01, 9.6109e-01, 1.2737e-01, 4.1566e-01, 3.9062e-01, 3.0825e-01, 4.9397e-01, 4.5440e-01, 5.2856e-01, 2.1089e-01, 4.5024e-01, 3.9093e-01, 4.3543e-01, 1.2896e-01, 3.8236e-01, 5.7791e-02, 5.9610e-02, 3.2190e-01, 4.1077e-01, 6.7217e-01, 3.1503e-01, 4.5539e-01, 3.8127e-01, 3.7299e-01, 4.9606e-01, 5.1592e-01, 8.7739e-01, 1.2913e-01, 3.2640e-01, 5.1213e-01, 2.5983e-01, 3.1244e-01, 8.0140e-02, 3.2804e-01, 1.5592e-01, 4.3599e-01, 5.4296e-01, 3.3799e-01, 5.6262e-01, 9.3698e-01, 4.7990e-01, 4.9927e-02, 4.0214e-01, 5.5437e-01, 4.3915e-01, 1.3080e-01, 3.5957e-01, 6.5735e-02, 9.8948e-02, 4.7541e-01, 9.1836e-02, 3.4417e-01, 3.5615e-01, 4.0770e-02, 4.5717e-01, 6.4114e-01, 2.4542e-01, 5.0354e-01, 1.7951e-01, 6.0904e-01, 1.5958e+00, 2.1165e-01, 3.6238e-01, 2.0053e-01, 4.2348e-01, 6.8393e-01, 8.5349e-01, 1.3414e-01, -1.2184e-03, 4.1054e-01, 7.6441e-01, 6.1769e-02, 3.8833e-01, 3.6897e-01, 3.5290e-01, 2.8261e-01, 3.1730e-01, 4.8138e-01, -1.5993e-01, 3.7400e-01, 2.7083e-01, 2.0941e-01, 5.4596e-01], requires_grad=True)] . . Looking at shape of last layer . final = model.get_submodule(&#39;model.decoder.layers.5.final_layer_norm&#39;) final_paramaeters = list(final.parameters()) print(f&quot;{final_paramaeters = }&quot;) . final_paramaeters = [Parameter containing: tensor([ 9.2454, 9.3895, 9.3544, 9.0685, 9.2224, 9.8569, 9.3900, 9.4416, 9.4985, 9.2981, 9.5326, 9.2260, 8.8878, 9.4862, 9.5422, 9.3088, 9.6653, 8.9836, 9.5670, 9.0307, 9.4179, 9.8929, 9.3411, 8.9442, 8.3855, 9.0165, 9.5142, 9.5201, 9.2902, 9.5196, 8.8687, 9.3270, 8.7709, 9.5791, 9.4227, 8.9457, 9.4278, 9.2320, 9.5537, 9.3045, 9.2281, 9.1897, 8.9683, 9.3930, 9.1265, 9.2261, 9.1755, 9.2192, 9.1531, 9.2323, 9.1581, 9.3413, 8.4585, 9.3836, 9.7359, 8.8970, 9.4054, 8.9220, 9.2355, 9.6045, 9.6126, 9.4839, 9.2955, 9.2803, 9.5649, 8.8892, 9.4749, 8.8119, 9.3922, 9.0771, 9.7973, 8.9035, 9.7339, 9.1203, 9.5283, 8.9696, 8.4717, 9.3626, 9.3828, 7.9538, 8.8453, 9.0190, 9.3108, 8.3297, 8.7236, 8.8562, 9.1680, 8.8641, 7.8828, 8.7943, 8.4220, 8.8387, 9.3143, 8.1786, 9.1979, 9.0642, 8.2838, 8.6224, 8.8548, 8.2028, 8.3914, 9.4564, 10.2469, 9.0537, 8.7376, 9.3791, 8.5842, 8.4631, 8.6599, 8.8171, 7.8897, 8.6041, 8.4556, 8.9208, 10.1143, 7.9758, 8.2237, 8.5698, 9.2252, 8.1479, 8.0188, 8.9071, 8.1475, 9.6910, 8.2373, 8.2525, 8.6017, 8.4775, 7.6445, 8.5943, 8.4234, 9.5359, 7.9101, 9.0395, 8.2788, 9.1683, 8.9006, 9.3443, 10.6461, 8.7802, 8.7067, 8.1328, 8.4786, 9.5398, 8.9038, 8.7195, 8.6432, 8.6484, 8.0920, 7.6238, 8.0674, 9.1098, 8.9414, 8.5768, 8.5224, 8.2418, 8.2112, 8.5999, 8.4768, 8.9988, 9.0594, 8.4397, 7.2651, 8.8350, 8.4989, 8.2867, 9.2490, 8.9484, 9.0761, 9.4235, 8.6788, 8.3734, 8.5445, 8.6480, 8.5919, 8.7318, 8.9115, 8.3845, 7.7635, 8.0614, 8.0440, 8.3904, 9.2142, 8.9592, 8.3101, 8.5018, 8.3161, 8.6132, 8.5134, 8.6191, 9.2030, 8.4010, 8.6543, 8.9678, 8.5206, 8.7887, 8.4305, 8.9793, 8.4836, 8.3803, 8.5192, 9.0187, 8.2780, 8.4214, 8.5277, 8.3268, 8.6899, 8.8909, 8.5217, 8.8556, 8.1597, 9.0187, 8.8114, 9.0544, 8.1888, 8.0256, 8.2712, 7.8735, 8.3806, 8.3239, 8.1951, 8.1542, 8.8955, 8.1172, 8.7627, 8.6084, 8.8146, 8.5941, 8.4780, 7.9555, 8.5277, 8.8061, 8.1250, 8.5714, 8.6387, 7.6968, 8.5164, 8.5684, 8.8306, 8.1602, 8.7625, 8.7649, 8.5770, 8.8186, 8.6728, 8.8203, 8.8378, 8.8105, 8.2568, 8.4017, 9.9819, 9.0695, 8.9472, 8.4494, 7.6861, 8.1042, 9.4347, 9.3720, 9.0644, 9.1978, 9.8322, 9.0001, 9.1845, 9.4331, 9.3469, 11.0728, 9.3463, 8.5851, 9.6459, 9.1978, 9.2272, 9.5648, 9.5100, 9.6435, 9.5191, 9.8178, 9.3789, 9.5861, 9.2071, 9.2581, 8.5441, 9.6824, 9.0314, 9.2823, 10.2148, 10.1498, 9.3458, 8.9451, 9.7831, 9.0849, 8.7979, 9.0224, 8.8580, 9.6999, 9.0158, 9.4426, 9.2253, 9.1951, 9.4550, 9.1783, 9.5661, 9.3228, 9.4391, 9.2358, 9.1685, 8.8517, 9.4883, 9.0652, 9.4498, 8.6077, 9.7002, 10.4473, 9.9884, 8.8662, 9.4317, 9.2922, 9.0668, 9.7620, 9.2281, 9.4860, 9.6106, 8.0309, 8.9221, 9.0221, 9.0459, 10.2337, 9.7973, 9.5885, 9.0249, 8.8571, 8.7396, 8.9452, 9.2020, 9.1573, 8.4453, 9.3205, 8.6279, 8.8441, 8.9208, 9.7410, 8.9751, 9.3891, 9.5010, 8.9050, 8.8219, 8.4705, 9.4688, 9.2351, 9.1935, 9.7405, 9.1623, 8.1793, 8.0767, 8.1733, 8.9422, 8.4693, 8.9346, 9.1120, 8.0441, 9.5878, 9.5636, 8.8612, 9.0740, 9.1084, 9.7573, 9.8492, 9.6772, 9.1868, 8.7703, 8.4915, 8.4426, 8.7710, 9.0574, 8.4157, 10.3115, 9.0996, 8.5651, 9.0585, 8.4534, 8.7063, 8.4291, 8.3241, 7.9195, 9.0210, 8.3222, 8.5985, 8.7874, 9.1164, 10.2389, 7.7741, 8.5940, 9.1308, 9.3498, 8.7384, 8.3300, 8.2650, 8.7969, 8.6335, 8.6550, 8.7559, 8.2821, 8.7692, 8.7830, 8.4424, 8.6879, 8.6025, 8.6327, 8.8367, 9.4620, 8.5763, 8.3675, 8.4179, 9.2793, 8.8078, 9.3775, 9.6580, 10.1902, 8.9006, 8.5452, 8.6059, 8.5685, 8.4081, 9.1445, 8.5781, 8.9791, 8.7608, 8.6678, 8.4435, 7.6760, 8.6099, 8.8083, 8.1700, 8.5081, 8.1777, 9.2411, 8.9585, 8.1853, 8.3657, 7.9898, 8.8000, 8.1188, 9.3628, 8.9330, 7.7698, 9.6513, 9.2959, 9.1233, 9.0433, 8.2871, 8.7241, 8.2236, 8.3967, 8.2571, 9.3786, 8.6354, 8.7345, 8.3856, 8.4556, 8.7689, 8.7359, 8.6211, 9.7834, 8.9445, 8.8958, 8.1290, 8.5490, 9.0263, 8.3258, 8.2379, 8.8249, 8.7301, 8.6340, 9.3168, 8.7775, 9.9242, 8.9798, 9.1412, 8.5955, 8.1734, 8.9969, 9.5123, 9.0581, 8.2497, 8.3555, 9.3501, 8.7719, 8.4376, 8.8456, 8.2080, 8.9806, 8.5660, 9.1352, 8.5920, 8.2595, 8.1272, 9.0418, 8.6972, 8.3413, 8.2742, 8.3118, 8.2167, 8.5550, 8.7187, 8.8749, 9.7556, 8.4383, 9.0293, 8.1725, 8.5115, 8.9174, 8.9519, 9.0915], requires_grad=True), Parameter containing: tensor([-1.6685e+00, -6.0155e-01, -5.9975e-01, 8.4297e-01, 8.5853e-01, 5.6530e-02, -1.2840e+00, -5.1519e-01, 1.6774e+00, 3.2501e-01, 1.4737e-01, -9.6427e-01, 2.1513e-01, 9.5219e-01, -3.7011e-03, 6.6861e-01, 7.9758e-01, 2.4703e-01, -9.5743e-02, 1.9413e-01, -4.1348e-01, -8.3267e-01, 9.7684e-01, -5.1446e-01, 5.3158e-01, 1.0447e+00, 1.7422e-01, 1.8719e+00, 7.0798e-01, -5.2600e-01, 3.0636e-01, 3.1010e-01, -6.3830e-02, -2.3082e-01, 1.1787e+00, -2.5507e-01, -1.2747e+00, 7.3436e-01, -6.5267e-01, 1.0654e+00, 7.2399e-01, -1.2560e+00, -6.7986e-01, -2.0358e-01, -2.1730e-01, 5.1018e-02, 3.6179e-01, 2.0001e+00, -6.3287e-01, 1.5726e+00, 2.8116e-01, -5.0017e-01, -1.6484e+00, -9.0159e-01, -2.5041e-01, -1.7400e-01, 6.4630e-01, 5.9313e-02, -7.2617e-03, 5.0565e-01, 1.8716e+00, -8.8190e-01, -1.5941e-02, 7.8757e-02, -7.3102e-01, -4.5485e-01, 1.1036e+00, -3.2698e-01, -8.0969e-01, -6.6129e-01, -6.8337e-01, -1.6216e-01, -9.3829e-02, 6.4593e-01, -1.3784e+00, 5.6243e-01, 8.1852e-01, 1.3817e-01, 5.7122e-01, -7.8534e-01, -9.2640e-01, 1.3659e-01, -6.8277e-01, 8.1809e-01, -1.4720e-01, -2.1538e+00, -7.1303e-02, 3.9166e-01, -7.9192e-01, 1.0671e+00, 1.1110e+00, 9.8533e-01, -4.9213e-01, -8.4603e-01, -1.1119e+00, 1.6191e+00, 7.9375e-02, -1.0472e-01, -5.4553e-01, -2.3597e-01, -2.6790e-01, -1.5157e+00, -2.6880e+00, 1.6904e-01, 2.3876e-01, -5.1432e-01, 5.7074e-01, 1.5021e+00, -1.7612e+00, -5.1162e-01, 1.8071e+00, -2.2087e-01, 2.1651e-01, 3.1280e-01, -7.8104e-01, -2.3347e-01, 2.3287e+00, 4.3430e-01, 6.7748e-02, -7.1022e-01, 1.3716e+00, -6.8236e-01, -1.9249e-02, 6.1708e-01, 3.5377e-01, -3.0060e-01, 8.7717e-01, 7.6281e-02, 1.6436e+00, 6.5745e-02, 1.3911e+00, -1.1550e+00, -1.0942e+00, -5.4705e-02, -3.8439e-01, -2.0564e-01, -4.0284e-01, 1.8441e+00, 1.9942e+00, -3.3832e-01, -8.4892e-02, 2.6425e-01, -1.2417e-01, -8.9078e-01, 9.9491e-01, -1.2496e-01, 1.8860e-01, -1.9992e-01, 1.2828e+00, -1.6894e+00, 1.7569e+00, -1.2428e-01, -6.2974e-01, 9.5339e-01, 5.5913e-01, 8.3872e-01, 3.8710e-01, 4.7107e-01, 8.8813e-01, 1.5112e+00, 6.4772e-02, 2.2407e+00, -2.4373e+00, 5.4596e-02, -2.3119e+00, 7.8280e-01, -1.9582e+00, -4.4601e-01, -7.2071e-01, 1.0691e+00, -6.3960e-01, -9.6271e-01, 2.2167e+00, 1.6286e+00, 1.8287e-01, -1.0599e+00, 8.2727e-01, 4.2197e-01, -1.7488e-01, 2.2607e+00, 1.6864e+00, 1.5625e+00, -2.4543e-01, 1.7482e-01, -1.4680e+00, -6.5810e-01, -1.7268e-01, 4.3401e-02, 1.2926e+00, 4.0332e-01, 1.2770e-01, -5.4604e-02, 6.3163e-01, 5.8788e-01, 3.2761e-01, 5.9546e-01, -1.4995e-02, -2.2789e-01, -3.0784e-01, -1.0060e-01, -1.6770e-01, -1.0096e+00, 9.2021e-01, -8.9897e-01, -5.9694e-01, 8.2038e-01, -9.0749e-01, -3.0484e-01, 3.2038e-01, 1.2042e+00, 6.0027e-01, 1.8709e-02, -4.0982e-01, 9.0638e-01, -9.6504e-01, -6.3824e-01, -2.3503e-02, -2.9762e-01, 1.1074e+00, 1.2170e-01, 1.1205e+00, -1.9938e-01, -2.7814e-01, -3.8689e-01, 1.1914e+00, -6.5604e-01, 7.1130e-02, -7.0655e-01, 1.4939e+00, -2.6654e-01, 4.9578e-01, -1.8316e+00, -6.2531e-01, 2.2550e+00, -9.1826e-01, 2.1526e+00, 1.7631e-01, 1.2235e+00, -9.9429e-01, -8.9968e-01, -9.7487e-01, -3.5716e-01, -3.8364e-01, -2.2766e+00, -1.4803e+00, 2.7549e-01, -5.8828e-01, -4.4274e-01, 2.0661e-02, 9.6894e-01, -5.4657e+00, 3.6806e+00, -5.8913e-01, 6.1390e-02, 9.8940e-01, 1.8229e+00, 3.6467e-01, 2.9497e-01, 2.1930e+00, 1.8576e+00, -7.6800e-01, 1.3635e+00, 2.8457e-01, 2.9478e-02, -1.5696e+00, 6.0662e-01, -1.1586e+00, 7.8294e-01, 3.4371e-01, 1.4571e-01, -4.5860e-01, -1.1644e+00, -1.2903e-01, -1.0055e+00, -5.4373e-02, 1.3311e+00, -1.2074e+00, 8.7602e-02, 8.2454e-01, -2.2496e+00, 2.4152e+00, -7.4065e-02, 3.5327e-01, 1.2092e+00, 6.9553e-02, 2.4961e+00, -1.5597e+00, 4.1607e-01, -7.9795e-02, -4.4723e-01, 2.6720e-01, -1.9072e+00, -6.5835e-01, -2.5336e-01, -9.1617e-01, 8.8624e-01, -6.2251e-01, 9.6169e-01, 1.1279e+00, -5.6577e-01, 1.8407e-01, 6.5294e-01, -6.1990e-01, 7.9014e-01, -6.0878e-01, 1.0077e+00, 1.2790e+00, -1.3704e-02, 7.4945e-02, 5.6748e-01, 1.0100e+00, -2.2963e-01, -9.2723e-01, -3.3553e-01, -7.0238e-01, -2.3026e+00, -5.3322e-02, -9.2703e-01, 1.4448e+00, -8.7800e-01, -6.4034e-01, -1.2203e+00, -1.1720e+00, 4.9662e-01, 3.4336e-01, -1.3538e+00, 4.1525e-01, -6.6715e-01, 4.1263e-01, -4.0352e-01, -3.7377e-01, 2.3441e+00, 3.5528e-01, -3.1402e-01, 3.5890e+00, 2.8886e-02, 3.1700e-01, -7.7702e-01, 4.6834e-01, 5.4264e-01, -1.0964e+00, 1.4711e+00, 9.3168e-01, 5.4778e-01, -7.4466e-01, 7.7792e-01, 1.5176e+00, 1.6450e+00, 2.6295e-02, -1.8510e+00, 2.2687e-01, 1.3993e-01, 1.1727e+00, 6.4835e-02, 1.9505e-01, 2.2950e-01, -1.3806e+00, 7.7071e-02, -1.8424e+00, -9.5833e-01, -8.7708e-01, 9.1619e-01, 1.0074e+00, 8.0151e-03, 1.0098e+00, -3.9247e-02, -2.7759e-01, -2.1021e+00, -4.1539e-01, -1.5258e-01, 3.3655e-01, -2.6506e-01, 2.1964e+00, 6.0517e-01, 5.7097e-01, 7.5984e-02, 1.0848e+00, 4.8223e-01, 8.0175e-01, -9.1310e-01, 6.3781e-01, 1.1286e-01, 1.3899e+00, -4.5585e-01, -8.9240e-01, -5.6478e-01, -1.0510e+00, -6.3237e-01, 7.5205e-01, -5.0555e-01, -4.2338e-01, 1.1653e+00, -4.3769e-01, -4.9660e-01, 8.4734e-01, 3.1255e-01, 1.4222e+00, 5.1850e-01, 5.9261e-03, 6.8774e-01, -2.2485e+00, -2.1259e-01, 1.7378e-01, -3.9461e+00, 8.5505e-01, -1.4455e+00, 2.2031e-02, -8.7173e-01, 9.4395e-01, 1.3690e+00, 9.2501e-01, 5.9211e-01, 5.9655e-01, -9.7749e-01, 5.1079e-01, 1.7735e-02, 3.1332e-01, 2.8223e-01, 2.2100e-01, 9.7640e-01, 7.5128e-01, -1.2068e+00, 8.0254e-01, 4.7232e-01, -5.7225e-01, 3.0082e-01, -4.5279e-01, -3.4367e-01, -2.8903e-01, 1.1790e+00, -2.3224e+00, 7.0363e-01, 4.5137e-01, 1.5505e+00, 8.4144e-01, 3.9210e-02, -9.5217e-01, -9.1495e-01, -3.6971e-01, 1.3037e-01, 1.0739e+00, -5.2155e-02, -1.7844e+00, -6.9291e-01, 6.2565e-01, -1.6121e+00, -4.0668e-01, 6.9844e-01, 2.1026e-01, -3.4400e-01, -2.3706e-02, -4.4798e-01, 6.0481e-01, 7.8424e-01, 6.2746e-01, -7.7199e-01, 2.0300e-01, 9.1969e-01, -1.1502e+00, -3.1036e-01, 3.8410e-01, 3.3024e+00, 9.6322e-02, 3.5212e-01, 1.4104e+00, -2.7992e-01, 4.1524e-01, -1.1456e+00, -2.6424e-01, -6.5836e-02, -5.0440e-01, 5.7824e-01, -7.8925e-01, -2.0960e+00, -1.2973e-01, 1.0862e+00, 1.3762e+00, -3.2528e-02, -2.2924e+00, -8.9146e-01, -3.0597e+00, 6.0693e-01, -2.5389e-01, -2.9927e-01, 3.3115e-01, -4.1729e-01, 1.3418e+00, 8.3576e-01, -1.0882e+00, 1.0617e+00, -2.8175e-01, 1.1439e+00, -4.9022e-01, -1.1799e-01, -4.8219e-01, 9.3034e-02, 1.2776e+00, -1.2725e-01, 5.8007e-01, 1.3756e+00, 1.2398e-01, -3.1594e-01, -7.3134e-02, 4.6101e-01, 1.4797e-01, -8.3583e-01, -1.8117e+00, 1.3540e-01, 1.4121e-01, 5.1246e-01, 1.6791e-01, -1.5676e+00], requires_grad=True)] . . Looking at how neural network really work? . partial in python is something which is usually used in lot of languages. It&#39;s just subsituting value of x, in a function which is partial filled with already existing function. . Jeremy explained with this notebook today . I followed along the notebook, making slight changes in variable names, function names, along with changing defined value to expirement with how does a neural network really work notebook version on my own. . Note: The above trick when working with notebooks is a trick told by Alex Strick during delft-fastai sessions. . Important: Usually when I expalined this, one of my students said this is like how we draw an owl. In deep learning ..., there is just step 1, computer automatically does the step2 by drawing a beautiful owl. . Intuitive understanding with neural networks . Using RELUs we can tweak our function in such a way to fit the data. What neural networks, with a bunch of RELU functions does is it helps to optimize in such a way to fit any swiggly line or complex things which needn&#39;t be always quadratic. . Important: For Linear algebra, almost all time you need is matrix multiplcation. In schools, you learn linear algrebra as if you need tons of experience to do machine learning. Yet it&#8217;s this operation of matrix multiplication that GPUs are so good at it, and there are even tensor cores for this. . Refresher on matrix multiplication . Using Titanic dataset,see who survived and who didnt&#39; with excel to understand neural networks. [1:05:00] . Next week, we are going to look into how validation sets and more into metrics. We will be looking into Kaggle notebook on how to get started with NLP. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastaicourse/2022/05/10/fastai-53.html",
            "relUrl": "/fastai/fastaicourse/2022/05/10/fastai-53.html",
            "date": " â€¢ May 10, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Practical Deep Learning for Coders Course - Lesson 2",
            "content": "Lesson Setup . Jeremy was taking this session from his home, as the venue in University of queensland was already booked by someone else. Jeremy was really really pumped for this lesson and it&#39;s like going to the early days of fast.ai with lot of super exciting work happening. The magic of Top Down Learning ðŸµ:Just 1 week into the @fastdotai cohort, the â€œshare your projectsâ€ discussion is at 100+ messages ðŸ™The MOOC releases later this year, but the awesome community never ceases to amaze me. . &mdash; Sanyam Bhutani (@bhutanisanyam1) May 3, 2022 . Jeremy mentioned some technique on using Jupyter notebooks, and asked to take a look at jupyter extensions. The navigation section and how to collapse headings was explained during class. [24:00] . Fastbook Chapter 2 . This week we started by taking a look at putting model in production using fastai. This was the same thing which is covered in chapter 2 of Deep Learning book To build grizzly bears and teddy bears classifier. . Few things have changed in book in this version: . using search_images_ddg instead of bing search apis | using huggingfaces spaces as deployment instead of voila even though it&#39;s still worksRandomResizedCrop could be a good idea to understand different varieties of same image. . | . Does RandomResizedCrop crop duplicate the image -- i.e. you get multiple copies and you ensure that all the parts of the image are used in training? or does it just make one crop? . Jeremy answered it in video at [32:30]. His answer was it doesn&#39;t copying image. In each epoch everyimage get&#39;s written and what happens is in-memory image is being wrapped by recropping and colouring in realtime during model training. It&#39;s like infinitely multi-copies of images. . Check the book to learn more in detail about various augmentations. . Sanyam mentioned that RandomResized crop as a augmentation is very helpful: . Important: Actually this technique is SUPER helpful-in a recent interview, Chris Deotte (4x Grandmaster) shared how these resizing techniques helped them win a solo gold. This was in the petfinder Kaggle competition (2nd run of the comp) . . Note: Jeremy is running on a laptop with 4GB GPU. Jeremy says in GPU, just run one thing at a time else you will get CUDA error. . How to do fast.ai course . Tips for people in Yellow bucket: . Note: If you are in yellow, always stop try. First go ahead and watch video fully without touching your keyboard and write code. Then watch again and follow the course. This is an unusual way as it can&#8217;t be done in real college lectures, but it&#8217;s very effective way indeed. . I asked Wayde Gilliam who is a long term fastai community member after the lesson about his process of watching lectures. He was gracious enough to share it with mith . Important: 1. Watch the livestream and jot down timestamp to go back to for anything I found interesting in journal A (or just a piece of paper) . Important: 2. Go back through the video after 2-3 days, hit those spots I noted during the livestream. Will write detailed notes in another Journal (we&#8217;ll call that journal B) . Important: There&#8217;s too much info to digest in real-time so this approach works well and its what I&#8217;ve been doing for 4-5 yrs. . Huggingface spaces . Jeremy pointed to tanishq tutorial on Gradio + HuggingFace Spaces. . Also Jeremy mentioned some good tools which are useful: . Github Desktop: Hamel who was a employee in github previously, is even using github desktop. Some complicated stuff in git can be solved using this tool. Even knowing terminal is cool. | WSL: As a datascientist, you spend a lot of time in terminals. Just use ubuntu with windows terminal. Any time Jeremy shows in terminal, he just uses windows terminal. | In terminal, he uses Tmux as a terminal emulator as pointed out in fast.ai forums for my question. | . Jeremy like Windows due to easiness in streaming, good apps and recording capabilities. Yet Jeremy also has a linux environment with a good Deep learning jig. . Note: Jupyter notebooks debugging with magic methods %time, %debug . In fastai for inference, it returns back a tensor. One of issue in gradio tensors is not supported at moment. So we need to convert tensors to float and do prediction. . Jeremy created a cats vs dogs classifier using spaces. His daughter when realised he is building such a classifier googled something which is a mix of cat and dog. For that his initial prediction was like 50-50% for both cats and dogs. . This kind of shows how important the support system around you and how much they acknowledge the work you do. This personally touched me. As my sister was encouraging me to go an all-nighter to complete the Music genre classification spaces. . TODO: Look through Jeremy setup and how he worked with gradio in local [58:00 onwards 1:14:00] . fastsetup . Installing python and jupyter-notebooks with proper git and conda setup. . Fastai setup . Important: A big issue in laptops with linux or mac there is a python default version, don&#8217;t use that python. As that python version is for your operating system to do it&#8217;s stuff. Don&#8217;t mess on top of it. Use mamba based installation for fastai now: . mamba install fastai mamba install -c fastchan jupyter nbdev . Trying gradio API with github Pages . An example API in gradio Example Jeremy showcased . With live demo, we could have easily used it with any websites. Without any software just with the browser, you can run this file. That&#39;s the cool thing about javascript and can host in website called github pages . fetch(&#39;https://hf.space/embed/kurianbenoy/audioclassification/+/api/predict/&#39;, { method: &quot;POST&quot;, body: JSON.stringify({&quot;data&quot;:[ {&quot;data&quot;: null, &quot;is_example&quot;: true, &quot;name&quot;: &quot;000003.ogg&quot;} ]}), headers: { &quot;Content-Type&quot;: &quot;application/json&quot; }}) .then(function(response){ return response.json(); }) .then(function(json_response){ console.log(json_response) }) . . He used alembic theme. With a particular configuration. At top of any github pages, you should add three dashes. The world of javascript apps, he build this cool apps. . Important: The magic of using gradio APIs can be summarized as the following. It exposes a reliable way of sharing microservices. With this if you are just creating any hugging face spaces, with that APIs. You can use it any websites, apps etc. It looks to me there is no limitation with using Gradio API at the moment. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastaicourse/2022/05/03/fastai-52.html",
            "relUrl": "/fastai/fastaicourse/2022/05/03/fastai-52.html",
            "date": " â€¢ May 3, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Music genre classifier using fast.ai",
            "content": "Introduction . UPDATE - This project got featured in some of the cool project Jeremy Howard shared in lesson3 of fastaiv5 course. . During first lesson of Practical Deep Learning for Coders course, Jeremy had mentioned how using simple computer vision model we can build even a model to classify audio with image classification model itself. . Recently Kaggle grandmaster Rob Mulla conducted a challenge to classify music according to what genre it was. At stakes there was a RTX 3080 Ti GPU. Let&#39;s look how we can classify music genres using a simple computer vision model which was taught in the first lesson of fast.ai. . Downloading packages and importing libraries . . Note: I had already installed fastai, pytorch for training this model before hand. . ! pip install -Uqq kaggle git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[&quot;fastai&quot;] . . from fastai.data.all import * from fastai.imports import * from fastai.vision.all import * from huggingface_hub import push_to_hub_fastai . Collecting Data . In this piece of code, I will show you how you can download datasets from Kaggle in general and the datasets I had used for training model. Inorder to train models in audio, first convert the audio to a spectogram and throw an image model. Check this tweet from Dien Hoa Truong who won a NVIDIA RTX 3080 Ti GPU in this competition. This tweet makes me stick to the spectrogram approach for the Kaggle PogChamp music genre classification competition, and finish #1. Thanks @marktenenholtz :) https://t.co/xwtXQRfk51 . &mdash; Dien Hoa Truong (@DienhoaT) April 28, 2022 . For this competition you need two datasets: . The competition data | Image data generated from converting audio to melspectograms in form of images | The data provided here are over 20,000 royalty free song samples (30 second clips) and their musical genres. Your task is to create a machine learning algorithm capable of predicting the genres of unlabeled music files. Create features, design architectures, do whatever it takes to predict them the best. . The code for downloading data from kaggle has been adopted from Jeremy&#39;s notebook . creds = &quot;&quot; from pathlib import Path cred_path = Path(&quot;~/.kaggle/kaggle.json&quot;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(creds) cred_path.chmod(0o600) . path = Path(&quot;../input/kaggle-pog-series-s01e02&quot;) path.ls() . (#6) [Path(&#39;input/kaggle-pog-series-s01e02/genres.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/sample_submission.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/test.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/test&#39;),Path(&#39;input/kaggle-pog-series-s01e02/train.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/train&#39;)] . from zipfile import ZipFile from kaggle import api if not path.exists(): api.competition_download_cli(str(path)) ZipFile(f&quot;{path}.zip&quot;).extractall(path) . Downloading kaggle-pog-series-s01e02.zip to /home . 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.05G/9.05G [07:54&lt;00:00, 20.5MB/s] . . . ! kaggle datasets download -d dienhoa/music-genre-spectrogram-pogchamps . Downloading music-genre-spectrogram-pogchamps.zip to /home 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6.80G/6.80G [07:00&lt;00:00, 14.7MB/s] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.80G/6.80G [07:00&lt;00:00, 17.4MB/s] . Quick EDA and Data Cleaning . df_train = pd.read_csv(&quot;../input/kaggle-pog-series-s01e02/train.csv&quot;) df_train.head() . song_id filename filepath genre_id genre . 0 10150 | 010150.ogg | train/010150.ogg | 7 | Instrumental | . 1 7358 | 007358.ogg | train/007358.ogg | 2 | Punk | . 2 20573 | 020573.ogg | train/020573.ogg | 5 | Folk | . 3 11170 | 011170.ogg | train/011170.ogg | 12 | Old-Time / Historic | . 4 16662 | 016662.ogg | train/016662.ogg | 1 | Rock | . df_train[&quot;filepath&quot;] = df_train[&quot;filepath&quot;].str.replace(&quot;ogg&quot;, &quot;png&quot;) . Shows a highly imbalanced dataset . df_train[&quot;genre&quot;].value_counts() . Rock 3097 Electronic 3073 Punk 2584 Experimental 1801 Hip-Hop 1761 Folk 1215 Chiptune / Glitch 1181 Instrumental 1045 Pop 945 International 814 Ambient Electronic 796 Classical 495 Old-Time / Historic 408 Jazz 306 Country 142 Soul-RnB 94 Spoken 94 Blues 58 Easy Listening 13 Name: genre, dtype: int64 . df_train.head() . song_id filename filepath genre_id genre . 0 10150 | 010150.ogg | train/010150.png | 7 | Instrumental | . 1 7358 | 007358.ogg | train/007358.png | 2 | Punk | . 2 20573 | 020573.ogg | train/020573.png | 5 | Folk | . 3 11170 | 011170.ogg | train/011170.png | 12 | Old-Time / Historic | . 4 16662 | 016662.ogg | train/016662.png | 1 | Rock | . df_train = df_train.set_index(&quot;song_id&quot;) . df_train = df_train.drop( [ 23078, 3137, 4040, 15980, 11088, 9963, 24899, 16312, 22698, 17940, 22295, 3071, 13954, ] ) . df_train.shape . (19909, 4) . Loading Data using fastai DataLoaders . For creating this notebook, I spend a major portion of my time in cleaning and sorting out appropriate datablocks/dataloaders for training image models using fast.ai. This is something which you as a practitioner experience, compared to learning all the theory and backpropogation algorithm. . So let&#39;s see how we load this data using fast.ai. There are two approaches which we will discuss below. Both the approaches of loading data works, but the first approach as a disadvantage, which I will tell in a moment. . Approach 1. Using DataBlock and loading images . Create a data frame temp_train and create new column is_valid | is_valid is default column named created for using ColSplitter | Now set get_x which specifies the path of files for inputting data which is set as base_path+filename path: lambda o:f&#39;{path}/&#39;+o.path | Now set get_y which specifies the variable to predict, ie the genre of music | . temp_train = df_train temp_train.loc[:15000, &quot;is_valid&quot;] = True temp_train.loc[15000:, &quot;is_valid&quot;] = False . path = Path(&quot;../input/music-genre-spectrogram-pogchamps/spectograms/&quot;) . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), splitter=ColSplitter(), get_x=lambda o: f&quot;{path}/&quot; + o.path, get_y=lambda o: o.genre, item_tfms=Resize(224), batch_tfms=aug_transforms(), ) . dls = dblock.dataloaders(temp_train) . dls.show_batch() . # dblock.summary(df_train) . This worked really well, and with this approach I was even able to train a ML model which got 50% accuracy. Saturday evening side-project: Trained a baseline ML model to classify audio files to identify their music genre using @fastdotai based on a kaggle dataset.Acheived only 50% accuracy, probably because problem is hard. Next job is to check what @DienhoaT has done to win a GPU. pic.twitter.com/EahvgtYBDL . &mdash; Kurian Benoy (@kurianbenoy2) April 30, 2022 . Yet when it came to export models, due to usage of lamda method in DataBlock. I got Pickling error as the model was not able to be exported with learn.export() method. . 2. Using DataLoaders methods with loading from dataframe method . This issue got me into using approach that using ImageDataLoaders.from_df in fastai. Let&#39;s first take a look at our df_train dataframe: . df_train.head() . filename filepath genre_id genre . song_id . 10150 010150.ogg | train/010150.png | 7 | Instrumental | . 7358 007358.ogg | train/007358.png | 2 | Punk | . 20573 020573.ogg | train/020573.png | 5 | Folk | . 11170 011170.ogg | train/011170.png | 12 | Old-Time / Historic | . 16662 016662.ogg | train/016662.png | 1 | Rock | . If you look at the dataframe, we know that on appending to the path, the filepath column. . This is the exact value for get_x method in fastai fn_col = 1 which specifies the column name filepath at position 1. | label or get_y is specified by the column name genre at position 3. | valid_pct (ensure what percentage of data to be used for validation) | y_block=CategoryBlock to ensure it&#39;s used for normal classification only and not multi-label | . dls = ImageDataLoaders.from_df( df_train, path, valid_pct=0.2, seed=34, y_block=CategoryBlock, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224), fn_col=1, label_col=3, ) dls.show_batch() . Training fastai model . I trained using a resnet18 model at first, later we stepped up to use resnet50 model. . learn = vision_learner(dls, resnet50, metrics=error_rate) . learn.lr_find() . SuggestedLRs(valley=0.0008317637839354575) . learn.fine_tune(10, 0.0008317637839354575) . epoch train_loss valid_loss error_rate time . 0 | 2.869285 | 2.171426 | 0.616428 | 01:43 | . epoch train_loss valid_loss error_rate time . 0 | 2.312176 | 1.843815 | 0.558654 | 02:07 | . 1 | 2.102361 | 1.719162 | 0.539061 | 02:08 | . 2 | 1.867139 | 1.623988 | 0.527003 | 02:08 | . 3 | 1.710557 | 1.527913 | 0.507661 | 02:07 | . 4 | 1.629478 | 1.456836 | 0.479779 | 02:05 | . 5 | 1.519305 | 1.433036 | 0.474253 | 02:05 | . 6 | 1.457465 | 1.379757 | 0.464456 | 02:05 | . 7 | 1.396283 | 1.369344 | 0.457925 | 02:05 | . 8 | 1.359388 | 1.367973 | 0.453655 | 02:05 | . 9 | 1.364363 | 1.368887 | 0.456167 | 02:04 | . learn.export(&quot;model.pkl&quot;) . Pushing models to hugging face . huggingface_hub has released two new functions to easily push fastai models to Huggingface Hub. . Using push_to_hub_fastai you can easily push the fastai Learner to huggingface. | Additionally, you can load any fastai Learner from the Hub using from_pretrained_fastai | . Omar Espejel had shared a fantastic notebook on these new functionalities in huggingface here. . Note: You should install git-lfs and login to huggingface account with token before pushing . from huggingface_hub import push_to_hub_fastai push_to_hub_fastai( learn, &quot;kurianbenoy/music_genre_classification_baseline&quot;, commit_message=&quot;Resnet50 with 10 epochs of training&quot;, ) . /home/kurianbenoy/music_genre_classification_baseline is already a clone of https://huggingface.co/kurianbenoy/music_genre_classification_baseline. Make sure you pull the latest changes with `repo.git_pull()`. To https://huggingface.co/kurianbenoy/music_genre_classification_baseline 390320d..3605083 main -&gt; main . &#39;https://huggingface.co/kurianbenoy/music_genre_classification_baseline/commit/360508311005aefeb3ca29933f2173202afe4f30&#39; . If you want to load this model in fastai and use it directly for inference just from_pretrain_fastai as shown in the below screenshot: . . Taking a look at results . learn.show_results() . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(15, 10)) . Inference function . from fastai.vision.all import * from huggingface_hub import from_pretrained_fastai learn = from_pretrained_fastai(&quot;kurianbenoy/music_genre_classification_baseline&quot;) def predict(img): img = PILImage.create(img) _pred, _pred_w_idx, probs = learn.predict(img) labels_probs = {labels[i]: float(probs[i]) for i, _ in enumerate(labels)} return labels_probs . Conclusion . We trained a ML model which can identify with 54.4% accuracy to classify in a music file which genre it is. It&#39;s not so bad for a baseline model. Dien Hoa Truong has shared some techniques which he learned during Kaggle competition with music genres. [Machine Learning] Speed up your experiment so that you can try out a lot of ideas and find what works best for your problem. Below â¬‡ï¸ is what I have learned during the Kaggle competition with music genreðŸŽ¶: . &mdash; Dien Hoa Truong (@DienhoaT) April 30, 2022 . Thanks for reading :pray: .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastaicourse/2022/05/01/AudioCNNDemo.html",
            "relUrl": "/fastai/fastaicourse/2022/05/01/AudioCNNDemo.html",
            "date": " â€¢ May 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Practical Deep Learning for Coders Course - Lesson 1",
            "content": "First there was a set of introductions by university officials at UQ like VC. One curious thing was everyone of UQ staff were honouring something traditionaly of that land to live in reconciliation. . Then lecture of Jeremy starts, seeing his face the chatbox is in delight. . Jeremy mentions there are two categories of students who attend the course: . Students who have enrolled via University of Queensland(with almost 350 people attending in-person and about 100 people remotely as well). | fastai fellows who have acknowledged for their contribution to community. | Jeremy recommends having study buddies when we are learning the course is important. So he asks to create Study groups wherever possible. This course is now happening after a gap of 2 years, so there is a lot of new things which has to be covered as Deep learning moves so fast. . Using Dalle-2 technique we can generative creative images from generate twitter bios. For a creative person, this can be very helpful to create good artwork. Then one of another popular techniques was using Pathways language model which is able to answers question with explanations and even explains why some jokes are funny. . Jeremy talks about his interest in education.He is a homeschooler and learned from books by Paul Lockhart &amp; David Perkins which were inspiration for fast.ai. fastai teaches stuff in top-down manner. You will go into as much technical stuff as you, yet you will learn and implement cool stuff steadily. . About fast.ai course . He wrote an awesome book and this course. His book is one of the best sellers in Deep Learning and used to teach folks in companies like Tesla, OpenAI etc. Almost 6 million people watched his videos so far.Jeremy has won multiple competitions in Kaggle, was the CEO of kaggle. He build Enlitic, a medical company which was build for medical purpose with two other succesful startups. . Jeremy mentioned for this course, we are not using any material directly from Deep Learning For Coders with Fastai &amp; Pytorch book. Yet he recommends to read portions of book after each chapter. . Usually multiple people learn better if the same idea is exposed in different way from multiple sources. That&#39;s the why behind this approach. . Jeremy started coding hands-own a bird or park classifier, which was considered as a very difficult problem in 2015. Even a comic depicted this. Yet things have changed so drastically in past few years, that it&#39;s very easy to do that now. . Yet let&#39;s look, why we couldn&#39;t build a bird classifer in 2015:- For classifying histopothical images. They used computer vision techniques.- They got big team of datascientist, mathematicans with lot of features who build relevant feature for machine learning hand by hand. . These project took years | Also deep learning was not in radar for researchers then. | . What has now changed? . Using neural network they build these features on their own. | Mathew D Zeiler &amp; Rob Fergus(and actual weights) showed with visualization how neural networks work | Combine all features to learn and go slowly in past, neural networks learned on it&#39;s own these techniques. | . If it&#39;s a bird or not? notebook can be found here. I am slightly tweaking this model to leverage pytorch image-models released by timm. . urls = search_images(&#39;bird photos&#39;, max_images=1) urls[0] . from fastdownload import download_url dest = &#39;bird.jpg&#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) . Note: . Image based algorithms, are not for images. Image for music classification by Deolho, Ethan sutin sounds from image recognizer. You can do music classification, with some creativity using cnns. . | Also needing lots of data is a myth created by companies who sell data processng units. There are lot of free resources like Kaggle, Colab etc. . | . Observation by Jeremy:Tensorflow is slowly dying. Check this article which he cited. Yet pytorch has lot of hairy code, which can be solved using good abstractions in fastai. . fastai library tries to provide good and the best fine-tuned models, which work well compared to other libraries. He showed code required for implementing AdamW in pytorch and in fastai. | . Tanishq Abraham pointed me to implemtation of AdamW to chapter 16 in fastbook. . download_url(search_images(&#39;forest photos&#39;, max_images=1)[0], &#39;forest.jpg&#39;, show_progress=False) Image.open(&#39;forest.jpg&#39;).to_thumb(256,256) searches = &#39;forest&#39;,&#39;bird&#39; path = Path(&#39;bird_or_not&#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) resize_images(path/o, max_size=400, dest=path/o) . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . As the code showed, data cleaning is a big part of machine learninng. When we are learning this course as practitioners, we will spend lot of time of building and loading models. Like in compiler course lot of time is not spend on techniques, but on getting the environment up and ready. . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(224, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch() . After examining, 100s of project and datascience requirments. fastai came up with this approach of DataBlock, which consists of five things: . blocks | get_items | splitter | Batch_tfms(optional) | get_y | item_tfms | Without validation data, it won&#39;t allow to train. parent_label, return parent folder. we saved as forests or birds. We need same size. Idea to do quickly, why not publish vision_learners with pets dataset. . Now it&#39;s time to train our model . learn = vision_learner(dls, &#39;vit_tiny_patch16_224&#39;, metrics=error_rate) learn.fine_tune(10) . One thing which is cool is that the whole presentation is also made with Jupyter Notebooks using RiseJS. Also jupyter notebooks can be used for writing books like Deep Learning for Coders, for blogging using fastpages, for CI/CD pipeline to run in parallel execution in fastai repo. . Tanishq Mathew Abraham has summarized on what can be done in this twitter threads. Awesome and surprising things you can do with Jupyter Notebooks â¬‡ . &mdash; Tanishq Mathew Abraham (@iScienceLuvr) April 27, 2022 After this Jeremy, showed all the examples in Chapter 1 in Deep Learning for coders. My notes then: . We are still scratching the surface. Lot of marketing out there, some of first open source models available. The deep learning when it broke X, y, z in domain. In NLP it breaks lot of stuff . What&#39;s really go in on : in arthur samuel with graph. The graphs are build with gv2 in jupyter notebook. Deploying models in ML is a bit tricky. But it&#39;s just predict and shows results. . Conclusion by Jeremy . So after first lesson: . a) If you know python, then it&#39;s kind of easy for you. b) If don&#39;t know python, it&#39;s very difficult . Regardless of what level you are. Experiment yourself and do something more complex. Go ahead and push yourself a little bit, but not much. Then present your work. Do stuff on things where you are interested. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastaicourse/fastbook/2022/04/26/fastai-51.html",
            "relUrl": "/fastaicourse/fastbook/2022/04/26/fastai-51.html",
            "date": " â€¢ Apr 26, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Practical Deep Learning for Coders Course - Lesson 0",
            "content": "Last week I enrolled for the live cohort of Deep Learning For Coders with fastai coruse which is going to be taken by Jeremy Howard. It&#39;s&#39; a previlege at the same time, a dream come true moment for me, as a fastai student who took some part of fast.ai from the year 2018. . The fastai course has seen a lot of success stories over the years. Amazing folks like Aman Arora, Even Oldridge, Sanyam Bhutani,Radek Osmulski, Jason Antic, Wayde Gilliam, Zach Mueller ... While there are lot of hidden gems whom may not be so well know in community as well like Bhuvana who shared her journey in Pycon India 2019. The way this course has democratized AI &amp; ML is simply amazing. . Today Radek Osmulski joined NVIDIA AI. Today is my first day at @NVIDIAAI! ðŸ¥³-From learning to code at 29-through learning ML @fastdotai-winning a @kaggle competition-jobs at ðŸ”¥ startups-moving continents thx to AI-to joining the illustrious Merlin team â¤ï¸I am beyond grateful ðŸ™Will make this one count! . &mdash; Radek Osmulski ðŸ‡ºðŸ‡¦ (@radekosmulski) April 26, 2022 . It was Radek who had posted in fastai forums few years back on how he will approach taking the fastai course. Inspired by that I am creating this action list items on things which I am planning to achieve during duration of this course. . I will do following: . Try out every notebook given by Jeremy during class and writes in Kaggle | Write blogpost every week with lesson summary and occasionally on new things I learning during course. | Build some hugging face spaces using fastai. | Challenge myself to complete the existing project I am doing on IPL Commentary Summarizer and learn NLP extensively. | Attend the delft-fast-ai-study-group sessions atleast for five weeks. | Go to sleep before midnight | Dream and breath pytorch | Extra Credits: . Participate in NLP competition provided by jeremy link. | Frequent in fast.ai forums and discord group | Take a look at notebooks in Transformer Book to learn NLP extensively during class. | Try to read Pytorch book | Complete all the college assignments in Pytorch | I won&#39;t do: . Be active in twitter during course | Write or read EDA notebooks | Won&#39;t write any notebooks in tensorflow :) | Won&#39;t participate in any other kaggle comps during course period. | I will be not attending Real Python office hours, unless I have a very compelling doubt. | With that I am winding for today. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastaicourse/fastbook/2022/04/26/fastai-50-ipynb.html",
            "relUrl": "/fastaicourse/fastbook/2022/04/26/fastai-50-ipynb.html",
            "date": " â€¢ Apr 26, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Building a fine-tuned translation system for English-Malayalam",
            "content": "Hey, everyone. We all are familiar with translation systems like using google translate. So today, let&#39;s build a fine tuned translation system for converting text from english to malayalam. It&#39;s built using Blurr library - built on top of Hugging face and fast.ai made by Wayde Gilliam. Also our translation system is going to be fine tuned on top of KDE specific dataset. You can find the trained model here. . . Installation . ! python3 -m pip install -Uqq datasets== fastai==2.6.3 ! python3 -m pip install -Uqq transformers[sentencepiece] ! python3 -m pip install -Uqq ohmeow-blurr==1.0.5 ! python3 -m pip install -Uqq nltk ! python3 -m pip install -Uqq sacrebleu ! python3 -m pip install -Uqq git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[&quot;fastai&quot;] . . ERROR: Could not find a version that satisfies the requirement datasets== (from versions: 0.0.9, 1.0.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.2, 1.13.3, 1.14.0, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.17.0, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 2.0.0, 2.1.0, 2.2.0) ERROR: No matching distribution found for datasets== . . Loading Data . A translation system is an example of sequence to sequence models, which is usually used for tasks which involves generating new data. Translation usually needs datasets in both the source language and target language (the language to which it needs to be translated). . We are using KDE4 datasets, and choose both source language and translation language as english and malayalam respectively. Usually these datasets are curated by community volunteers to their native language, and this was probably done by KDE community volunteers in Kerala. When someone is localizing these texts into there in local languague, usually computer science specific terms are still written in english. . import pandas from datasets import load_dataset . raw_datasets = load_dataset(&quot;kde4&quot;, lang1=&quot;en&quot;, lang2=&quot;ml&quot;, split=&quot;train[:1000]&quot;) . Using custom data configuration en-ml-lang1=en,lang2=ml Reusing dataset kde4 (/home/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac) . Most of translation dataset is in form of id and translation json output - with both en and ml as objects. . raw_datasets[0] . {&#39;id&#39;: &#39;0&#39;, &#39;translation&#39;: {&#39;en&#39;: &#39;Add Feed to Akregator&#39;, &#39;ml&#39;: &#39;à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµ u200d à´«àµ€à´¡àµ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµ u200dà´•àµà´•àµà´•&#39;}} . . Transforming data into DataLoaders . Importing libraries and get hugging-face objects . from blurr.text.data.all import * from blurr.text.modeling.all import * from blurr.text.utils import * from fastai.data.all import * from fastai.callback.all import * from fastai.learner import load_learner, Learner from fastai.optimizer import * from transformers import * . pretrained_model_name = &quot;Helsinki-NLP/opus-mt-en-ml&quot; model_cls = AutoModelForSeq2SeqLM hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=model_cls) hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model) . translation_df = pd.DataFrame(raw_datasets[&quot;translation&quot;], columns=[&quot;en&quot;, &quot;ml&quot;]) translation_df.head() . en ml . 0 Add Feed to Akregator | à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµâ€ à´«àµ€à´¡àµ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµâ€à´•àµà´•àµà´• | . 1 Add Feeds to Akregator | à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµâ€ à´«àµ€à´¡àµà´•à´³àµâ€ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµâ€à´•àµà´•àµà´• | . 2 Add All Found Feeds to Akregator | à´Žà´²àµà´²à´¾ à´«àµ€à´¡àµà´•à´³àµà´‚ à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµâ€ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµâ€à´•àµà´•àµà´• | . 3 Subscribe to site updates (using news feed) | à´¸àµˆà´±àµà´±àµà´•à´³à´¿à´²àµ† à´ªàµà´¤àµà´®à´•à´³à´±à´¿à´¯à´¾à´¨àµâ€ à´µà´°à´¿à´•àµà´•à´¾à´°à´¨à´¾à´•àµà´• (à´µà´¾à´°àµâ€à´¤àµà´¤à´¾ à´«àµ€à´¡àµà´•à´³àµâ€ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ) | . 4 Imported Feeds | à´Žà´Ÿàµà´¤àµà´¤ à´«àµ€à´¡àµà´•à´³àµâ€ | . blocks = (Seq2SeqTextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), noop) dblock = DataBlock(blocks=blocks, get_x=ColReader(&quot;en&quot;), get_y=ColReader(&quot;ml&quot;), splitter=RandomSplitter()) . dls = dblock.dataloaders(translation_df, bs=16) dls.show_batch(dataloaders=dls, max_n=2, input_trunc_at=100, target_trunc_at=250) . text target . 0 Aâ–versionâ–controlâ–historyâ–entryâ–consists ofâ–severalâ–lines.â–Specify theâ–regularâ–expressionâ–toâ–detect | à´’à´°àµ à´­à´¾à´·à´¾à´¨àµà´¤à´° à´¨à´¿à´¯à´¨àµà´¤àµà´°à´£à´¤àµà´¤à´¿à´¨àµà´±àµ† à´¨à´¾à´³àµà´µà´´à´¿ à´šàµ‡à´°àµà´•àµà´•àµà´¨àµà´¨à´¤à´¿à´²àµ à´ªà´² à´µà´°à´¿à´•à´³àµà´£àµà´Ÿà´¾à´•àµà´‚. à´†à´¦àµà´¯à´¤àµà´¤àµ† à´µà´°à´¿ à´•à´£àµà´Ÿàµà´ªà´¿à´Ÿà´¿à´•àµà´•à´¾à´¨àµà´³àµà´³ à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚ à´¨à´¿à´°àµà´¦àµà´¦àµ‡à´¶à´¿à´•àµà´•àµà´• (à´®àµà´¨àµà´¨à´¿à´²àµ† à´µà´¿à´¶à´¦àµ€à´•à´°à´£à´‚ à´•àµ‚à´Ÿà´¾à´¤àµ†). à´‡à´¨à´‚ à´¤à´¿à´°à´¿à´•àµà´•à´¾à´¨àµà´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨ à´•àµ€à´•à´³àµ† à´’à´¨àµà´¨à´¿à´šàµà´šà´¾à´•àµà´•à´¾à´¨àµ à´¬àµà´°à´¾à´•àµà´•à´±àµà´±àµà´•à´³àµ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´•. à´’à´´à´¿à´šàµà´šàµ à´µà´¿à´Ÿàµà´Ÿ | . 1 â–Mailodyâ–canâ–storeâ–allâ–attchements ofâ–allâ–messages inâ–aâ–certainâ–folder.â–Thenâ–youâ–neverâ–haveâ–toâ–saveâ– | à´Žà´²àµà´²à´¾ à´¸à´¨àµà´¦àµ‡à´¶à´™àµà´™à´³àµà´Ÿàµ‡à´¯àµà´‚ à´Žà´²àµà´²à´¾ à´…à´¨àµà´¬à´¨àµà´§à´™àµà´™à´³àµà´‚ à´’à´°àµ à´ªàµà´°à´¤àµà´¯àµ‡à´• à´…à´±à´¯à´¿à´²àµ à´¸àµ‚à´•àµà´·à´¿à´•àµà´•à´¾à´¨àµ à´®àµ†à´¯à´¿à´²à´¡à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚. à´¨à´¿à´™àµà´™à´³àµà´•àµà´•à´µà´¯àµ† à´¸à´¨àµà´¦àµ‡à´¶à´™àµà´™à´³à´¿à´²àµà´¨à´¿à´¨àµà´¨àµ à´ªàµà´°à´¤àµà´¯àµ‡à´•à´‚ à´¸àµ‚à´•àµà´·à´¿à´•àµà´•àµ‡à´£àµà´Ÿà´¤à´¿à´²àµà´². à´…à´µ à´…à´±à´¯à´¿à´²àµ à´‰à´£àµà´Ÿà´¾à´¯à´¿à´°à´¿à´•àµà´•àµà´‚. à´ªàµà´°à´¤àµà´¯àµ‡à´•à´‚ à´¶àµà´°à´¦àµà´§à´¿à´•àµà´•àµà´•, à´ˆ à´…à´± à´‡à´Ÿà´•àµà´•à´¿à´Ÿà´•àµà´•àµ à´•à´¾à´²à´¿à´¯à´¾à´•àµà´•à´¿à´•àµà´•àµŠà´£àµà´Ÿà´¿à´°à´¿à´•àµà´• | . . Training fine-tuned translation system . Using blurr High-level API . Bugs in ohmeow v1.0.4 has been fixed by the open-source maintainer. . from blurr.text.utils import BlurrText NLP = BlurrText() . learn = BlearnerForTranslation.from_data( translation_df, pretrained_model_name, src_lang_name=&quot;English&quot;, src_lang_attr=&quot;en&quot;, trg_lang_name=&quot;Malayalam&quot;, trg_lang_attr=&quot;ml&quot;, dl_kwargs={&quot;bs&quot;: 16}, ) . metrics_cb = BlearnerForTranslation.get_metrics_cb() learn.fit_one_cycle(1, lr_max=4e-5, cbs=[metrics_cb]) . [nltk_data] Downloading package wordnet to /home/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /home/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package omw-1.4 to /home/nltk_data... [nltk_data] Package omw-1.4 is already up-to-date! . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 5.512897 | 4.821253 | 0.023251 | 0.158193 | 4.086147 | 00:19 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=250) . text target prediction . 0 â–Mailodyâ–isâ–ableâ–toâ–convertâ–yourâ–plainâ–messageâ–toâ–aâ–htmlâ–messageâ–andâ–includeâ–that in theâ–outgoingâ–message.â–Thisâ–means theâ–receiverâ–willâ–alsoâ–haveâ–clickableâ–linksâ–andâ–coloredâ–quoteâ–levels. | à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¸à´¾à´¦à´¾ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´Žà´šàµà´šàµà´Ÿà´¿à´Žà´‚à´Žà´²àµ à´¸à´¨àµà´¦àµ‡à´¶à´®à´¾à´•àµà´•à´¿ à´®à´¾à´±àµà´±à´¿ à´…à´¤àµ à´ªàµà´±à´¤àµà´¤àµ‡à´•àµà´•àµ à´…à´¯à´•àµà´•àµà´¨àµà´¨ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿à´²àµ à´‰à´³àµà´ªàµà´ªàµ†à´Ÿàµà´¤àµà´¤à´¾à´¨àµ à´®àµ†à´¯à´¿à´²à´¡à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚. à´…à´¤à´¾à´¯à´¤àµ à´žàµŠà´Ÿàµà´Ÿà´¾à´µàµà´¨àµà´¨ à´•à´£àµà´£à´¿à´•à´³àµà´‚ à´µà´°àµà´£àµà´£ à´‰à´¦àµà´§à´°à´£à´¿ à´¤à´²à´µàµà´‚ à´¸àµà´µàµ€à´•à´°àµà´¤àµà´¤à´¾à´µà´¿à´¨àµà´•àµ‚à´Ÿà´¿ à´²à´­àµà´¯à´®à´¾à´µàµà´‚ | [à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¸à´®àµà´ªà´¾à´¦à´¨ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´’à´°àµ html à´¸à´¨àµà´¦àµ‡à´¶à´®à´¾à´•àµà´•à´¿ à´®à´¾à´±àµà´±àµà´µà´¾à´¨àµà´‚ à´ªàµà´±à´¤àµà´¤àµà´³àµà´³ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿àµ½ à´‰àµ¾ à´•àµà´•àµŠà´³àµà´³àµà´¨àµà´¨ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿àµ½ à´‰àµ¾à´ªàµà´ªàµ†à´Ÿàµà´¤àµà´¤àµà´µà´¾à´¨àµà´‚ Middià´¯àµà´•àµà´•àµàµ à´•à´´à´¿à´¯àµà´‚. à´‡à´¤à´¿à´¨àµ¼à´¤àµà´¥à´‚ à´±à´¿à´•àµà´•àµ‹àµ¼à´¡àµ à´šàµ†à´¯àµà´¯à´¾à´µàµà´¨àµà´¨ à´•à´£àµà´£à´¿à´•à´³àµà´•àµà´•àµà´‚ à´¨à´¿à´±à´™àµà´™à´³àµà´•àµà´•àµà´‚ à´µà´²à´•àµà´•àµ†à´Ÿàµà´Ÿàµà´•à´³àµà´‚ à´¨à´¿à´±à´™àµà´™à´³àµà´•àµà´•àµàµ., à´ªà´¤à´¿à´ªàµà´ªàµ à´¨à´¿à´¯à´¨àµà´¤àµà´°à´¿à´¤ à´šà´°à´¿à´¤àµà´°à´¤àµà´¤à´¿à´¨àµà´±àµ† à´ªàµà´°à´¾à´°à´‚à´­à´®à´¾à´¯à´¤à´¿à´¨àµà´±àµ† à´¸à´¾à´§à´¾à´°à´£ à´ªàµà´°à´¯àµ‹à´—à´‚. à´¸à´¾à´§à´¾à´°à´£à´¯à´¾à´¯à´¿ à´ˆ à´µà´°à´¿à´¯à´¿àµ½ &quot;$2Loux&quot; à´•àµ€à´µà´¾à´¤à´•à´‚ à´‰à´£àµà´Ÿàµ. à´¸àµà´µà´¤à´µàµ‡à´¯àµà´³àµà´³ à´®àµ‚à´²àµà´²àµà´¯à´‚: description from play played for play play for play for play play play for filme cout fillulume for for courtyourtime fume time ck., à´¤àµà´Ÿà´™àµà´™àµà´¨àµà´¨à´¤à´¿à´¨à´¾à´¯à´¿, &quot;à´ªàµà´¤à´¿à´¯&quot; à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´¤àµà´¤àµ à´†à´¦àµà´¯à´‚ à´’à´°àµ à´ªàµà´¤à´¿à´¯ à´’à´ªàµà´ªàµ à´‰à´£àµà´Ÿà´¾à´•àµà´•àµà´•. à´…à´ªàµà´ªàµ‹àµ¾ à´¨à´¿à´™àµà´™àµ¾à´•àµà´•àµàµ à´¤à´¿à´°àµà´¤àµà´¤à´¾à´¨àµà´‚ à´’à´ªàµà´ªàµà´•à´³àµà´Ÿàµ† à´¶àµ‡à´–à´°à´‚ à´¸à´‚à´°à´•àµà´·à´¿à´•àµà´•à´¾à´¨àµà´‚ à´¸à´¾à´§à´¿à´•àµà´•àµà´‚., à´¤à´¿à´°àµà´¤àµà´¤àµ à´šàµ†à´¯àµà´¤ à´«à´¯àµ½ à´¸àµ‚à´•àµà´·à´¿à´¯àµà´•àµà´•àµà´®àµà´ªàµ‹àµ¾ à´µà´°à´¿à´¯àµà´Ÿàµ† à´…à´µà´¸à´¾à´¨à´™àµà´™àµ¾ à´¸à´œàµà´œàµ€à´•à´°à´¿à´¯àµà´•àµà´•àµà´¨àµà´¨àµ. à´¡àµ‹à´Žà´¸àµ/ à´œà´¾à´²à´•à´™àµà´™àµ¾: CRS+LLF; à´¯àµà´Žà´«àµ: LRIFX; à´’à´ªàµà´ªà´‚ CRL++D=0, LRD=0A, LRFAA +0A, à´®àµàµ»à´•à´¾à´´àµà´šà´•àµ¾ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµ½ à´ªà´°à´¾à´œà´¯à´‚. à´ˆ à´•à´®à´¾àµ»à´¡àµ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´¯àµà´•àµà´•àµà´•:% 1 à´Žà´¨àµà´¨ à´†à´œàµà´ž à´‡à´ªàµà´ªàµ‹àµ¾ à´ªàµà´°à´µàµ¼à´¤àµà´¤à´¨à´°à´¹à´¿à´¤à´®à´¾à´¯à´¿à´°à´¿à´•àµà´•àµà´‚., &quot;Subject&quot; à´…à´²àµà´²àµ†à´™àµà´•à´¿àµ½ &#39;suck&#39; à´’à´°àµ à´†à´´à´¤àµà´¤à´¿à´²àµà´³àµà´³ à´…à´µà´¸àµà´¥à´¯à´¾à´£àµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´ªàµ‚àµ¼à´£àµà´£à´®à´¾à´¯àµà´‚ à´…à´§à´¿à´•à´¾à´°à´¤àµà´¤à´¿àµ½ à´•àµŠà´£àµà´Ÿàµà´µà´°àµà´¨àµà´¨àµ, à´¸à´¸àµà´ªàµ†àµ»à´¡àµ à´’à´°àµ à´¨à´¿à´¦àµà´°à´¾ à´¸à´‚à´¸àµà´¥à´¾à´¨à´®à´¾à´£àµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´Šàµ¼à´œàµà´œà´‚ à´•àµà´±à´¯àµà´•àµà´•àµà´®àµà´ªàµ‹à´³àµ à´®à´¾à´¤àµà´°à´‚ à´Šàµ¼à´œàµà´œà´‚ à´¸à´‚à´­à´°à´¿à´•àµà´•àµà´• à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´³àµà´³àµ‚., à´ˆ à´®àµàµ»à´•à´°àµà´¤àµ½ à´²àµˆà´¨à´¿à´‚à´—àµ à´²àµˆà´¨à´¿à´‚à´—àµ à´²àµˆà´¨à´¿à´‚à´—àµ à´µàµ‡à´³à´¯à´¿àµ½ à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´®àµà´³àµà´³àµ. ( à´µà´¿à´¶à´¦à´¾à´‚à´¶à´™àµà´™àµ¾à´•àµà´•àµ à´¡àµ‹à´•àµà´¸àµ à´•à´¾à´£àµà´•.), à´«à´¯à´²à´¿à´¨àµà´±àµ† à´ªà´•àµ¼à´ªàµà´ªàµ à´ªàµà´°à´•àµà´°à´¿à´¯à´¯à´¿à´²àµ à´ªà´¿à´¶à´•àµ: à´µà´¾à´¯à´¨à´¯àµà´•àµà´•àµà´³àµà´³ à´«à´¯àµ½ à´¤àµà´±à´•àµà´•àµà´¨àµà´¨à´¤à´¿àµ½ à´ªà´°à´¾à´œà´¯à´‚:% 1, â–ª à´®à´¾à´®àµ‹à´¦àµ€à´¸ à´¤àµà´±à´¨àµà´¨àµ à´¨àµ‹à´•àµà´•àµà´•à´¯àµà´‚ à´…à´Ÿà´¯àµ à´•àµà´•àµà´•à´¯àµà´‚ à´šàµ†à´¯àµà´¯àµà´¨àµà´¨à´¤àµ à´•àµà´°à´®à´®à´¾à´¯ à´ªàµà´°à´¯àµ‹à´—à´¤àµà´¤à´¿àµ½ à´šàµ‡à´°àµà´•à´¯à´¿à´²àµà´²., à´Žà´²àµà´²à´¾ à´‰à´ªà´¯àµ‹à´•àµà´¤à´¾à´•àµà´•àµ¾à´•àµà´•àµà´‚ à´†à´ªàµà´ªà´¿àµ¾à´Ÿàµà´Ÿàµà´•àµ¾ à´‡àµ»à´¸àµà´±àµà´±àµ‹àµ¾ à´šàµ†à´¯àµà´¯àµà´•à´¯àµ‹ à´¨àµ€à´•àµà´•à´‚ à´šàµ†à´¯àµà´¯àµà´•à´¯àµ‹ à´šàµ†à´¯àµà´¯àµà´•., à´«à´¯à´²à´¿à´¨àµà´±àµ† à´ªà´•àµ¼à´ªàµà´ªàµ à´ªàµà´°à´•àµà´°à´¿à´¯à´¯à´¿à´²àµ à´ªà´¿à´¶à´•àµ: à´µà´¾à´¯à´¨ à´ªà´°à´¾à´œà´¯à´ªàµà´ªàµ†à´Ÿàµà´Ÿàµ:% 1, à´µàµˆà´°àµà´¦àµà´§àµà´¯à´™àµà´™à´³àµà´Ÿàµ† à´Žà´£àµà´£à´‚ à´¸à´‚à´¬à´¨àµà´§à´¿à´šàµà´šàµ à´’à´°àµ à´¸à´‚à´µà´¾à´¦à´‚ à´•à´¾à´£à´¿à´•àµà´•àµà´•., à´¨à´¿à´™àµà´™àµ¾ à´¬à´¾à´±àµà´±à´±à´¿ à´…à´§à´¿à´•à´¾à´°à´¤àµà´¤à´¿àµ½ à´¨à´¿à´¨àµà´¨àµà´‚ à´“à´Ÿà´¿ à´°à´•àµà´·à´ªàµ†à´Ÿà´¾àµ» à´ªàµ‹à´µàµà´•à´¯à´¾à´£àµ, à´‡à´ªàµà´ªàµ‹àµ¾ à´’à´¨àµà´¨àµà´‚ à´šàµ†à´¯àµà´¯à´¾à´¨à´¿à´²àµà´²., à´µà´¿à´²à´¾à´¸à´™àµà´™à´³àµ à´šàµ‡àµ¼à´¤àµà´¤à´¿à´Ÿàµà´Ÿà´¿à´²àµà´². à´…à´¯à´¯àµà´•àµà´•àµà´¨àµà´¨à´¤à´¿à´¨àµ à´®àµà´®àµà´ªàµ à´•àµà´±à´žàµà´žà´¤àµ à´’à´¨àµà´¨àµ†à´™àµà´•à´¿à´²àµà´‚ à´šàµ‡àµ¼à´•àµà´•àµ‚., à´•àµà´·à´®à´¿à´•àµà´•à´£à´‚, à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´«àµ‹à´£àµº à´ªà´¤à´¿à´ªàµà´ªàµ à´ªà´¿à´¨àµà´¤àµà´£à´¯àµà´•àµà´•àµà´¨àµà´¨à´¿à´²àµà´².] | . Using mid-level of blurr APIs . b = dls.one_batch() . len(b), b[0][&quot;input_ids&quot;].shape, b[1].shape . (2, torch.Size([16, 72]), torch.Size([16, 114])) . dls.show_batch(dataloaders=dls, input_trunc_at=250, target_trunc_at=250) . text target . 0 Aâ–versionâ–controlâ–historyâ–entryâ–consists ofâ–severalâ–lines.â–Specify theâ–regularâ–expressionâ–toâ–detect theâ–firstâ–line (without theâ–leadingâ–comment).â–Useâ–parenthesesâ–toâ–group theâ–keysâ–youâ–wantâ–toâ–useâ–forâ–sorting.â–Ifâ–leftâ–empty,â–thenâ–KDiff3â–assumesâ–thatâ–e | à´’à´°àµ à´­à´¾à´·à´¾à´¨àµà´¤à´° à´¨à´¿à´¯à´¨àµà´¤àµà´°à´£à´¤àµà´¤à´¿à´¨àµà´±àµ† à´¨à´¾à´³àµà´µà´´à´¿ à´šàµ‡à´°àµà´•àµà´•àµà´¨àµà´¨à´¤à´¿à´²àµ à´ªà´² à´µà´°à´¿à´•à´³àµà´£àµà´Ÿà´¾à´•àµà´‚. à´†à´¦àµà´¯à´¤àµà´¤àµ† à´µà´°à´¿ à´•à´£àµà´Ÿàµà´ªà´¿à´Ÿà´¿à´•àµà´•à´¾à´¨àµà´³àµà´³ à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚ à´¨à´¿à´°àµà´¦àµà´¦àµ‡à´¶à´¿à´•àµà´•àµà´• (à´®àµà´¨àµà´¨à´¿à´²àµ† à´µà´¿à´¶à´¦àµ€à´•à´°à´£à´‚ à´•àµ‚à´Ÿà´¾à´¤àµ†). à´‡à´¨à´‚ à´¤à´¿à´°à´¿à´•àµà´•à´¾à´¨àµà´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨ à´•àµ€à´•à´³àµ† à´’à´¨àµà´¨à´¿à´šàµà´šà´¾à´•àµà´•à´¾à´¨àµ à´¬àµà´°à´¾à´•àµà´•à´±àµà´±àµà´•à´³àµ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´•. à´’à´´à´¿à´šàµà´šàµ à´µà´¿à´Ÿàµà´Ÿ | . 1 â–Thereâ–isâ–noâ–Inboxâ–found inâ–anyâ–resource.â–Startingâ–aâ–newâ–messageâ–willâ–cause theâ–messageâ–toâ–beâ–lostâ–afterâ–youâ–haveâ–sentâ–it.â–Youâ–willâ–notâ–haveâ–aâ–localâ–copyâ–anymore.â–Ifâ–youâ–wantâ–aâ–copy,â–oneâ–wayâ–toâ–doâ–thisâ–isâ–toâ–addâ–yourselfâ–asâ–a CCâ–to theâ–message.&lt;pad&gt;&lt; | à´µà´¿à´­à´µà´™àµà´™à´³à´¿à´²àµŠà´¨àµà´¨àµà´‚ à´’à´°àµ à´‡à´¨àµà´¬àµ‹à´•àµà´¸àµ à´•à´¾à´£àµà´¨àµà´¨à´¿à´²àµà´². à´’à´°àµ à´ªàµà´¤à´¿à´¯ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´¤àµà´Ÿà´™àµà´™àµà´¨àµà´¨à´¤àµ à´…à´¤àµ à´…à´¯à´šàµà´š à´¶àµ‡à´·à´‚ à´¨à´·àµà´Ÿà´ªàµà´ªàµ†à´Ÿà´¾à´¨àµ à´•à´¾à´°à´£à´®à´¾à´•àµà´‚. à´ªàµà´°à´¾à´¦àµ‡à´¶à´¿à´• à´ªà´•à´°àµà´ªàµà´ªàµà´•à´³àµŠà´¨àµà´¨àµà´‚ à´’à´°à´¿à´•àµà´•à´²àµà´‚ à´²à´­àµà´¯à´®à´²àµà´²à´¾à´¤à´¾à´µàµà´‚. à´‰à´¦à´¾à´¹à´°à´£à´®à´¾à´¯à´¿ à´’à´°àµ à´ªà´•à´°àµà´ªàµà´ªàµ à´†à´µà´¶àµà´¯à´®àµà´£àµà´Ÿàµ†à´™àµà´•à´¿à´²àµ à´’à´°àµ à´•à´¾à´°àµà´¬à´£àµ à´ªà´¤à´¿à´ªàµà´ªàµà´•àµ‚à´Ÿà´¿ à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµà´¤àµà´¤à´¤à´¾ | . 2 â–Toâ–preventâ–dataâ–lossâ–orâ–otherâ–damage,â–youâ–canâ–have theâ–system suspendâ–orâ–hibernate,â–soâ–youâ–doâ–notâ–accidentallyâ–runâ–out ofâ–batteryâ–power.â–Configure theâ–number ofâ–minutesâ–belowâ–which theâ–machineâ–willâ–run theâ–configuredâ–action.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt; | à´µà´¿à´µà´°à´¨à´·àµà´Ÿà´®àµ‹ à´¹à´¾à´¨à´¿à´¯àµ‹ à´¤à´Ÿà´¯à´¾à´¨àµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´®à´¯à´™àµà´™àµà´•à´¯àµ‹ à´¶à´¿à´¶à´¿à´°à´¨à´¿à´¦àµà´°à´¯à´¿à´²à´¾à´•àµà´•à´¯àµ‹à´šàµ†à´¯àµà´¯à´¾à´µàµà´¨àµà´¨à´¤à´¾à´£àµ, à´…à´™àµà´™à´¨àµ† à´†à´•à´¸àµà´®à´¿à´•à´®à´¾à´¯à´¿à´Ÿàµà´Ÿàµà´ªàµ‹à´²àµà´‚ à´¬à´¾à´±àµà´±à´±à´¿ à´Šà´°àµà´œàµà´œà´‚ à´¤àµ€à´°à´¾à´¤à´¿à´°à´¿à´¯àµà´•àµà´•àµà´‚. à´•àµà´°à´®àµ€à´•à´°à´¿à´šàµà´š à´¨à´Ÿà´ªà´Ÿà´¿à´¯àµà´®à´¾à´¯à´¿ à´®àµà´¨àµà´¨àµ‹à´Ÿàµà´Ÿàµàµ à´ªàµ‹à´•àµ‡à´£àµà´Ÿà´¤àµàµ à´Žà´¤àµà´° à´®à´¿à´¨à´¿à´±àµà´±àµà´•à´³à´¿à´²àµ à´¤à´¾à´´àµ†à´¯à´¾à´•àµà´®àµà´ªàµ‹à´´à´¾à´£àµ†à´¨àµà´¨àµàµ à´¤à´¾à´´àµ† à´•àµà´°à´®àµ€à´•à´°à´¿à´¯àµà´•àµà´•àµà´• | . 3 â–AutoSyncâ–isâ–aâ–featureâ–from MP3tunesâ–whichâ–allowsâ–youâ–toâ–automaticallyâ–moveâ–yourâ–musicâ–betweenâ–computersâ–andâ–devices.â–Youâ–canâ–uploadâ–musicâ–fromâ–oneâ–locationâ–andâ–haveâ–itâ–downloadâ–instantlyâ–toâ–otherâ–locations.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa | Enable harmony | . 4 â–Regularâ–expressionâ–forâ–linesâ–whereâ–KDiff3â–shouldâ–automaticallyâ–chooseâ–oneâ–source.â–Whenâ–aâ–lineâ–withâ–aâ–conflictâ–matches theâ–regularâ–expressionâ–then -â–ifâ–available - C,â–otherwise Bâ–willâ–beâ–chosen.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | à´•àµ†à´¡à´¿à´«àµ3 à´¸àµà´µà´¤à´¨àµà´¤àµà´°à´®à´¾à´¯à´¿ à´’à´°àµ à´¸àµà´°àµ‹à´¤à´¸àµà´¸àµ à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´•àµà´•àµà´¨àµà´¨à´¿à´Ÿà´¤àµà´¤àµ à´µà´°à´¿à´•à´³àµà´•àµà´•àµà´³àµà´³ à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚. à´¸à´‚à´˜à´Ÿàµà´Ÿà´¨à´®àµà´³àµà´³ à´µà´°à´¿ à´šàµ‡à´°àµà´¨àµà´¨àµà´µà´°àµà´®àµà´ªàµ‹à´³àµ à´…à´¤à´¿à´¨àµà´±àµ† à´¨à´¿à´¤àµà´¯à´­à´¾à´µà´‚ - à´¸à´¿ à´‰à´£àµà´Ÿàµ†à´™àµà´•à´¿à´²àµ à´…à´¤àµ, à´…à´²àµà´²àµ†à´™àµà´•à´¿à´²àµ à´¬à´¿ à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´•àµà´•à´ªàµà´ªàµ†à´Ÿàµà´‚. | . 5 â–Loadingâ–externalâ–imagesâ–givesâ–spammers theâ–acknowledgementâ–thatâ–youâ–receivedâ–thisâ–messageâ–soâ–theyâ–willâ–useâ–yourâ–emailâ–addressâ–toâ–spamâ–you.â–Soâ–youâ–shouldâ–onlyâ–continueâ–forâ–veryâ–trustedâ–messages.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | à´ªàµà´±à´®àµ†à´¨à´¿à´¨àµà´¨àµŠà´°àµ à´šà´¿à´¤àµà´°à´‚ à´•à´¯à´±àµà´±àµà´¨àµà´¨à´¤àµ à´¨à´¿à´™àµà´™à´³àµà´•àµà´•àµ à´ˆ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´²à´­à´¿à´šàµà´šàµ†à´¨àµà´¨ à´®à´Ÿà´•àµà´•à´°à´¶àµ€à´¤à´¿ à´šà´µà´±à´¯à´¯àµà´•àµà´•àµà´¨àµà´¨à´µà´°àµà´•àµà´•àµ à´²à´­à´¿à´šàµà´šàµ‡à´•àµà´•à´¾à´‚. à´…à´µà´°àµ à´¨à´¿à´™àµà´™à´³àµà´Ÿ à´‡à´¤à´ªà´¾à´²àµ à´µà´¿à´²à´¾à´¸à´‚ à´¨à´¿à´™àµà´™à´³àµà´•àµà´•àµàµ à´¨àµ‡à´°àµ‡à´¯àµà´‚ à´šà´µà´±à´¯à´¯àµà´•àµà´•à´¾à´¨àµà´ªà´¯àµ‹à´—à´¿à´šàµà´šàµ‡à´•àµà´•à´¾à´‚. à´…à´¤àµà´•àµŠà´£àµà´Ÿàµ à´µà´³à´°àµ† à´µà´¿à´¶àµà´µà´¸àµà´¤ à´¸à´¨àµà´¦àµ‡à´¶à´™àµà´™à´³àµ à´®à´¾à´¤àµà´°à´‚ à´¤àµà´Ÿà´°àµà´¨àµà´¨à´¾à´²àµ à´®à´¤à´¿. | . 6 â–Mailodyâ–isâ–ableâ–toâ–convertâ–yourâ–plainâ–messageâ–toâ–aâ–htmlâ–messageâ–andâ–includeâ–that in theâ–outgoingâ–message.â–Thisâ–means theâ–receiverâ–willâ–alsoâ–haveâ–clickableâ–linksâ–andâ–coloredâ–quoteâ–levels.&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa | à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¸à´¾à´¦à´¾ à´¸à´¨àµà´¦àµ‡à´¶à´‚ à´Žà´šàµà´šàµà´Ÿà´¿à´Žà´‚à´Žà´²àµ à´¸à´¨àµà´¦àµ‡à´¶à´®à´¾à´•àµà´•à´¿ à´®à´¾à´±àµà´±à´¿ à´…à´¤àµ à´ªàµà´±à´¤àµà´¤àµ‡à´•àµà´•àµ à´…à´¯à´•àµà´•àµà´¨àµà´¨ à´¸à´¨àµà´¦àµ‡à´¶à´¤àµà´¤à´¿à´²àµ à´‰à´³àµà´ªàµà´ªàµ†à´Ÿàµà´¤àµà´¤à´¾à´¨àµ à´®àµ†à´¯à´¿à´²à´¡à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚. à´…à´¤à´¾à´¯à´¤àµ à´žàµŠà´Ÿàµà´Ÿà´¾à´µàµà´¨àµà´¨ à´•à´£àµà´£à´¿à´•à´³àµà´‚ à´µà´°àµà´£àµà´£ à´‰à´¦àµà´§à´°à´£à´¿ à´¤à´²à´µàµà´‚ à´¸àµà´µàµ€à´•à´°àµà´¤àµà´¤à´¾à´µà´¿à´¨àµà´•àµ‚à´Ÿà´¿ à´²à´­àµà´¯à´®à´¾à´µàµà´‚ | . 7 â–Youâ–haveâ–clickedâ–onâ–aâ–linkâ–whichâ–mightâ–notâ–indicateâ–correctlyâ–whereâ–youâ–areâ–reallyâ–goingâ–to.â–Pleaseâ–checkâ–ifâ–youâ–reallyâ–wantâ–toâ–viewâ–aâ–pageâ–onâ–thisâ–server:â–%1â–Doâ–youâ–wantâ–toâ–goâ–there?&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | à´¨à´¿à´™àµà´™à´³àµ à´’à´°àµ à´•à´£àµà´£à´¿à´¯à´¿à´²àµ à´žàµŠà´Ÿàµà´Ÿà´¿à´¯à´¤àµ à´¨à´¿à´™àµà´™à´³àµ†à´µà´¿à´Ÿàµ‡à´•àµà´•à´¾à´£àµ à´µà´¾à´¸àµà´¤à´µà´¤àµà´¤à´¿à´²àµ à´ªàµ‹à´•àµà´¨àµà´¨à´¤àµ à´Žà´¨àµà´¨àµ à´•àµƒà´¤àµà´¯à´®à´¾à´¯à´¿ à´¸àµ‚à´šà´¿à´ªàµà´ªà´¿à´•àµà´•àµà´¨àµà´¨à´¿à´²àµà´². à´‡ à´¸àµ‡à´µà´•à´¨àµà´±àµ† à´¤à´¾à´³à´¿à´²àµ‡à´•àµà´•àµ à´¤à´¨àµà´¨àµ† à´¯à´¾à´£àµ‹ à´ªàµ‹à´•àµ‡à´£àµà´Ÿà´¤àµ†à´¨àµà´¨àµ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµà´•:% 1 à´¨à´¿à´™àµà´™à´³àµà´•àµà´•à´¿à´µà´¿à´Ÿàµ† à´ªàµ‹à´•à´£àµ‹? | . 8 â–Tryâ–toâ–align Bâ–and Câ–whenâ–comparingâ–orâ–mergingâ–three inputâ–files.â–Notâ–recommendedâ–forâ–mergingâ–becauseâ–mergeâ–mightâ–getâ–moreâ–complicated. (Defaultâ–isâ–off.)&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt; | à´…à´•à´¤àµà´¤àµà´µà´¿à´Ÿà´¾à´¨àµà´³àµà´³ 3à´«à´¯à´²àµà´•à´³àµ à´¤à´¾à´°à´¤à´®àµà´¯à´®àµ à´šàµ†à´¯àµà´¯àµà´®àµà´ªàµ‹à´´àµ‹ à´²à´¯à´¨à´‚ à´¨à´Ÿà´¤àµà´¤àµà´®àµà´ªàµ‹à´´àµ‹à´¬à´¿à´¯àµà´‚ à´¸à´¿à´¯àµà´‚ à´¨à´¿à´°à´’à´ªàµà´ªà´¿à´•àµà´•à´¾à´¨àµ à´¶àµà´°à´®à´¿à´•àµà´•à´£à´‚. à´•àµ‚à´Ÿàµà´¤à´²àµ à´•àµà´´à´ªàµà´ªà´‚ à´ªà´¿à´Ÿà´¿à´šàµà´šà´¤à´¾à´¯à´¤àµà´•àµŠà´£àµà´Ÿàµ à´²à´¯à´¨à´¤àµà´¤à´¿à´¨àµà´±àµ† à´•à´¾à´°àµà´¯à´¤àµà´¤à´¿à´²àµ à´…à´¤àµ à´¶àµà´ªà´¾à´°àµà´¶ à´šàµ†à´¯àµà´¤à´¿à´Ÿàµà´Ÿà´¿à´²àµà´². (à´“à´«àµ à´†à´£àµ à´¸à´¹à´œà´‚.) | . seq2seq_metrics = {&quot;bleu&quot;: {&quot;returns&quot;: &quot;bleu&quot;}, &quot;meteor&quot;: {&quot;returns&quot;: &quot;meteor&quot;}, &quot;sacrebleu&quot;: {&quot;returns&quot;: &quot;score&quot;}} model = BaseModelWrapper(hf_model) learn_cbs = [BaseModelCallback] fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)] learn = Learner( dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), # CrossEntropyLossFlat() cbs=learn_cbs, splitter=partial(blurr_seq2seq_splitter, arch=hf_arch), ) learn.freeze() . [nltk_data] Downloading package wordnet to /home/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /home/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package omw-1.4 to /home/nltk_data... [nltk_data] Package omw-1.4 is already up-to-date! . learn.lr_find(suggest_funcs=[minimum, steep, valley, slide]) . SuggestedLRs(minimum=0.00010000000474974513, steep=2.75422871709452e-06, valley=0.00013182566908653826, slide=0.2089296132326126) . learn.fit_one_cycle(15, lr_max=5e-4, cbs=fit_cbs) . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 5.345715 | 4.811335 | 0.038182 | 0.183787 | 5.607312 | 00:15 | . 1 | 4.658613 | 4.272337 | 0.058925 | 0.206963 | 3.932045 | 00:32 | . 2 | 4.196794 | 4.031921 | 0.069354 | 0.167185 | 4.876618 | 00:25 | . 3 | 3.826715 | 4.102871 | 0.071109 | 0.122637 | 4.639123 | 00:18 | . 4 | 3.551352 | 4.046614 | 0.080512 | 0.202187 | 6.996449 | 00:26 | . 5 | 3.329875 | 3.928597 | 0.054780 | 0.179460 | 3.403195 | 00:53 | . 6 | 3.197556 | 3.818610 | 0.109263 | 0.212960 | 9.055532 | 00:21 | . 7 | 3.034615 | 3.821332 | 0.100355 | 0.200819 | 8.208149 | 00:20 | . 8 | 2.900438 | 3.807550 | 0.101014 | 0.222323 | 7.221607 | 00:35 | . 9 | 2.820454 | 3.813579 | 0.111548 | 0.219794 | 9.026264 | 00:29 | . 10 | 2.756891 | 3.791952 | 0.107236 | 0.223636 | 9.965611 | 00:31 | . 11 | 2.740331 | 3.809624 | 0.115999 | 0.239258 | 10.261043 | 00:24 | . 12 | 2.685602 | 3.804891 | 0.126693 | 0.234446 | 11.131677 | 00:22 | . 13 | 2.653799 | 3.800799 | 0.119988 | 0.227742 | 10.985718 | 00:23 | . 14 | 2.636473 | 3.801427 | 0.124357 | 0.229134 | 10.967688 | 00:25 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=500) . text target prediction . 0 â–Withâ–aâ–dynamicâ–playlist, Amarokâ–becomesâ–yourâ–ownâ–personalâ–dj,â–automaticallyâ–selectingâ–tracksâ–forâ–you,â–basedâ–onâ–aâ–number ofâ–parametersâ–thatâ–youâ–select. | Turn dynamic mode on | [Username for logins to disabled, format: Artist - Track (Album), from the currently playlist column name and token for playlist layouts, à´µà´°à´¿à´•à´³à´¿à´²àµ† à´…à´•àµà´·à´°à´™àµà´™à´³àµà´Ÿàµ† à´…à´µà´¸àµà´¥ à´µà´¿à´—à´£à´¿à´•àµà´•àµà´•. (à´®àµà´¨àµà´¨à´¿à´²àµ† à´µà´¿à´µà´°à´™àµà´™à´³àµà´®à´¾à´¯à´¿ à´¤à´¾à´°à´¤à´®àµà´¯à´‚ à´šàµ†à´¯àµà´¯à´¾à´¨àµà´‚ à´¸à´¿à´®àµà´²àµ‡à´±àµà´±à´±à´¿à´•àµà´•àµ à´•à´´à´¿à´¯àµà´‚.), à´’à´°àµ à´Ÿà´¾à´¬àµ à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµ‚. à´’à´°àµ à´šàµ†à´±à´¿à´¯ à´Ÿà´¾à´¬àµ à´¬à´¾à´±àµà´±à´±à´¿à´¯à´¿à´²àµà´²à´¾à´¤àµ† à´¸àµà´µà´¯à´‚ à´Ÿà´¾à´¬àµ à´¬à´¾à´±àµà´±à´±à´¿ à´¯à´¾à´¨àµà´¤àµà´°à´¿à´•à´®à´¾à´¯à´¿ à´®à´±à´¯àµà´•àµà´•àµà´¨àµà´¨àµ. à´’à´°àµ à´Ÿà´¾à´¬àµ à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµ‚., à´‡à´¤àµà´¤à´°à´‚ à´ªàµà´°à´•à´Ÿà´¨à´‚ à´¤àµà´Ÿà´™àµà´™àµà´®àµà´ªàµ‹à´³àµê ±, à´®àµ†à´¯à´¿à´²à´¡à´¿à´•à´³à´¿àµ½ à´šà´¿à´°à´¿à´šàµà´šà´¤à´¾à´¯à´¿ à´¸àµà´µà´¯à´®àµ‡à´µ à´²à´¯à´¿à´ªàµà´ªà´¿à´•àµà´•àµà´•. à´‰à´¦à´¾à´¹à´°à´£à´®à´¾à´¯à´¿ à´’à´°àµ à´…à´•àµà´·à´°à´°àµ‚à´ªà´‚ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµà´¨àµà´¨àµà´£àµà´Ÿàµ‹à´¯àµ†à´¨àµà´¨àµ à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•à´¾à´¤àµ† à´¸àµà´µà´¯à´‚ à´šà´¿à´°à´¿à´šàµà´šàµà´•àµŠà´£àµà´Ÿà´¾à´µàµà´‚., à´®àµà´¨àµê ±à´œàµà´œàµ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµà´•àµŠà´£àµà´Ÿàµ à´ªà´°à´¾à´œà´¯à´ªàµà´ªàµ†à´Ÿàµà´Ÿàµ. à´ˆ à´†à´œàµà´ž à´ªà´°à´¿à´¶àµ‹à´§à´¿à´•àµà´•àµ‚:% 1 à´®àµà´¨àµê ±à´¨à´Ÿà´ªà´Ÿà´¿ à´†à´œàµà´ž à´‡à´ªàµà´ªàµ‹àµ¾ à´ªàµà´°à´µà´°àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´•à´¤àµà´¤à´¨à´°à´¹à´¿à´¤à´®à´¾à´•àµà´•àµà´‚., à´ªàµà´±à´®àµ†à´¨à´¿à´¨àµà´¨àµ- à´®à´¿à´šàµà´šà´®àµà´³àµà´³ à´®à´Ÿàµà´Ÿàµà´‚ à´ªàµà´°à´¾à´µàµ¼à´¤àµà´¤à´¿à´•à´®à´¾à´•àµà´•àµà´¨àµà´¨àµ. à´µà´²à´¿à´¯ à´«à´¯à´²àµà´•à´³àµ†à´•àµà´•àµà´±à´¿à´šàµà´šàµà´³àµà´³ à´ªà´°à´¿à´¶àµ‹à´§à´•à´¨àµà´±àµ† à´ªà´°à´¿à´¶àµ‹à´§à´•à´¨àµà´­à´µà´‚ à´µà´³à´°àµ† à´ªà´¤àµà´•àµà´•àµ†à´¯à´¾à´µàµà´‚., à´ˆ à´¸àµà´²àµˆà´¡à´¨àµà´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ, à´¸à´¿à´¸àµà´±àµà´±à´‚ à´¸àµ‹à´•àµà´•à´±àµà´±àµ à´ªàµà´±à´¤àµà´¤àµ‡à´¯àµà´•àµà´•àµàµ à´ªà´¤à´¿à´•àµà´•àµà´¨àµà´¨àµ, à´ªàµà´±à´¤àµà´¤à´¾à´•àµà´•àµà´µà´¾à´¨àµà´³àµà´³ à´®àµ†à´¯à´¿à´²à´¡à´¿à´­à´¾à´—à´‚ à´²à´­àµà´¯à´®à´²àµà´², à´¦à´¯à´µà´¾à´¯à´¿ à´…à´¤àµ à´…à´¯à´•àµà´•àµà´¨àµà´¨à´¤à´¿à´¨àµ à´®àµà´®àµà´ªà´¾à´¯à´¿ à´•àµà´°à´®àµ€à´•à´°à´¿à´¯àµà´•àµà´•àµà´•, à´ˆ à´®àµà´¨àµê ± à´ªàµ‚à´°àµê ±à´µàµà´µ à´ªàµà´°à´•àµà´°à´¿à´¯à´¯àµà´•àµà´•àµà´³àµà´³ à´¸à´®à´¯à´¤àµà´¤àµ à´µà´°à´¿à´¯à´¿à´²àµê ± à´®à´¾à´¤àµà´°à´®àµ‡ à´‰à´ªà´¯àµ‹à´—à´¿à´•àµà´•àµ‚. (à´µà´¿à´¶à´¦àµ€à´•à´°à´£à´™àµà´™à´³àµà´•àµà´•àµ à´ªàµà´°à´®à´¾à´£à´ªà´¤àµà´°à´‚ à´¨àµ‹à´•àµà´•àµà´•.), à´«à´¯à´²àµê ± à´ªà´•à´°àµê ±à´ªàµà´ªàµ†à´Ÿàµà´•àµà´•àµà´®àµà´ªàµ‹à´³àµê ± à´ªà´¿à´¶à´•àµ: à´«à´¯à´²à´¿à´²àµâ™¬ à´µà´¾à´¯à´¨à´¯àµà´•àµà´•àµà´³àµà´³ à´«à´¯à´²àµê ± à´¤àµà´±à´•àµà´•àµà´¨àµà´¨à´¤àµ à´ªà´°à´¾à´œà´¯à´ªàµà´ªàµ†à´Ÿàµà´Ÿàµ. à´«à´¯à´²àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´• à´¨à´¾à´®à´‚:% 1, à´¨à´¿à´™àµà´™à´³àµà´ªà´¯àµ‹à´—à´¿à´•àµà´•àµà´¨àµà´¨ à´¬àµ€à´—à´¿à´³àµê ± à´¬à´¾à´•àµà´•àµ†à´¨àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´•à´¡àµà´•à´³àµê ± à´¤àµ†à´°à´žàµà´žàµ†à´Ÿàµà´•àµà´•àµà´•., à´‰à´ªà´¯àµ‹à´•àµà´¤à´¾à´µàµà´¨à´¾à´®à´‚ à´¤à´¿à´°à´¿à´šàµà´šà´±à´¿à´žàµà´žà´¿à´Ÿàµà´Ÿà´¿à´²àµà´², à´…à´²àµà´²àµ†à´™àµà´•à´¿àµ½ à´ªà´¾à´±àµà´±àµ‡à´£àµà´•à´³à´¿à´²àµ† à´…à´Ÿà´¯à´¾à´³à´®à´¿à´Ÿàµà´•., à´…à´¨àµà´¬à´¨àµà´§à´™àµà´™à´³àµà´Ÿàµ† à´…à´³à´µàµ% 1 is new name for translate is the filter, representing the playlist column name and token for playlist layouts, à´¬à´¾à´±àµà´±à´±à´¿à´•à´³à´¿à´²àµ†à´¤àµà´¤à´¿à´šàµà´šàµ‡à´°àµà´®àµà´ªàµ‹à´³àµê ± à´¶àµ‹à´­ à´¨à´¿à´¯à´¨àµà´¤àµà´°à´¿à´•àµà´•àµà´¨àµà´¨àµ, (&quot;à´’à´´à´¿à´žàµà´ž à´‡à´Ÿà´™àµà´™à´³àµê ± à´•à´¾à´£à´¿à´¯àµà´•àµà´•àµà´•&quot; à´ªàµà´°à´µà´°àµê ±à´¤àµà´¤à´®à´°à´¹à´¿à´¤à´®à´¾à´•àµà´•à´¿à´¯à´¾à´²àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´• à´’à´´à´¿à´žàµà´ž à´‡à´Ÿà´™àµà´™à´³àµà´Ÿàµ† à´µàµà´¯ à´…à´²àµà´²à´¾à´¹àµà´•àµà´•à´³àµê ± à´’à´´à´¿à´µà´¾à´•àµà´•àµà´‚.), à´®à´¾à´¤à´¾à´ªà´¿à´¤à´¾à´•àµà´•àµ¾ à´¤àµà´±à´•àµà´•àµà´¨àµà´¨à´¤àµà´‚ à´…à´Ÿà´¯àµà´•àµà´•àµà´¨àµà´¨à´¤àµà´‚ à´²à´¯à´¿à´ªàµà´ªà´¿à´•àµà´•àµà´¨àµà´¨à´¤àµà´‚ à´²à´¯à´¿à´ªàµà´ªà´¿à´•àµà´•àµà´•.] | . . Now let&#39;s translate with our trained models . blurr top 3 translation predictions . test_text = &quot;How are you doing&quot; outputs = learn.blurr_generate(test_text, key=&quot;translation_texts&quot;, num_return_sequences=3) outputs . [{&#39;translation_texts&#39;: [&#39;à´Žà´¨àµà´¤àµŠà´•àµà´•àµ†à´¯àµà´£àµà´Ÿàµ?&#39;, &#39;à´Žà´™àµà´™à´¨àµ†à´¯àµà´£àµà´Ÿàµ?&#39;, &#39;à´Žà´¨àµà´¤àµŠà´•àµà´•àµ†à´¯àµà´£àµà´Ÿàµ.&#39;]}] . Saving trained ML model . export_fname = &quot;saved_model&quot; learn.metrics = None learn.export(fname=f&quot;{export_fname}.pkl&quot;) . from huggingface_hub import push_to_hub_fastai push_to_hub_fastai( learn, &quot;kurianbenoy/kde_en_ml_translation_model&quot;, commit_message=&quot;New version with 15 epoch of training&quot;, ) . /home/kurianbenoy/kde_en_ml_translation_model is already a clone of https://huggingface.co/kurianbenoy/kde_en_ml_translation_model. Make sure you pull the latest changes with `repo.git_pull()`. W0511 16:57:02.992604 140162781013824 repository.py:685] /home/kurianbenoy/kde_en_ml_translation_model is already a clone of https://huggingface.co/kurianbenoy/kde_en_ml_translation_model. Make sure you pull the latest changes with `repo.git_pull()`. remote: Enforcing permissions... remote: Allowed refs: all To https://huggingface.co/kurianbenoy/kde_en_ml_translation_model 62f36f9..766c8a0 main -&gt; main W0511 16:58:04.917035 140162781013824 repository.py:1144] remote: Enforcing permissions... remote: Allowed refs: all To https://huggingface.co/kurianbenoy/kde_en_ml_translation_model 62f36f9..766c8a0 main -&gt; main . &#39;https://huggingface.co/kurianbenoy/kde_en_ml_translation_model/commit/766c8a06c07bb6352d0537ac1972d3c70360fd53&#39; . Prediction with our trained model . Correct Prediction . test_text = &quot;How are you doing&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text) . [{&#39;translation_texts&#39;: &#39;à´Žà´¨àµà´¤àµŠà´•àµà´•àµ†à´¯àµà´£àµà´Ÿàµ?&#39;}] . test_text1 = &quot;Add All Found Feeds to Akregator.&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text1) . [{&#39;translation_texts&#39;: &#39;à´Žà´²àµà´²à´¾ à´«àµ€à´¡àµà´•à´³àµà´‚ à´…à´•àµà´°à´¿à´—àµ‡à´±àµà´±à´±à´¿à´²àµê ± à´•àµ‚à´Ÿàµà´Ÿà´¿à´šàµà´šàµ‡à´°àµ à´ªà´¶àµà´šà´¾à´¤àµà´¤à´ªà´¿à´•àµà´•àµà´•à´•àµà´•àµà´•.&#39;}] . Wrong Prediction . test_text2 = &quot;Subscribe to site updates (using news feed).&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text2) . [{&#39;translation_texts&#39;: &#39;à´¸àµˆà´±àµà´±àµà´•à´³à´¿à´²àµ† à´ªàµà´¤àµà´®à´•à´³à´±à´¿à´¯à´¾à´¨àµê ± à´µà´°à´¿à´•àµà´•à´¾à´°à´¨à´¾à´•àµà´• (à´µà´¾à´°àµê ±à´¤àµà´¤à´¾ à´«àµ€à´¡àµà´•à´³àµâ™¬ à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ).&#39;}] . Expected: &#39;à´¸àµˆà´±àµà´±àµà´•à´³à´¿à´²àµ† à´ªàµà´¤àµà´®à´•à´³à´±à´¿à´¯à´¾à´¨àµ u200d à´µà´°à´¿à´•àµà´•à´¾à´°à´¨à´¾à´•àµà´• (à´µà´¾à´°àµ u200dà´¤àµà´¤à´¾ à´«àµ€à´¡àµà´•à´³àµ u200d à´‰à´ªà´¯àµ‹à´—à´¿à´šàµà´šàµàµ . Thanks to . Wayde Gilliam - for creating blurr, and helping with doubts in translation bits | Kevin Bird - for helping in editing the article. | Ashwin Jayaprakash - for trying out notebook and reporting issues which was later fixed by Wayde in blurr. | . fin. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/13/hfenml-translate.html",
            "relUrl": "/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/13/hfenml-translate.html",
            "date": " â€¢ Mar 13, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
            "content": "from datasets import load_dataset raw_datasets = load_dataset(&quot;squad&quot;) . Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453... Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data. . raw_datasets . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 87599 }) validation: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 10570 }) }) . Recently a tweet went viral . We can see that NLP is hard, and even the best publicly avaiable models performs poorly in this tasks | . . from transformers import pipeline classifier = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased&quot;) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.weight&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;classifier.weight&#39;, &#39;pre_classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . [{&#39;label&#39;: &#39;LABEL_0&#39;, &#39;score&#39;: 0.5706434845924377}] . checkpoint = &quot;cardiffnlp/twitter-roberta-base-sentiment&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . [{&#39;label&#39;: &#39;LABEL_2&#39;, &#39;score&#39;: 0.9608174562454224}] . checkpoint = &quot;finiteautomata/bertweet-base-sentiment-analysis&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji . [{&#39;label&#39;: &#39;NEG&#39;, &#39;score&#39;: 0.9447618126869202}] . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;Where do I work?&quot;, context=&quot;My name is Sylvain and I work at Hugging Face in Brooklyn&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Hugging Face&#39;, &#39;end&#39;: 45, &#39;score&#39;: 0.6949771046638489, &#39;start&#39;: 33} . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;What was feedback?&quot;, context=&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Brilliant service&#39;, &#39;end&#39;: 101, &#39;score&#39;: 0.13661321997642517, &#39;start&#39;: 84} . For specific question answering - use question answering models . Encoder-only models like BERT tend to be great at extracting answers to factoid questions like â€œWho invented the Transformer architecture?â€ but fare poorly when given open-ended questions like â€œWhy is the sky blue?â€ In these more challenging cases . If youâ€™re interested in this type of generative question answering, we recommend checking out our demo based on the ELI5 dataset. Using Lonformer model. . [1] https://yjernite.github.io/lfqa.html . print(&quot;Context: &quot;, raw_datasets[&quot;train&quot;][0][&quot;context&quot;]) print(&quot;Question: &quot;, raw_datasets[&quot;train&quot;][0][&quot;question&quot;]) print(&quot;Answer: &quot;, raw_datasets[&quot;train&quot;][0][&quot;answers&quot;]) . Context: Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34;Venite Ad Me Omnes&#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? Answer: {&#39;text&#39;: [&#39;Saint Bernadette Soubirous&#39;], &#39;answer_start&#39;: [515]} . raw_datasets[&quot;train&quot;].filter(lambda x: len(x[&quot;answers&quot;][&quot;text&quot;]) != 1) . Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 0 }) . print(raw_datasets[&quot;validation&quot;][0][&quot;answers&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;answers&quot;]) . {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]} {&#39;text&#39;: [&#39;Santa Clara, California&#39;, &#34;Levi&#39;s Stadium&#34;, &#34;Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California.&#34;], &#39;answer_start&#39;: [403, 355, 355]} . print(raw_datasets[&quot;validation&quot;][2][&quot;context&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;question&quot;]) . Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the &#34;golden anniversary&#34; with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as &#34;Super Bowl L&#34;), so that the logo could prominently feature the Arabic numerals 50. Where did Super Bowl 50 take place? . from transformers import AutoTokenizer model_checkpoint = &quot;bert-base-cased&quot; tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . context = raw_datasets[&quot;train&quot;][0][&quot;context&quot;] question = raw_datasets[&quot;train&quot;][0][&quot;question&quot;] inputs = tokenizer(question, context) tokenizer.decode(inputs[&quot;input_ids&quot;]) . &#39;[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]&#39; . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, ) for ids in inputs[&quot;input_ids&quot;]: print(tokenizer.decode(ids)) . [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;offset_mapping&#39;, &#39;overflow_to_sample_mapping&#39;]) . inputs[&quot;overflow_to_sample_mapping&quot;] . [0, 0, 0, 0] . inputs = tokenizer( raw_datasets[&quot;train&quot;][2:6][&quot;question&quot;], raw_datasets[&quot;train&quot;][2:6][&quot;context&quot;], max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) print(f&quot;The 4 examples gave {len(inputs[&#39;input_ids&#39;])} features.&quot;) print(f&quot;Here is where each comes from: {inputs[&#39;overflow_to_sample_mapping&#39;]}.&quot;) . The 4 examples gave 19 features. Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]. . Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If thatâ€™s not the case, we loop to find the first and last token of the answer: . answers = raw_datasets[&quot;train&quot;][2:6][&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(inputs[&quot;offset_mapping&quot;]): sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) start_positions, end_positions . ([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0], [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0]) . idx = 0 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] start = start_positions[idx] end = end_positions[idx] labeled_answer = tokenizer.decode(inputs[&quot;input_ids&quot;][idx][start : end + 1]) print(f&quot;Theoretical answer: {answer}, labels give: {labeled_answer}&quot;) . Theoretical answer: the Main Building, labels give: the Main Building . idx = 4 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] decoded_example = tokenizer.decode(inputs[&quot;input_ids&quot;][idx]) print(f&quot;Theoretical answer: {answer}, decoded example: {decoded_example}&quot;) . Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP] . max_length = 384 stride = 128 def preprocess_training_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) offset_mapping = inputs.pop(&quot;offset_mapping&quot;) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) answers = examples[&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(offset_mapping): sample_idx = sample_map[i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) inputs[&quot;start_positions&quot;] = start_positions inputs[&quot;end_positions&quot;] = end_positions return inputs . train_dataset = raw_datasets[&quot;train&quot;].map( preprocess_training_examples, batched=True, remove_columns=raw_datasets[&quot;train&quot;].column_names, ) len(raw_datasets[&quot;train&quot;]), len(train_dataset) . (87599, 88729) . &#39;Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot; Venite Ad Me Omnes &quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]&#39; . Indeed, we donâ€™t see the answer inside the context. . âœï¸ Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied. . Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. Weâ€™ll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here: . def preprocess_validation_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) example_ids = [] for i in range(len(inputs[&quot;input_ids&quot;])): sample_idx = sample_map[i] example_ids.append(examples[&quot;id&quot;][sample_idx]) sequence_ids = inputs.sequence_ids(i) offset = inputs[&quot;offset_mapping&quot;][i] inputs[&quot;offset_mapping&quot;][i] = [ o if sequence_ids[k] == 1 else None for k, o in enumerate(offset) ] inputs[&quot;example_id&quot;] = example_ids return inputs . validation_dataset = raw_datasets[&quot;validation&quot;].map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) len(raw_datasets[&quot;validation&quot;]), len(validation_dataset) . (10570, 10822) . small_eval_set = raw_datasets[&quot;validation&quot;].select(range(100)) trained_checkpoint = &quot;distilbert-base-cased-distilled-squad&quot; tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint) eval_set = small_eval_set.map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) . tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . import torch from transformers import AutoModelForQuestionAnswering eval_set_for_model = eval_set.remove_columns([&quot;example_id&quot;, &quot;offset_mapping&quot;]) eval_set_for_model.set_format(&quot;torch&quot;) device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names} trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to( device ) with torch.no_grad(): outputs = trained_model(**batch) . start_logits = outputs.start_logits.cpu().numpy() end_logits = outputs.end_logits.cpu().numpy() . import collections example_to_features = collections.defaultdict(list) for idx, feature in enumerate(eval_set): example_to_features[feature[&quot;example_id&quot;]].append(idx) . import numpy as np n_best = 20 max_answer_length = 30 predicted_answers = [] for example in small_eval_set: example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = eval_set[&quot;offset_mapping&quot;][feature_index] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length. if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answers.append( { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } ) best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]}) . from datasets import load_metric metric = load_metric(&quot;squad&quot;) . metric . Metric(name: &#34;squad&#34;, features: {&#39;predictions&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;prediction_text&#39;: Value(dtype=&#39;string&#39;, id=None)}, &#39;references&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answers&#39;: Sequence(feature={&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answer_start&#39;: Value(dtype=&#39;int32&#39;, id=None)}, length=-1, id=None)}}, usage: &#34;&#34;&#34; Computes SQuAD scores (F1 and EM). Args: predictions: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair as given in the references (see below) - &#39;prediction_text&#39;: the text of the answer references: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair (see above), - &#39;answers&#39;: a Dict in the SQuAD dataset format { &#39;text&#39;: list of possible texts for the answer, as a list of strings &#39;answer_start&#39;: list of start positions for the answer, as a list of ints } Note that answer_start values are not taken into account to compute the metric. Returns: &#39;exact_match&#39;: Exact match (the normalized answer exactly match the gold answer) &#39;f1&#39;: The F-score of predicted tokens versus the gold answer Examples: &gt;&gt;&gt; predictions = [{&#39;prediction_text&#39;: &#39;1976&#39;, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; references = [{&#39;answers&#39;: {&#39;answer_start&#39;: [97], &#39;text&#39;: [&#39;1976&#39;]}, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; squad_metric = datasets.load_metric(&#34;squad&#34;) &gt;&gt;&gt; results = squad_metric.compute(predictions=predictions, references=references) &gt;&gt;&gt; print(results) {&#39;exact_match&#39;: 100.0, &#39;f1&#39;: 100.0} &#34;&#34;&#34;, stored examples: 0) . theoretical_answers = [ {&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in small_eval_set ] print(predicted_answers[0]) print(theoretical_answers[0]) . {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;prediction_text&#39;: &#39;Denver Broncos&#39;} {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;answers&#39;: {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]}} . metric.compute(predictions=predicted_answers, references=theoretical_answers) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . from tqdm.auto import tqdm def compute_metrics(start_logits, end_logits, features, examples): example_to_features = collections.defaultdict(list) for idx, feature in enumerate(features): example_to_features[feature[&quot;example_id&quot;]].append(idx) predicted_answers = [] for example in tqdm(examples): example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] # Loop through all features associated with that example for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = features[feature_index][&quot;offset_mapping&quot;] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answer = { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } answers.append(answer) # Select the answer with the best score if len(answers) &gt; 0: best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append( {&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]} ) else: predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: &quot;&quot;}) theoretical_answers = [{&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in examples] return metric.compute(predictions=predicted_answers, references=theoretical_answers) . compute_metrics(start_logits, end_logits, eval_set, small_eval_set) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) . Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#39;qa_outputs.bias&#39;, &#39;qa_outputs.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . from huggingface_hub import notebook_login notebook_login() . Login successful Your token has been saved to /root/.huggingface/token Authenticated through git-credential store but this isn&#39;t the helper defined on your machine. You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default git config --global credential.helper store . ! sudo apt install git-lfs ! git-lfs install . Reading package lists... Done Building dependency tree Reading state information... Done git-lfs is already the newest version (2.3.4-1). The following package was automatically installed and is no longer required: libnvidia-common-470 Use &#39;sudo apt autoremove&#39; to remove it. 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Error: Failed to call git rev-parse --git-dir --show-toplevel: &#34;fatal: not a git repository (or any of the parent directories): .git n&#34; Git LFS initialized. . from transformers import TrainingArguments args = TrainingArguments( &quot;bert-finetuned-squad&quot;, evaluation_strategy=&quot;no&quot;, save_strategy=&quot;epoch&quot;, learning_rate=2e-5, num_train_epochs=1, weight_decay=0.01, fp16=True, push_to_hub=True, ) . PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). . from transformers import Trainer trainer = Trainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=validation_dataset, tokenizer=tokenizer, ) trainer.train() . /content/bert-finetuned-squad is already a clone of https://huggingface.co/kurianbenoy/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`. Using amp half precision backend /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 88729 Num Epochs = 1 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 11092 . . [ 4154/11092 2:29:47 &lt; 4:10:18, 0.46 it/s, Epoch 0.37/1] Step Training Loss . 500 | 1.691300 | . 1000 | 1.575600 | . 1500 | 1.441200 | . 2000 | 1.356100 | . 2500 | 1.297700 | . 3000 | 1.245200 | . 3500 | 1.242400 | . 4000 | 1.181400 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; predictions, _ = trainer.predict(validation_dataset) start_logits, end_logits = predictions compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[&quot;validation&quot;]) . trainer.push_to_hub(commit_message=&quot;Training complete&quot;) . &lt;/div&gt; .",
            "url": "https://kurianbenoy.github.io/ml-blog/huggingface/nlp/fastai/2022/02/27/QuestionAnsweringWithHF.html",
            "relUrl": "/huggingface/nlp/fastai/2022/02/27/QuestionAnsweringWithHF.html",
            "date": " â€¢ Feb 27, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Starting_my_ml_blog",
            "content": "Starting my ML blog . I am into the field of AI/DataScience for some time. To dive deep into this field, I am creating this ML blogpost space. This space is exclusively for my ML/AI articles. Also by 27th February, 2022 expect a blog post on question answering with blurr. .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/02/21/Starting_my_ML_blog.html",
            "relUrl": "/2022/02/21/Starting_my_ML_blog.html",
            "date": " â€¢ Feb 21, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://kurianbenoy.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://kurianbenoy.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}