{
  
    
        "post0": {
            "title": "Music genre classification using fast.ai",
            "content": "During first lesson of Practical Deep Learning for Coders course, Jeremy had mentioned how using simple computer vision model we can build even a model to classify audio with image classification model itself. . Recently Kaggle grandmaster Rob Mulla conducted a challenge to classify music according to what genre it was. At stakes there was a RTX 3080 Ti GPU. Let&#39;s look how we can classify music genres using a simple computer vision model which was taught in the first lesson of fast.ai. . Downloading packages and importing libraries . . Note: I had already installed fastai, pytorch for training this model before hand. . ! pip install -Uqq kaggle git+https://github.com/huggingface/huggingface_hub#egg=huggingface-hub[&quot;fastai&quot;] . from fastai.data.all import * from fastai.imports import * from fastai.vision.all import * from huggingface_hub import push_to_hub_fastai . /opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory warn(f&#34;Failed to load image Python extension: {e}&#34;) . Collecting Data . In this piece of code, I will show you how you can download datasets from Kaggle in general and the datasets I had used for training model. Inorder to train models in audio, first convert the audio to a spectogram and throw an image model. Check this tweet from Dien Hoa Truong who won a NVIDIA RTX 3080 Ti GPU in this competition. This tweet makes me stick to the spectrogram approach for the Kaggle PogChamp music genre classification competition, and finish #1. Thanks @marktenenholtz :) https://t.co/xwtXQRfk51 . &mdash; Dien Hoa Truong (@DienhoaT) April 28, 2022 . For this competition you need two datasets: . The competition data | Image data generated from converting audio to melspectograms in form of images | The data provided here are over 20,000 royalty free song samples (30 second clips) and their musical genres. Your task is to create a machine learning algorithm capable of predicting the genres of unlabeled music files. Create features, design architectures, do whatever it takes to predict them the best. . The code for downloading data from kaggle has been adopted from Jeremy&#39;s notebook . creds = &quot;&quot; from pathlib import Path cred_path = Path(&quot;~/.kaggle/kaggle.json&quot;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(creds) cred_path.chmod(0o600) . path = Path(&quot;../input/kaggle-pog-series-s01e02&quot;) path.ls() . (#6) [Path(&#39;input/kaggle-pog-series-s01e02/genres.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/sample_submission.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/test.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/test&#39;),Path(&#39;input/kaggle-pog-series-s01e02/train.csv&#39;),Path(&#39;input/kaggle-pog-series-s01e02/train&#39;)] . from zipfile import ZipFile from kaggle import api if not path.exists(): api.competition_download_cli(str(path)) ZipFile(f&quot;{path}.zip&quot;).extractall(path) . Downloading kaggle-pog-series-s01e02.zip to /home . 100%|██████████| 9.05G/9.05G [07:54&lt;00:00, 20.5MB/s] . . . ! kaggle datasets download -d dienhoa/music-genre-spectrogram-pogchamps . Downloading music-genre-spectrogram-pogchamps.zip to /home 100%|█████████████████████████████████████▉| 6.80G/6.80G [07:00&lt;00:00, 14.7MB/s] 100%|██████████████████████████████████████| 6.80G/6.80G [07:00&lt;00:00, 17.4MB/s] . Quick EDA and Data Cleaning . df_train = pd.read_csv(&quot;../input/kaggle-pog-series-s01e02/train.csv&quot;) df_train.head() . song_id filename filepath genre_id genre . 0 10150 | 010150.ogg | train/010150.ogg | 7 | Instrumental | . 1 7358 | 007358.ogg | train/007358.ogg | 2 | Punk | . 2 20573 | 020573.ogg | train/020573.ogg | 5 | Folk | . 3 11170 | 011170.ogg | train/011170.ogg | 12 | Old-Time / Historic | . 4 16662 | 016662.ogg | train/016662.ogg | 1 | Rock | . df_train[&quot;filepath&quot;] = df_train[&quot;filepath&quot;].str.replace(&quot;ogg&quot;, &quot;png&quot;) . Shows a highly imbalanced dataset . df_train[&quot;genre&quot;].value_counts() . Rock 3097 Electronic 3073 Punk 2584 Experimental 1801 Hip-Hop 1761 Folk 1215 Chiptune / Glitch 1181 Instrumental 1045 Pop 945 International 814 Ambient Electronic 796 Classical 495 Old-Time / Historic 408 Jazz 306 Country 142 Soul-RnB 94 Spoken 94 Blues 58 Easy Listening 13 Name: genre, dtype: int64 . df_train.head() . song_id filename filepath genre_id genre . 0 10150 | 010150.ogg | train/010150.png | 7 | Instrumental | . 1 7358 | 007358.ogg | train/007358.png | 2 | Punk | . 2 20573 | 020573.ogg | train/020573.png | 5 | Folk | . 3 11170 | 011170.ogg | train/011170.png | 12 | Old-Time / Historic | . 4 16662 | 016662.ogg | train/016662.png | 1 | Rock | . df_train = df_train.set_index(&quot;song_id&quot;) . df_train = df_train.drop( [ 23078, 3137, 4040, 15980, 11088, 9963, 24899, 16312, 22698, 17940, 22295, 3071, 13954, ] ) . df_train.shape . (19909, 4) . Loading Data using fastai DataLoaders . For creating this notebook, I spend a major portion of my time in cleaning and sorting out appropriate datablocks/dataloaders for training image models using fast.ai. This is something which you as a practitioner experience, compared to learning all the theory and backpropogation algorithm. . So let&#39;s see how we load this data using fast.ai. There are two approaches which we will discuss below. Both the approaches of loading data works, but the first approach as a disadvantage, which I will tell in a moment. . Approach 1. Using DataBlock and loading images . Create a data frame temp_train and create new column is_valid | is_valid is default column named created for using ColSplitter | Now set get_x which specifies the path of files for inputting data which is set as base_path+filename path: lambda o:f&#39;{path}/&#39;+o.path | Now set get_y which specifies the variable to predict, ie the genre of music | . temp_train = df_train temp_train.loc[:15000, &quot;is_valid&quot;] = True temp_train.loc[15000:, &quot;is_valid&quot;] = False . path = Path(&quot;../input/music-genre-spectrogram-pogchamps/spectograms/&quot;) . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), splitter=ColSplitter(), get_x=lambda o: f&quot;{path}/&quot; + o.path, get_y=lambda o: o.genre, item_tfms=Resize(224), batch_tfms=aug_transforms(), ) . dls = dblock.dataloaders(temp_train) . dls.show_batch() . # dblock.summary(df_train) . This worked really well, and with this approach I was even able to train a ML model which got 50% accuracy. Saturday evening side-project: Trained a baseline ML model to classify audio files to identify their music genre using @fastdotai based on a kaggle dataset.Acheived only 50% accuracy, probably because problem is hard. Next job is to check what @DienhoaT has done to win a GPU. pic.twitter.com/EahvgtYBDL . &mdash; Kurian Benoy (@kurianbenoy2) April 30, 2022 . Yet when it came to export models, due to usage of lamda method in DataBlock. I got Pickling error as the model was not able to be exported with learn.export() method. . 2. Using DataLoaders methods with loading from dataframe method . This issue got me into using approach that using ImageDataLoaders.from_df in fastai. Let&#39;s first take a look at our df_train dataframe: . df_train.head() . filename filepath genre_id genre . song_id . 10150 010150.ogg | train/010150.png | 7 | Instrumental | . 7358 007358.ogg | train/007358.png | 2 | Punk | . 20573 020573.ogg | train/020573.png | 5 | Folk | . 11170 011170.ogg | train/011170.png | 12 | Old-Time / Historic | . 16662 016662.ogg | train/016662.png | 1 | Rock | . If you look at the dataframe, we know that on appending to the path, the filepath column. . This is the exact value for get_x method in fastai fn_col = 1 which specifies the column name filepath at position 1. | label or get_y is specified by the column name genre at position 3. | valid_pct (ensure what percentage of data to be used for validation) | y_block=CategoryBlock to ensure it&#39;s used for normal classification only and not multi-label | . dls = ImageDataLoaders.from_df( df_train, path, valid_pct=0.2, seed=34, y_block=CategoryBlock, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224), fn_col=1, label_col=3, ) dls.show_batch() . Training fastai model . I trained using a resnet18 model at first, later we stepped up to use resnet50 model. . learn = vision_learner(dls, resnet50, metrics=error_rate) . learn.lr_find() . SuggestedLRs(valley=0.0008317637839354575) . learn.fine_tune(10, 0.0008317637839354575) . epoch train_loss valid_loss error_rate time . 0 | 2.869285 | 2.171426 | 0.616428 | 01:43 | . epoch train_loss valid_loss error_rate time . 0 | 2.312176 | 1.843815 | 0.558654 | 02:07 | . 1 | 2.102361 | 1.719162 | 0.539061 | 02:08 | . 2 | 1.867139 | 1.623988 | 0.527003 | 02:08 | . 3 | 1.710557 | 1.527913 | 0.507661 | 02:07 | . 4 | 1.629478 | 1.456836 | 0.479779 | 02:05 | . 5 | 1.519305 | 1.433036 | 0.474253 | 02:05 | . 6 | 1.457465 | 1.379757 | 0.464456 | 02:05 | . 7 | 1.396283 | 1.369344 | 0.457925 | 02:05 | . 8 | 1.359388 | 1.367973 | 0.453655 | 02:05 | . 9 | 1.364363 | 1.368887 | 0.456167 | 02:04 | . learn.export(&quot;model.pkl&quot;) . Pushing models to hugging face . hugging_face_hub had released a new fastai functions to easily push models. Just two functions are needed: from_pretrained_fastai and push_to_hub_fastai. . Omar Espejel had shared a fantastic notebook on these new functionalities in huggingface here. . Note: You should install git-lfs and login to huggingface account with token before pushing . push_to_hub_fastai( learn, &quot;kurianbenoy/music_genre_classification_baseline&quot;, commit_message=&quot;Resnet50 with 10 epochs of training&quot;, ) . /home/kurianbenoy/music_genre_classification_baseline is already a clone of https://huggingface.co/kurianbenoy/music_genre_classification_baseline. Make sure you pull the latest changes with `repo.git_pull()`. To https://huggingface.co/kurianbenoy/music_genre_classification_baseline 390320d..3605083 main -&gt; main . &#39;https://huggingface.co/kurianbenoy/music_genre_classification_baseline/commit/360508311005aefeb3ca29933f2173202afe4f30&#39; . Taking a look at results . learn.show_results() . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(15, 10)) . Conclusion . We trained a ML model which can identify with 54.4% accuracy to classify in a music file which genre it is. It&#39;s not so bad for a baseline model. Dien Hoa Truong has shared some techniques which he learned during Kaggle competition with music genres. [Machine Learning] Speed up your experiment so that you can try out a lot of ideas and find what works best for your problem. Below ⬇️ is what I have learned during the Kaggle competition with music genre🎶: . &mdash; Dien Hoa Truong (@DienhoaT) April 30, 2022 . Thanks for reading :pray: .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/05/01/AudioCNNDemo.html",
            "relUrl": "/fastai/fastbook/2022/05/01/AudioCNNDemo.html",
            "date": " • May 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Practical Deep Learning for Coders Course - Lesson 1",
            "content": "First there was a set of introductions by university officials at UQ like VC. One curious thing was everyone of UQ staff were honouring something traditionaly of that land to live in reconciliation. . Then lecture of Jeremy starts, seeing his face the chatbox is in delight. . Jeremy mentions there are two categories of students who attend the course: . Students who have enrolled via University of Queensland(with almost 350 people attending in-person and about 100 people remotely as well). | fastai fellows who have acknowledged for their contribution to community. | Jeremy recommends having study buddies when we are learning the course is important. So he asks to create Study groups wherever possible. This course is now happening after a gap of 2 years, so there is a lot of new things which has to be covered as Deep learning moves so fast. . Using Dalle-2 technique we can generative creative images from generate twitter bios. For a creative person, this can be very helpful to create good artwork. Then one of another popular techniques was using Pathways language model which is able to answers question with explanations and even explains why some jokes are funny. . Jeremy talks about his interest in education.He is a homeschooler and learned from books by Paul Lockhart &amp; David Perkins which were inspiration for fast.ai. fastai teaches stuff in top-down manner. You will go into as much technical stuff as you, yet you will learn and implement cool stuff steadily. . About fast.ai course . He wrote an awesome book and this course. His book is one of the best sellers in Deep Learning and used to teach folks in companies like Tesla, OpenAI etc. Almost 6 million people watched his videos so far.Jeremy has won multiple competitions in Kaggle, was the CEO of kaggle. He build Enlitic, a medical company which was build for medical purpose with two other succesful startups. . Jeremy mentioned for this course, we are not using any material directly from Deep Learning For Coders with Fastai &amp; Pytorch book. Yet he recommends to read portions of book after each chapter. . Usually multiple people learn better if the same idea is exposed in different way from multiple sources. That&#39;s the why behind this approach. . Jeremy started coding hands-own a bird or park classifier, which was considered as a very difficult problem in 2015. Even a comic depicted this. Yet things have changed so drastically in past few years, that it&#39;s very easy to do that now. . Yet let&#39;s look, why we couldn&#39;t build a bird classifer in 2015:- For classifying histopothical images. They used computer vision techniques.- They got big team of datascientist, mathematicans with lot of features who build relevant feature for machine learning hand by hand. . These project took years | Also deep learning was not in radar for researchers then. | . What has now changed? . Using neural network they build these features on their own. | Mathew D Zeiler &amp; Rob Fergus(and actual weights) showed with visualization how neural networks work | Combine all features to learn and go slowly in past, neural networks learned on it&#39;s own these techniques. | . If it&#39;s a bird or not? notebook can be found here. I am slightly tweaking this model to leverage pytorch image-models released by timm. . urls = search_images(&#39;bird photos&#39;, max_images=1) urls[0] . from fastdownload import download_url dest = &#39;bird.jpg&#39; download_url(urls[0], dest, show_progress=False) from fastai.vision.all import * im = Image.open(dest) im.to_thumb(256,256) . Note: . Image based algorithms, are not for images. Image for music classification by Deolho, Ethan sutin sounds from image recognizer. You can do music classification, with some creativity using cnns. . | Also needing lots of data is a myth created by companies who sell data processng units. There are lot of free resources like Kaggle, Colab etc. . | . Observation by Jeremy:Tensorflow is slowly dying. Check this article which he cited. Yet pytorch has lot of hairy code, which can be solved using good abstractions in fastai. . fastai library tries to provide good and the best fine-tuned models, which work well compared to other libraries. He showed code required for implementing AdamW in pytorch and in fastai. | . Tanishq Abraham pointed me to implemtation of AdamW to chapter 16 in fastbook. . download_url(search_images(&#39;forest photos&#39;, max_images=1)[0], &#39;forest.jpg&#39;, show_progress=False) Image.open(&#39;forest.jpg&#39;).to_thumb(256,256) searches = &#39;forest&#39;,&#39;bird&#39; path = Path(&#39;bird_or_not&#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) resize_images(path/o, max_size=400, dest=path/o) . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . As the code showed, data cleaning is a big part of machine learninng. When we are learning this course as practitioners, we will spend lot of time of building and loading models. Like in compiler course lot of time is not spend on techniques, but on getting the environment up and ready. . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(224, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch() . After examining, 100s of project and datascience requirments. fastai came up with this approach of DataBlock, which consists of five things: . blocks | get_items | splitter | Batch_tfms(optional) | get_y | item_tfms | Without validation data, it won&#39;t allow to train. parent_label, return parent folder. we saved as forests or birds. We need same size. Idea to do quickly, why not publish vision_learners with pets dataset. . Now it&#39;s time to train our model . learn = vision_learner(dls, &#39;vit_tiny_patch16_224&#39;, metrics=error_rate) learn.fine_tune(10) . One thing which is cool is that the whole presentation is also made with Jupyter Notebooks using RiseJS. Also jupyter notebooks can be used for writing books like Deep Learning for Coders, for blogging using fastpages, for CI/CD pipeline to run in parallel execution in fastai repo. . Tanishq Mathew Abraham has summarized on what can be done in this twitter threads. Awesome and surprising things you can do with Jupyter Notebooks ⬇ . &mdash; Tanishq Mathew Abraham (@iScienceLuvr) April 27, 2022 After this Jeremy, showed all the examples in Chapter 1 in Deep Learning for coders. My notes then: . We are still scratching the surface. Lot of marketing out there, some of first open source models available. The deep learning when it broke X, y, z in domain. In NLP it breaks lot of stuff . What&#39;s really go in on : in arthur samuel with graph. The graphs are build with gv2 in jupyter notebook. Deploying models in ML is a bit tricky. But it&#39;s just predict and shows results. . Conclusion by Jeremy . So after first lesson: . a) If you know python, then it&#39;s kind of easy for you. b) If don&#39;t know python, it&#39;s very difficult . Regardless of what level you are. Experiment yourself and do something more complex. Go ahead and push yourself a little bit, but not much. Then present your work. Do stuff on things where you are interested. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/04/26/fastai-51.html",
            "relUrl": "/fastai/fastbook/2022/04/26/fastai-51.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Practical Deep Learning for Coders Course - Lesson 0",
            "content": "Last week I enrolled for the live cohort of Deep Learning For Coders with fastai Couse which is going to be taken by Jeremy Howard. It&#39;s&#39; a previlege at the same time, a dream come true moment for me, as a fastai student who took some part of fast.ai from the year 2018. . The fastai course has seen a lot of success stories over the years. Amazing folks like Aman Arora, Even Oldridge, Sanyam Bhutani,Radek Osmulski, Jason Antic, Wayde Gilliam, Zach Mueller ... While there are lot of hidden gems whom may not be so well know in community as well like Bhuvana who shared her journey in Pycon India 2019. The way this course has democratized AI &amp; ML is simply amazing. . Today Radek Osmulski joined NVIDIA AI. Today is my first day at @NVIDIAAI! 🥳-From learning to code at 29-through learning ML @fastdotai-winning a @kaggle competition-jobs at 🔥 startups-moving continents thx to AI-to joining the illustrious Merlin team ❤️I am beyond grateful 🙏Will make this one count! . &mdash; Radek Osmulski 🇺🇦 (@radekosmulski) April 26, 2022 . It was Radek who had posted in fastai forums few years back on how he will approach taking the fastai course. Inspired by that I am creating this action list items on things which I am planning to achieve during duration of this course. . I will do following: . Try out every notebook given by Jeremy during class and writes in Kaggle | Write blogpost every week with lesson summary and occasionally on new things I learning during course. | Build some hugging face spaces using fastai | Challenge myself to complete the existing project I am doing on IPL Commentary Summarizer and learn NLP extensively. | Attend the delft-fast-ai-study-group sessions atleast for five weeks. | Go to sleep before midnight | Dream and breath pytorch | Extra Credits: . Participate in NLP competition provided by jeremy link. | Frequent in fast.ai forums and discord group | Take a look at notebooks in Transformer Book if you cover NLP extensively during class | Try to read Pytorch book | Complete all the college assignments in Pytorch | I won&#39;t do: . Be active in twitter during course | Read EDA notebooks | Won&#39;t write any notebooks in tensorflow :) | Won&#39;t participate in any other kaggle comps during course period. | I will be not attending Real Python community meetups, unless I have a very compelling doubt. | With that I am winding for today. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/fastbook/2022/04/26/fastai-50-ipynb.html",
            "relUrl": "/fastai/fastbook/2022/04/26/fastai-50-ipynb.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Building a fine-tuned translation system for English-Malayalam",
            "content": "Hey, everyone. We all are familiar with translation systems like using google translate. So today, let&#39;s build a fine tuned translation system for converting text from english to malayalam. It&#39;s built using Blurr library - built on top of Hugging face and fast.ai made by Wayde Gilliam. Also our translation system is going to be fine tuned on top of KDE specific dataset. You can find the trained model here. . . . Loading Data . A translation system is an example of sequence to sequence models, which is usually used for tasks which involves generating new data. Translation usually needs datasets in both the source language and target language (the language to which it needs to be translated). . We are using KDE4 datasets, and choose both source language and translation language as english and malayalam respectively. Usually these datasets are curated by community volunteers to their native language, and this was probably done by KDE community volunteers in Kerala. When someone is localizing these texts into there in local languague, usually computer science specific terms are still written in english. . import pandas from datasets import load_dataset . raw_datasets = load_dataset(&quot;kde4&quot;, lang1=&quot;en&quot;, lang2=&quot;ml&quot;, split=&quot;train[:1000]&quot;) . Using custom data configuration en-ml-lang1=en,lang2=ml . Downloading and preparing dataset kde4/en-ml to /root/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac... Dataset kde4 downloaded and prepared to /root/.cache/huggingface/datasets/kde4/en-ml-lang1=en,lang2=ml/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac. Subsequent calls will reuse this data. . Most of translation dataset is in form of id and translation json output - with both en and ml as objects. . raw_datasets[0] . {&#39;id&#39;: &#39;0&#39;, &#39;translation&#39;: {&#39;en&#39;: &#39;Add Feed to Akregator&#39;, &#39;ml&#39;: &#39;അക്രിഗേറ്ററില് u200d ഫീഡ് കൂട്ടിച്ചേര് u200dക്കുക&#39;}} . . Transforming data into DataLoaders . Importing libraries and get hugging-face objects . from blurr.data.all import * from blurr.modeling.all import * from fastai.data.all import * from fastai.callback.all import * from fastai.learner import * from fastai.optimizer import * from transformers import * . pretrained_model_name = &quot;Helsinki-NLP/opus-mt-en-ml&quot; model_cls = AutoModelForSeq2SeqLM hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls) hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model) . (&#39;marian&#39;, transformers.models.marian.configuration_marian.MarianConfig, transformers.models.marian.tokenization_marian.MarianTokenizer, transformers.models.marian.modeling_marian.MarianMTModel) . translation_df = pd.DataFrame(raw_datasets[&quot;translation&quot;], columns=[&quot;en&quot;, &quot;ml&quot;]) translation_df.head() . en ml . 0 Add Feed to Akregator | അക്രിഗേറ്ററില്‍ ഫീഡ് കൂട്ടിച്ചേര്‍ക്കുക | . 1 Add Feeds to Akregator | അക്രിഗേറ്ററില്‍ ഫീഡുകള്‍ കൂട്ടിച്ചേര്‍ക്കുക | . 2 Add All Found Feeds to Akregator | എല്ലാ ഫീഡുകളും അക്രിഗേറ്ററില്‍ കൂട്ടിച്ചേര്‍ക്കുക | . 3 Subscribe to site updates (using news feed) | സൈറ്റുകളിലെ പുതുമകളറിയാന്‍ വരിക്കാരനാകുക (വാര്‍ത്താ ഫീഡുകള്‍ ഉപയോഗിച്ചു്) | . 4 Imported Feeds | എടുത്ത ഫീഡുകള്‍ | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; blocks = (Seq2SeqTextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), noop) dblock = DataBlock(blocks=blocks, get_x=ColReader(&quot;en&quot;), get_y=ColReader(&quot;ml&quot;), splitter=RandomSplitter()) . dls = dblock.dataloaders(translation_df, bs=1) dls.show_batch(dataloaders=dls, max_n=2, input_trunc_at=100, target_trunc_at=250) . text target . 0 A▁version▁control▁history▁entry▁consists of▁several▁lines.▁Specify the▁regular▁expression▁to▁detect | ഒരു ഭാഷാന്തര നിയന്ത്രണത്തിന്റെ നാള്വഴി ചേര്ക്കുന്നതില് പല വരികളുണ്ടാകും. ആദ്യത്തെ വരി കണ്ടുപിടിക്കാനുള്ള നിത്യഭാവം നിര്ദ്ദേശിക്കുക (മുന്നിലെ വിശദീകരണം കൂടാതെ). ഇനം തിരിക്കാനുപയോഗിക്കുന്ന കീകളെ ഒന്നിച്ചാക്കാന് ബ്രാക്കറ്റുകള് ഉപയോഗിക്കുക. ഒഴിച്ചു വിട്ട | . . Training fine-tuned translation system . Using blurr High-level API . learn = BlearnerForTranslation.from_data( translation_df, pretrained_model_name, src_lang_name=&quot;English&quot;, src_lang_attr=&quot;en&quot;, trg_lang_name=&quot;Malayalam&quot;, trg_lang_attr=&quot;ml&quot;, dl_kwargs={&quot;bs&quot;: 4}, ) . metrics_cb = BlearnerForTranslation.get_metrics_cb() learn.fit_one_cycle(1, lr_max=4e-5, cbs=[metrics_cb]) . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. [nltk_data] Downloading package omw-1.4 to /root/nltk_data... [nltk_data] Unzipping corpora/omw-1.4.zip. . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 4.847442 | 3.989661 | 0.028411 | 0.188749 | 5.151875 | 00:51 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=250) . text target prediction . 0 A▁version▁control▁history▁entry▁consists of▁several▁lines.▁Specify the▁regular▁expression▁to▁detect the▁first▁line (without the▁leading▁comment).▁Use▁parentheses▁to▁group the▁keys▁you▁want▁to▁use▁for▁sorting.▁If▁left▁empty,▁then▁KDiff3▁assumes▁that▁empty▁lines▁separate▁history▁entries.▁See the▁documentation▁for▁details. | ഒരു ഭാഷാന്തര നിയന്ത്രണത്തിന്റെ നാള്വഴി ചേര്ക്കുന്നതില് പല വരികളുണ്ടാകും. ആദ്യത്തെ വരി കണ്ടുപിടിക്കാനുള്ള നിത്യഭാവം നിര്ദ്ദേശിക്കുക (മുന്നിലെ വിശദീകരണം കൂടാതെ). ഇനം തിരിക്കാനുപയോഗിക്കുന്ന കീകളെ ഒന്നിച്ചാക്കാന് ബ്രാക്കറ്റുകള് ഉപയോഗിക്കുക. ഒഴിച്ചു വിട്ട | [ഒരു പതിപ്പ് നിയന്ത്രകന്റെ നിയന്ത്രണം പല വരികളിലുമുണ്ടു്. ആദ്യത്തെ വരി (വലുപ്പത്തില്നിയ്ക്കു്) കണ്ടെത്താനുള്ള ക്രമമായ പ്രയോഗം വ്യക്തമാക്കുക. നിങ്ങൾ ടൈപ്പ് ചെയ്യുവാനുള്ള കീകൾ ടൈപ്പ് ചെയ്യുക. വിട്ടുപോയാൽ, ഒഴിഞ്ഞ വര്ണ്ണങ്ങള്ക്കുള്ള വര്ണ്ണങ്ങള്ക്കുള്ള വര്ണങ്ങള്ണങ്ങള്ക്ക്ണങ്ങള്ണങ്ങള്ക്കങ്ങള്ണ്ക്കങ്ങള്ക്കണ്ടു്. വിവരങ്ങള്ക്ക് KDef3, ഡാറ്റ നഷ്ടപ്പെടാനോ മറ്റു് കേടുപാടുകള്ക്കോ തടയാനോ നിങ്ങൾക്ക് സിസ്റ്റം സസ്പെൻഡ് ചെയ്യാനോ, ഹ്യൂസ്ബര്ഡ് ചെയ്യാനോ സാധിക്കുന്നു, അതിനാൽ ബാറ്ററി അധികാരത്തിൽ നിന്ന് നിങ്ങൾ ഓടരുത്. മെഷീൻ ക്രമീകരിക്കുന്ന പ്രവർത്തനങ്ങളുടെ എണ്ണം താഴെ ക്രമീകരിക്കുക., ഒരു ഇമെയിൽ വിലാസം പോലെ കാണുന്ന ഒരു കണ്ണിയിൽ ക്ലിക്ക് ചെയ്തിട്ടുണ്ടു്. പക്ഷേ അതിന്റെ പിൻഭാഗത്ത് ഇതു് ചേരുന്നില്ല:% 1 lock to be track about to cause for play for play played by completion% 2, നിങ്ങൾ ശരിക്കും എങ്ങോട്ടു് പോകുമെന്നു് കൃത്യമായി സൂചിപ്പിക്കുവാൻ സാധ്യമല്ലാത്ത ഒരു കണ്ണിയിൽ ക്ലിക്ക് ചെയ്തിട്ടുണ്ടു്. ഈ സർവറിലുള്ള ഒരു താള് ശരിക്കും കാണണമെങ്കിൽ ദയവായി പരിശോധിക്കുക:% 1 അവിടെ പോകണമോ? @ title: group column] | . Using mid-level of blurr APIs . b = dls.one_batch() . len(b), b[0][&quot;input_ids&quot;].shape, b[1].shape . (2, torch.Size([1, 72]), torch.Size([1, 114])) . dls.show_batch(dataloaders=dls, input_trunc_at=250, target_trunc_at=250) . text target . 0 A▁version▁control▁history▁entry▁consists of▁several▁lines.▁Specify the▁regular▁expression▁to▁detect the▁first▁line (without the▁leading▁comment).▁Use▁parentheses▁to▁group the▁keys▁you▁want▁to▁use▁for▁sorting.▁If▁left▁empty,▁then▁KDiff3▁assumes▁that▁e | ഒരു ഭാഷാന്തര നിയന്ത്രണത്തിന്റെ നാള്വഴി ചേര്ക്കുന്നതില് പല വരികളുണ്ടാകും. ആദ്യത്തെ വരി കണ്ടുപിടിക്കാനുള്ള നിത്യഭാവം നിര്ദ്ദേശിക്കുക (മുന്നിലെ വിശദീകരണം കൂടാതെ). ഇനം തിരിക്കാനുപയോഗിക്കുന്ന കീകളെ ഒന്നിച്ചാക്കാന് ബ്രാക്കറ്റുകള് ഉപയോഗിക്കുക. ഒഴിച്ചു വിട്ട | . seq2seq_metrics = {&quot;bleu&quot;: {&quot;returns&quot;: &quot;bleu&quot;}, &quot;meteor&quot;: {&quot;returns&quot;: &quot;meteor&quot;}, &quot;sacrebleu&quot;: {&quot;returns&quot;: &quot;score&quot;}} model = BaseModelWrapper(hf_model) learn_cbs = [BaseModelCallback] fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)] learn = Learner( dls, model, opt_func=partial(Adam), loss_func=PreCalculatedCrossEntropyLoss(), # CrossEntropyLossFlat() cbs=learn_cbs, splitter=partial(blurr_seq2seq_splitter, arch=hf_arch), ) learn.freeze() . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! [nltk_data] Downloading package omw-1.4 to /root/nltk_data... [nltk_data] Package omw-1.4 is already up-to-date! . learn.lr_find(suggest_funcs=[minimum, steep, valley, slide]) . SuggestedLRs(minimum=0.0002511886414140463, steep=1.3182567499825382e-06, valley=0.0003981071640737355, slide=3.0199516913853586e-05) . learn.fit_one_cycle(3, lr_max=0.0003981071640737355, cbs=fit_cbs) . epoch train_loss valid_loss bleu meteor sacrebleu time . 0 | 3.522702 | 3.374413 | 0.033723 | 0.155236 | 3.005034 | 02:58 | . 1 | 2.269610 | 3.002045 | 0.022526 | 0.168159 | 3.360931 | 02:09 | . 2 | 1.717019 | 2.910453 | 0.051017 | 0.194193 | 5.273562 | 02:18 | . learn.show_results(learner=learn, input_trunc_at=500, target_trunc_at=500) . text target prediction . 0 ▁Regular▁expression▁for▁lines▁where▁KDiff3▁should▁automatically▁choose▁one▁source.▁When▁a▁line▁with▁a▁conflict▁matches the▁regular▁expression▁then -▁if▁available - C,▁otherwise B▁will▁be▁chosen. | കെഡിഫ്3 സ്വതന്ത്രമായി ഒരു സ്രോതസ്സ് തെരഞ്ഞെടുക്കുന്നിടത്ത് വരികള്ക്കുള്ള നിത്യഭാവം. സംഘട്ടനമുള്ള വരി ചേര്ന്നുവരുമ്പോള് അതിന്റെ നിത്യഭാവം - സി ഉണ്ടെങ്കില് അത്, അല്ലെങ്കില് ബി തെരഞ്ഞെടുക്കപ്പെടും. | കെഡിഫ്3 സ്വയം തിരഞ്ഞെടുക്കേണ്ട വരികളുടെ സാധാരണ പ്രയോഗം ഒരു സ്രോതസ്സ് തെരഞ്ഞെടുക്കുക. നിത്യഭാവവുമായി ചേരുമ്പോള്ച്ചയെ - സി - സി | . . Now let&#39;s translate with our trained models . blurr top 3 translation predictions . test_text = &quot;How are you doing&quot; outputs = learn.blurr_generate(test_text, key=&quot;translation_texts&quot;, num_return_sequences=3) outputs . [{&#39;translation_texts&#39;: [&#39;എന്തൊക്കെയുണ്ട്?&#39;, &#39;എങ്ങനെയുണ്ട്?&#39;, &#39;എന്തൊക്കെയുണ്ട് വിശേഷം&#39;]}] . Saving trained ML model . export_fname = &quot;saved_model&quot; learn.metrics = None learn.export(fname=f&quot;{export_fname}.pkl&quot;) . Prediction with our trained model . Correct Prediction . test_text = &quot;How are you doing&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text) . [{&#39;translation_texts&#39;: &#39;എന്തൊക്കെയുണ്ട്?&#39;}] . test_text1 = &quot;Add All Found Feeds to Akregator.&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text1) . [{&#39;translation_texts&#39;: &#39;എല്ലാ ഫീഡുകളും അക്രിഗേറ്ററിയിലേക്ക് ചേരുക.&#39;}] . Wrong Prediction . test_text2 = &quot;Subscribe to site updates (using news feed).&quot; inf_learn = load_learner(fname=f&quot;{export_fname}.pkl&quot;) inf_learn.blurr_translate(test_text2) . [{&#39;translation_texts&#39;: &#39;സൈറ്റ് പുതുക്കുവാനുള്ള ഉപാധികള്color (ഒരു ന്യൂസ് ആഹാരവുമായി.).&#39;}] . Expected: &#39;സൈറ്റുകളിലെ പുതുമകളറിയാന് u200d വരിക്കാരനാകുക (വാര് u200dത്താ ഫീഡുകള് u200d ഉപയോഗിച്ചു് . Thanks to . Wayde Gilliam - for creating blurr, and helping with doubts in translation bits | Kevin Bird - for helping in editing the article. | . fin. .",
            "url": "https://kurianbenoy.github.io/ml-blog/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/13/hfenml-translate.html",
            "relUrl": "/fastai/huggingface/translation/fine%20tuning/malayalam/2022/03/13/hfenml-translate.html",
            "date": " • Mar 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "A sneek peak into implementing QuestionAnswering With HuggingFace (inprogress)",
            "content": ". from datasets import load_dataset raw_datasets = load_dataset(&quot;squad&quot;) . Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453... Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data. . raw_datasets . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 87599 }) validation: Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 10570 }) }) . Recently a tweet went viral . We can see that NLP is hard, and even the best publicly avaiable models performs poorly in this tasks | . . from transformers import pipeline classifier = pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased&quot;) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.weight&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;classifier.weight&#39;, &#39;pre_classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . [{&#39;label&#39;: &#39;LABEL_0&#39;, &#39;score&#39;: 0.5706434845924377}] . checkpoint = &quot;cardiffnlp/twitter-roberta-base-sentiment&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . [{&#39;label&#39;: &#39;LABEL_2&#39;, &#39;score&#39;: 0.9608174562454224}] . checkpoint = &quot;finiteautomata/bertweet-base-sentiment-analysis&quot; classifier = pipeline(&quot;sentiment-analysis&quot;, model=checkpoint) classifier(&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;) . emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji . [{&#39;label&#39;: &#39;NEG&#39;, &#39;score&#39;: 0.9447618126869202}] . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;Where do I work?&quot;, context=&quot;My name is Sylvain and I work at Hugging Face in Brooklyn&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Hugging Face&#39;, &#39;end&#39;: 45, &#39;score&#39;: 0.6949771046638489, &#39;start&#39;: 33} . question_answerer = pipeline(&quot;question-answering&quot;) question_answerer( question=&quot;What was feedback?&quot;, context=&quot;Thank you for sending my baggage to Hyd and flying me to Calcutta at the same time. Brilliant service.#DieIndigo&quot;, ) . No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad) . {&#39;answer&#39;: &#39;Brilliant service&#39;, &#39;end&#39;: 101, &#39;score&#39;: 0.13661321997642517, &#39;start&#39;: 84} . For specific question answering - use question answering models . Encoder-only models like BERT tend to be great at extracting answers to factoid questions like “Who invented the Transformer architecture?” but fare poorly when given open-ended questions like “Why is the sky blue?” In these more challenging cases . If you’re interested in this type of generative question answering, we recommend checking out our demo based on the ELI5 dataset. Using Lonformer model. . [1] https://yjernite.github.io/lfqa.html . print(&quot;Context: &quot;, raw_datasets[&quot;train&quot;][0][&quot;context&quot;]) print(&quot;Question: &quot;, raw_datasets[&quot;train&quot;][0][&quot;question&quot;]) print(&quot;Answer: &quot;, raw_datasets[&quot;train&quot;][0][&quot;answers&quot;]) . Context: Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34;Venite Ad Me Omnes&#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? Answer: {&#39;text&#39;: [&#39;Saint Bernadette Soubirous&#39;], &#39;answer_start&#39;: [515]} . raw_datasets[&quot;train&quot;].filter(lambda x: len(x[&quot;answers&quot;][&quot;text&quot;]) != 1) . Dataset({ features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;], num_rows: 0 }) . print(raw_datasets[&quot;validation&quot;][0][&quot;answers&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;answers&quot;]) . {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]} {&#39;text&#39;: [&#39;Santa Clara, California&#39;, &#34;Levi&#39;s Stadium&#34;, &#34;Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California.&#34;], &#39;answer_start&#39;: [403, 355, 355]} . print(raw_datasets[&quot;validation&quot;][2][&quot;context&quot;]) print(raw_datasets[&quot;validation&quot;][2][&quot;question&quot;]) . Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi&#39;s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the &#34;golden anniversary&#34; with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as &#34;Super Bowl L&#34;), so that the logo could prominently feature the Arabic numerals 50. Where did Super Bowl 50 take place? . from transformers import AutoTokenizer model_checkpoint = &quot;bert-base-cased&quot; tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . context = raw_datasets[&quot;train&quot;][0][&quot;context&quot;] question = raw_datasets[&quot;train&quot;][0][&quot;question&quot;] inputs = tokenizer(question, context) tokenizer.decode(inputs[&quot;input_ids&quot;]) . &#39;[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]&#39; . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, ) for ids in inputs[&quot;input_ids&quot;]: print(tokenizer.decode(ids)) . [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP] [CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] . inputs = tokenizer( question, context, max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) inputs.keys() . dict_keys([&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;offset_mapping&#39;, &#39;overflow_to_sample_mapping&#39;]) . inputs[&quot;overflow_to_sample_mapping&quot;] . [0, 0, 0, 0] . inputs = tokenizer( raw_datasets[&quot;train&quot;][2:6][&quot;question&quot;], raw_datasets[&quot;train&quot;][2:6][&quot;context&quot;], max_length=100, truncation=&quot;only_second&quot;, stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) print(f&quot;The 4 examples gave {len(inputs[&#39;input_ids&#39;])} features.&quot;) print(f&quot;Here is where each comes from: {inputs[&#39;overflow_to_sample_mapping&#39;]}.&quot;) . The 4 examples gave 19 features. Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]. . Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If that’s not the case, we loop to find the first and last token of the answer: . answers = raw_datasets[&quot;train&quot;][2:6][&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(inputs[&quot;offset_mapping&quot;]): sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) start_positions, end_positions . ([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0], [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0]) . idx = 0 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] start = start_positions[idx] end = end_positions[idx] labeled_answer = tokenizer.decode(inputs[&quot;input_ids&quot;][idx][start : end + 1]) print(f&quot;Theoretical answer: {answer}, labels give: {labeled_answer}&quot;) . Theoretical answer: the Main Building, labels give: the Main Building . idx = 4 sample_idx = inputs[&quot;overflow_to_sample_mapping&quot;][idx] answer = answers[sample_idx][&quot;text&quot;][0] decoded_example = tokenizer.decode(inputs[&quot;input_ids&quot;][idx]) print(f&quot;Theoretical answer: {answer}, decoded example: {decoded_example}&quot;) . Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building&#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &#34; Venite Ad Me Omnes &#34;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP] . max_length = 384 stride = 128 def preprocess_training_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) offset_mapping = inputs.pop(&quot;offset_mapping&quot;) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) answers = examples[&quot;answers&quot;] start_positions = [] end_positions = [] for i, offset in enumerate(offset_mapping): sample_idx = sample_map[i] answer = answers[sample_idx] start_char = answer[&quot;answer_start&quot;][0] end_char = answer[&quot;answer_start&quot;][0] + len(answer[&quot;text&quot;][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it&#39;s the start and end token positions idx = context_start while idx &lt;= context_end and offset[idx][0] &lt;= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx &gt;= context_start and offset[idx][1] &gt;= end_char: idx -= 1 end_positions.append(idx + 1) inputs[&quot;start_positions&quot;] = start_positions inputs[&quot;end_positions&quot;] = end_positions return inputs . train_dataset = raw_datasets[&quot;train&quot;].map( preprocess_training_examples, batched=True, remove_columns=raw_datasets[&quot;train&quot;].column_names, ) len(raw_datasets[&quot;train&quot;]), len(train_dataset) . (87599, 88729) . &#39;Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building &#39;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot; Venite Ad Me Omnes &quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]&#39; . Indeed, we don’t see the answer inside the context. . ✏️ Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied. . Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We’ll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here: . def preprocess_validation_examples(examples): questions = [q.strip() for q in examples[&quot;question&quot;]] inputs = tokenizer( questions, examples[&quot;context&quot;], max_length=max_length, truncation=&quot;only_second&quot;, stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=&quot;max_length&quot;, ) sample_map = inputs.pop(&quot;overflow_to_sample_mapping&quot;) example_ids = [] for i in range(len(inputs[&quot;input_ids&quot;])): sample_idx = sample_map[i] example_ids.append(examples[&quot;id&quot;][sample_idx]) sequence_ids = inputs.sequence_ids(i) offset = inputs[&quot;offset_mapping&quot;][i] inputs[&quot;offset_mapping&quot;][i] = [ o if sequence_ids[k] == 1 else None for k, o in enumerate(offset) ] inputs[&quot;example_id&quot;] = example_ids return inputs . validation_dataset = raw_datasets[&quot;validation&quot;].map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) len(raw_datasets[&quot;validation&quot;]), len(validation_dataset) . (10570, 10822) . small_eval_set = raw_datasets[&quot;validation&quot;].select(range(100)) trained_checkpoint = &quot;distilbert-base-cased-distilled-squad&quot; tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint) eval_set = small_eval_set.map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[&quot;validation&quot;].column_names, ) . tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) . import torch from transformers import AutoModelForQuestionAnswering eval_set_for_model = eval_set.remove_columns([&quot;example_id&quot;, &quot;offset_mapping&quot;]) eval_set_for_model.set_format(&quot;torch&quot;) device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names} trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to( device ) with torch.no_grad(): outputs = trained_model(**batch) . start_logits = outputs.start_logits.cpu().numpy() end_logits = outputs.end_logits.cpu().numpy() . import collections example_to_features = collections.defaultdict(list) for idx, feature in enumerate(eval_set): example_to_features[feature[&quot;example_id&quot;]].append(idx) . import numpy as np n_best = 20 max_answer_length = 30 predicted_answers = [] for example in small_eval_set: example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = eval_set[&quot;offset_mapping&quot;][feature_index] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length. if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answers.append( { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } ) best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]}) . from datasets import load_metric metric = load_metric(&quot;squad&quot;) . metric . Metric(name: &#34;squad&#34;, features: {&#39;predictions&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;prediction_text&#39;: Value(dtype=&#39;string&#39;, id=None)}, &#39;references&#39;: {&#39;id&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answers&#39;: Sequence(feature={&#39;text&#39;: Value(dtype=&#39;string&#39;, id=None), &#39;answer_start&#39;: Value(dtype=&#39;int32&#39;, id=None)}, length=-1, id=None)}}, usage: &#34;&#34;&#34; Computes SQuAD scores (F1 and EM). Args: predictions: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair as given in the references (see below) - &#39;prediction_text&#39;: the text of the answer references: List of question-answers dictionaries with the following key-values: - &#39;id&#39;: id of the question-answer pair (see above), - &#39;answers&#39;: a Dict in the SQuAD dataset format { &#39;text&#39;: list of possible texts for the answer, as a list of strings &#39;answer_start&#39;: list of start positions for the answer, as a list of ints } Note that answer_start values are not taken into account to compute the metric. Returns: &#39;exact_match&#39;: Exact match (the normalized answer exactly match the gold answer) &#39;f1&#39;: The F-score of predicted tokens versus the gold answer Examples: &gt;&gt;&gt; predictions = [{&#39;prediction_text&#39;: &#39;1976&#39;, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; references = [{&#39;answers&#39;: {&#39;answer_start&#39;: [97], &#39;text&#39;: [&#39;1976&#39;]}, &#39;id&#39;: &#39;56e10a3be3433e1400422b22&#39;}] &gt;&gt;&gt; squad_metric = datasets.load_metric(&#34;squad&#34;) &gt;&gt;&gt; results = squad_metric.compute(predictions=predictions, references=references) &gt;&gt;&gt; print(results) {&#39;exact_match&#39;: 100.0, &#39;f1&#39;: 100.0} &#34;&#34;&#34;, stored examples: 0) . theoretical_answers = [ {&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in small_eval_set ] print(predicted_answers[0]) print(theoretical_answers[0]) . {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;prediction_text&#39;: &#39;Denver Broncos&#39;} {&#39;id&#39;: &#39;56be4db0acb8001400a502ec&#39;, &#39;answers&#39;: {&#39;text&#39;: [&#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;, &#39;Denver Broncos&#39;], &#39;answer_start&#39;: [177, 177, 177]}} . metric.compute(predictions=predicted_answers, references=theoretical_answers) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . from tqdm.auto import tqdm def compute_metrics(start_logits, end_logits, features, examples): example_to_features = collections.defaultdict(list) for idx, feature in enumerate(features): example_to_features[feature[&quot;example_id&quot;]].append(idx) predicted_answers = [] for example in tqdm(examples): example_id = example[&quot;id&quot;] context = example[&quot;context&quot;] answers = [] # Loop through all features associated with that example for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = features[feature_index][&quot;offset_mapping&quot;] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either &lt; 0 or &gt; max_answer_length if ( end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length ): continue answer = { &quot;text&quot;: context[offsets[start_index][0] : offsets[end_index][1]], &quot;logit_score&quot;: start_logit[start_index] + end_logit[end_index], } answers.append(answer) # Select the answer with the best score if len(answers) &gt; 0: best_answer = max(answers, key=lambda x: x[&quot;logit_score&quot;]) predicted_answers.append( {&quot;id&quot;: example_id, &quot;prediction_text&quot;: best_answer[&quot;text&quot;]} ) else: predicted_answers.append({&quot;id&quot;: example_id, &quot;prediction_text&quot;: &quot;&quot;}) theoretical_answers = [{&quot;id&quot;: ex[&quot;id&quot;], &quot;answers&quot;: ex[&quot;answers&quot;]} for ex in examples] return metric.compute(predictions=predicted_answers, references=theoretical_answers) . compute_metrics(start_logits, end_logits, eval_set, small_eval_set) . {&#39;exact_match&#39;: 83.0, &#39;f1&#39;: 88.25000000000004} . model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) . Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: [&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#39;qa_outputs.bias&#39;, &#39;qa_outputs.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . from huggingface_hub import notebook_login notebook_login() . Login successful Your token has been saved to /root/.huggingface/token Authenticated through git-credential store but this isn&#39;t the helper defined on your machine. You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default git config --global credential.helper store . ! sudo apt install git-lfs ! git-lfs install . Reading package lists... Done Building dependency tree Reading state information... Done git-lfs is already the newest version (2.3.4-1). The following package was automatically installed and is no longer required: libnvidia-common-470 Use &#39;sudo apt autoremove&#39; to remove it. 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Error: Failed to call git rev-parse --git-dir --show-toplevel: &#34;fatal: not a git repository (or any of the parent directories): .git n&#34; Git LFS initialized. . from transformers import TrainingArguments args = TrainingArguments( &quot;bert-finetuned-squad&quot;, evaluation_strategy=&quot;no&quot;, save_strategy=&quot;epoch&quot;, learning_rate=2e-5, num_train_epochs=1, weight_decay=0.01, fp16=True, push_to_hub=True, ) . PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). . from transformers import Trainer trainer = Trainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=validation_dataset, tokenizer=tokenizer, ) trainer.train() . /content/bert-finetuned-squad is already a clone of https://huggingface.co/kurianbenoy/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`. Using amp half precision backend /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 88729 Num Epochs = 1 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 11092 . . [ 4154/11092 2:29:47 &lt; 4:10:18, 0.46 it/s, Epoch 0.37/1] Step Training Loss . 500 | 1.691300 | . 1000 | 1.575600 | . 1500 | 1.441200 | . 2000 | 1.356100 | . 2500 | 1.297700 | . 3000 | 1.245200 | . 3500 | 1.242400 | . 4000 | 1.181400 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; predictions, _ = trainer.predict(validation_dataset) start_logits, end_logits = predictions compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[&quot;validation&quot;]) . trainer.push_to_hub(commit_message=&quot;Training complete&quot;) . &lt;/div&gt; .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/02/27/QuestionAnsweringWithHF.html",
            "relUrl": "/2022/02/27/QuestionAnsweringWithHF.html",
            "date": " • Feb 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Starting_my_ml_blog",
            "content": "Starting my ML blog . I am into the field of AI/DataScience for some time. To dive deep into this field, I am creating this ML blogpost space. This space is exclusively for my ML/AI articles. Also by 27th February, 2022 expect a blog post on question answering w/ blurr. .",
            "url": "https://kurianbenoy.github.io/ml-blog/2022/02/21/Starting_my_ML_blog.html",
            "relUrl": "/2022/02/21/Starting_my_ML_blog.html",
            "date": " • Feb 21, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kurianbenoy.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kurianbenoy.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}